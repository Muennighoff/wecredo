{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskedPretraining_Eager.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IW0_DO17MLT0",
        "4lzKWR3FMJVQ",
        "ccXBubyiL8Ud"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54ea54fd67e0408c82d0811d2346fbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_46ab8c81fc244aee9743d884ec920eda",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_324486834e594012846856a2b9487cd1",
              "IPY_MODEL_7679593b31d44523825c5ee9ed01e4b6"
            ]
          }
        },
        "46ab8c81fc244aee9743d884ec920eda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "324486834e594012846856a2b9487cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e2d6467414045f0bd0c209966ab76b2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3138,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3138,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c535869add4b4e4e91eb17c154e657a0"
          }
        },
        "7679593b31d44523825c5ee9ed01e4b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3cb9332cf1c4dbaa8e25ad6aba73886",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3138/3138 [00:05&lt;00:00, 560.46ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fd9c3ea003d49f886ebbe452c9fa210"
          }
        },
        "6e2d6467414045f0bd0c209966ab76b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c535869add4b4e4e91eb17c154e657a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3cb9332cf1c4dbaa8e25ad6aba73886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fd9c3ea003d49f886ebbe452c9fa210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eedf79eee9554a9f9d446f190a8ff8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4669decc3a0948b2a33c0d7122c63f24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85d157c18cc34ef68bcdc19f9cc97faf",
              "IPY_MODEL_c3aafdc18a35434a8c5761e66b28be5f"
            ]
          }
        },
        "4669decc3a0948b2a33c0d7122c63f24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85d157c18cc34ef68bcdc19f9cc97faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5d288de1d22b4036bc7154403ee6a2c9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3138,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3138,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9785b438d9284a4d97a19f21367049dc"
          }
        },
        "c3aafdc18a35434a8c5761e66b28be5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a45b1fb82e4c4694b94e4189f12d39d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3138/3138 [10:13&lt;00:00,  5.11ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fcb946afaf3421cbc5d2f164577ee85"
          }
        },
        "5d288de1d22b4036bc7154403ee6a2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9785b438d9284a4d97a19f21367049dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a45b1fb82e4c4694b94e4189f12d39d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fcb946afaf3421cbc5d2f164577ee85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RSXAat3wMLb"
      },
      "source": [
        "!pip install -q transformers\r\n",
        "!pip install -q datasets\r\n",
        "!pip install -q ltp"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qqWZKaXSWgz",
        "outputId": "0b913e05-9b7e-456f-fabe-b1584f75321e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 27 15:53:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    34W / 250W |   1269MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0_DO17MLT0"
      },
      "source": [
        "##### WWM Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qIxq8O7MRIK"
      },
      "source": [
        "## Whole Word Masking\r\n",
        "\r\n",
        "### WWM for CN\r\n",
        "\r\n",
        "# References:\r\n",
        "# https://github.com/brightmart/roberta_zh/blob/13f7849f0cb0e11573e032acddb35b83b096224e/create_pretraining_data.py\r\n",
        "# https://github.com/huggingface/transformers/blob/9152f16023b59d262b51573714b40325c8e49370/examples/legacy/run_chinese_ref.py#L78\r\n",
        "\r\n",
        "\r\n",
        "# 1) Generate ref ids based on LTP tokenizer > prepare_ref\r\n",
        "# 2) Generate mask for whole words\r\n",
        "# 3) Implement the masking\r\n",
        "\r\n",
        "from transformers import AutoTokenizer\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "from ltp import LTP\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "\r\n",
        "def random_word(tokens, ref_ids, tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\r\n",
        "    :param tokens: list of str, tokenized sentence.\r\n",
        "    :param ref_ids: list of int, 1 is where to place a mask\r\n",
        "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\r\n",
        "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\r\n",
        "    \"\"\"\r\n",
        "    output_label = []\r\n",
        "\r\n",
        "    for i, (token, ref_id) in enumerate(zip(tokens, ref_ids)):\r\n",
        "\r\n",
        "        prob = random.random()\r\n",
        "\r\n",
        "        if ref_id == 1:\r\n",
        "\r\n",
        "            # 80% randomly change token to mask token\r\n",
        "            if prob < 0.8:\r\n",
        "                tokens[i] = \"[MASK]\"\r\n",
        "\r\n",
        "            # 10% randomly change token to random token\r\n",
        "            elif prob < 0.9:\r\n",
        "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\r\n",
        "\r\n",
        "            # -> rest 10% randomly keep current token\r\n",
        "\r\n",
        "            # append current token to output (we will predict these later)\r\n",
        "            try:\r\n",
        "                output_label.append(tokenizer.vocab[token])\r\n",
        "            except KeyError:\r\n",
        "                # For unknown words (should not occur with BPE vocab)\r\n",
        "                output_label.append(tokenizer.vocab[\"[UNK]\"])\r\n",
        "        else:\r\n",
        "            # no masking token (will be ignored by loss function later)\r\n",
        "            output_label.append(-100)\r\n",
        "\r\n",
        "    return tokens, output_label\r\n",
        "\r\n",
        "def _is_chinese_char(cp):\r\n",
        "    \"\"\"\r\n",
        "    Checks whether CP is the codepoint of a CJK character.\r\n",
        "    \"\"\"\r\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\r\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\r\n",
        "    #\r\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\r\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\r\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\r\n",
        "    # space-separated words, so they are not treated specially and handled\r\n",
        "    # like the all of the other languages.\r\n",
        "    if (\r\n",
        "        (cp >= 0x4E00 and cp <= 0x9FFF)\r\n",
        "        or (cp >= 0x3400 and cp <= 0x4DBF)  #\r\n",
        "        or (cp >= 0x20000 and cp <= 0x2A6DF)  #\r\n",
        "        or (cp >= 0x2A700 and cp <= 0x2B73F)  #\r\n",
        "        or (cp >= 0x2B740 and cp <= 0x2B81F)  #\r\n",
        "        or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\r\n",
        "        or (cp >= 0xF900 and cp <= 0xFAFF)\r\n",
        "        or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\r\n",
        "    ):  #\r\n",
        "        return True\r\n",
        "\r\n",
        "    return False\r\n",
        "\r\n",
        "\r\n",
        "def is_chinese(word):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "      word: str\r\n",
        "    \"\"\"\r\n",
        "    # word like '180' or '身高' or '神'\r\n",
        "    for char in word:\r\n",
        "        char = ord(char)\r\n",
        "        if not _is_chinese_char(char):\r\n",
        "            return 0\r\n",
        "    return 1\r\n",
        "\r\n",
        "\r\n",
        "def get_chinese_word(tokens):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "      List[str]\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    word_set = set()\r\n",
        "\r\n",
        "    for token in tokens:\r\n",
        "        chinese_word = len(token) > 1 and is_chinese(token)\r\n",
        "        if chinese_word:\r\n",
        "            word_set.add(token)\r\n",
        "    word_list = list(word_set)\r\n",
        "    return word_list\r\n",
        "\r\n",
        "\r\n",
        "def add_sub_symbol(bert_tokens, chinese_word_set):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "      bert_tokens: List[str]\r\n",
        "      chinese_word_set: set\r\n",
        "    \"\"\"\r\n",
        "    if not chinese_word_set:\r\n",
        "        return bert_tokens\r\n",
        "    max_word_len = max([len(w) for w in chinese_word_set])\r\n",
        "\r\n",
        "    bert_word = bert_tokens\r\n",
        "    start, end = 0, len(bert_word)\r\n",
        "    while start < end:\r\n",
        "        single_word = True\r\n",
        "        if is_chinese(bert_word[start]):\r\n",
        "            l = min(end - start, max_word_len)\r\n",
        "            for i in range(l, 1, -1):\r\n",
        "                whole_word = \"\".join(bert_word[start : start + i])\r\n",
        "                if whole_word in chinese_word_set:\r\n",
        "                    for j in range(start + 1, start + i):\r\n",
        "                        bert_word[j] = \"##\" + bert_word[j]\r\n",
        "                    start = start + i\r\n",
        "                    single_word = False\r\n",
        "                    break\r\n",
        "        if single_word:\r\n",
        "            start += 1\r\n",
        "    return bert_word\r\n",
        "\r\n",
        "def prepare_ref(lines, ltp_tokenizer, bert_tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    Args:\r\n",
        "      lines: List[str] - e.g. [text1, text2]\r\n",
        "      ltp_tokenizer\r\n",
        "      bert_tokenizer\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      ref_ids: List[List[int], ...]\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    ltp_res = []\r\n",
        "\r\n",
        "    for i in range(0, len(lines), 100):\r\n",
        "        res = ltp_tokenizer.seg(lines[i : i + 100])[0]\r\n",
        "        res = [get_chinese_word(r) for r in res]\r\n",
        "        ltp_res.extend(res)\r\n",
        "    assert len(ltp_res) == len(lines)\r\n",
        "\r\n",
        "    bert_res = []\r\n",
        "    for i in range(0, len(lines), 100):\r\n",
        "        res = bert_tokenizer(lines[i : i + 100], add_special_tokens=True, truncation=True, max_length=512)\r\n",
        "        bert_res.extend(res[\"input_ids\"])\r\n",
        "    assert len(bert_res) == len(lines)\r\n",
        "\r\n",
        "    ref_ids = []\r\n",
        "    for input_ids, chinese_word in zip(bert_res, ltp_res):\r\n",
        "\r\n",
        "        input_tokens = []\r\n",
        "        for id in input_ids:\r\n",
        "            token = bert_tokenizer._convert_id_to_token(id)\r\n",
        "            input_tokens.append(token)\r\n",
        "        input_tokens = add_sub_symbol(input_tokens, chinese_word)\r\n",
        "        ref_id = []\r\n",
        "        # We only save pos of chinese subwords start with ##, which mean is part of a whole word.\r\n",
        "        for i, token in enumerate(input_tokens):\r\n",
        "            if token[:2] == \"##\":\r\n",
        "                clean_token = token[2:]\r\n",
        "                # save chinese tokens' pos\r\n",
        "                if len(clean_token) == 1 and _is_chinese_char(ord(clean_token)):\r\n",
        "                    ref_id.append(i)\r\n",
        "        ref_ids.append(ref_id)\r\n",
        "\r\n",
        "    assert len(ref_ids) == len(bert_res)\r\n",
        "\r\n",
        "    return ref_ids\r\n",
        "\r\n",
        "\r\n",
        "def cn_whole_word_mask(input_tokens, ref_ids, max_predictions=512, mlm_probability=0.15):\r\n",
        "    \"\"\"\r\n",
        "    Masks whole words in CN based on the reference ids & the standard _whole_word_mask for BERT for one individual example.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      input_tokens: List[str]\r\n",
        "      ref_tokens: List[int]\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      input_tokens: List[int]\r\n",
        "\r\n",
        "    TODO:\r\n",
        "      We could save the LTP dependency by copying the function from: https://github.com/HIT-SCIR/ltp/blob/c47b3f455c07c5dcc186f2b674efde8c67612baf/ltp/algorithms/maximum_forward_matching.py#L75\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    for i in range(len(input_tokens)):\r\n",
        "        if i in ref_ids:\r\n",
        "            # We move it back by -1 as the ref_ids start at 1, not 0\r\n",
        "            input_tokens[i-1] = \"##\" + input_tokens[i-1]\r\n",
        "\r\n",
        "    input_tokens = _whole_word_mask(input_tokens)\r\n",
        "\r\n",
        "    return input_tokens\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def _whole_word_mask(input_tokens, max_predictions=512, mlm_probability=0.15):\r\n",
        "    \"\"\"\r\n",
        "    Get 0/1 labels for masked tokens with whole word mask proxy\r\n",
        "\r\n",
        "    Args:\r\n",
        "      input_tokens: List[str]\r\n",
        "\r\n",
        "    Outputs:\r\n",
        "      input_tokens: List[int]\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cand_indexes = []\r\n",
        "    for (i, token) in enumerate(input_tokens):\r\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\r\n",
        "            continue\r\n",
        "\r\n",
        "        if len(cand_indexes) >= 1 and token.startswith(\"##\"):\r\n",
        "            cand_indexes[-1].append(i)\r\n",
        "\r\n",
        "        else:\r\n",
        "            cand_indexes.append([i])\r\n",
        "\r\n",
        "    random.shuffle(cand_indexes)\r\n",
        "    num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * mlm_probability))))\r\n",
        "    masked_lms = []\r\n",
        "    covered_indexes = set()\r\n",
        "    for index_set in cand_indexes:\r\n",
        "        if len(masked_lms) >= num_to_predict:\r\n",
        "            break\r\n",
        "        # If adding a whole-word mask would exceed the maximum number of\r\n",
        "        # predictions, then just skip this candidate.\r\n",
        "        if len(masked_lms) + len(index_set) > num_to_predict:\r\n",
        "            continue\r\n",
        "        is_any_index_covered = False\r\n",
        "        for index in index_set:\r\n",
        "            if index in covered_indexes:\r\n",
        "                is_any_index_covered = True\r\n",
        "                break\r\n",
        "        if is_any_index_covered:\r\n",
        "            continue\r\n",
        "        for index in index_set:\r\n",
        "            covered_indexes.add(index)\r\n",
        "            masked_lms.append(index)\r\n",
        "\r\n",
        "    assert len(covered_indexes) == len(masked_lms)\r\n",
        "    mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\r\n",
        "    return mask_labels\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class WWMTokenizer():\r\n",
        "    def __init__(self, col=\"text\", seq_len=512):\r\n",
        "        \"\"\"\r\n",
        "        Constructs Huggingface CN tokenizer & other\r\n",
        "\r\n",
        "            col: What column to tokenize if pretraining\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.tokenizer_cn = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\r\n",
        "        self.tokenizer_ltp = LTP(\"small\")\r\n",
        "        self.max_seq_length = seq_len\r\n",
        "        self.col = col\r\n",
        "\r\n",
        "    def tokenize_pretraining(self, example):\r\n",
        "        \"\"\"\r\n",
        "        Takes in an example & returns pretraining data\r\n",
        "\r\n",
        "        Args:\r\n",
        "            Example: dict with entry \"text\"\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            Dict of TF Tensors\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        inputs = example[self.col]\r\n",
        "    \r\n",
        "\r\n",
        "        ref_ids = prepare_ref([inputs], self.tokenizer_ltp, self.tokenizer_cn)\r\n",
        "\r\n",
        "        tokens = self.tokenizer_cn.tokenize(inputs)\r\n",
        "\r\n",
        "        if len(tokens) > self.max_seq_length - 2:\r\n",
        "            tokens = tokens[:(self.max_seq_length - 2)]\r\n",
        "            ref_ids = ref_ids[:(self.max_seq_length - 2)]\r\n",
        "\r\n",
        "        ref_ids = cn_whole_word_mask(tokens, ref_ids[0])\r\n",
        "        tokens, labels = random_word(tokens, ref_ids, self.tokenizer_cn)\r\n",
        "\r\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\r\n",
        "        lm_label_ids = ([-100] + labels + [-100])\r\n",
        "\r\n",
        "        input_ids = self.tokenizer_cn.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "        attention_mask = [1] * len(input_ids)\r\n",
        "        token_type_ids = [0] * len(input_ids)\r\n",
        "\r\n",
        "        while len(input_ids) < self.max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            attention_mask.append(0)\r\n",
        "            token_type_ids.append(0)\r\n",
        "            lm_label_ids.append(-100)\r\n",
        "\r\n",
        "        assert len(input_ids) == self.max_seq_length\r\n",
        "        assert len(attention_mask) == self.max_seq_length\r\n",
        "        assert len(token_type_ids) == self.max_seq_length\r\n",
        "        assert len(lm_label_ids) == self.max_seq_length\r\n",
        "\r\n",
        "\r\n",
        "        outputs = {'input_ids': tf.constant(input_ids), 'attention_mask': tf.constant(attention_mask), \r\n",
        "                'token_type_ids': tf.constant(attention_mask), 'lm_label_ids': tf.constant(lm_label_ids)}\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def to_tf_dataset(self, dataset): \r\n",
        "        \"\"\"\r\n",
        "        Turns dataset into a TF compatible dataset\r\n",
        "        \"\"\"\r\n",
        "        columns = ['input_ids', 'attention_mask', 'token_type_ids', 'lm_label_ids']\r\n",
        "        dataset.set_format(type='tensorflow', columns=columns)\r\n",
        "\r\n",
        "        return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \r\n",
        "                      'token_type_ids':tf.int32, 'lm_label_ids':tf.int32}\r\n",
        "\r\n",
        "        return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \r\n",
        "                        'token_type_ids': tf.TensorShape([None]), 'lm_label_ids':tf.TensorShape([None])}\r\n",
        "\r\n",
        "        ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\r\n",
        "        return ds"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lzKWR3FMJVQ"
      },
      "source": [
        "##### Performer Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZollJirsMRr-"
      },
      "source": [
        "# Config + Module for Performer Attention\r\n",
        "\r\n",
        "# Resources:\r\n",
        "# https://github.com/xl402/performer/blob/master/performer/networks/linear_attention.py\r\n",
        "# https://github.com/huggingface/transformers/blob/1c4236d8ef884f9f4cb1bf807ef622199c56df80/src/transformers/modeling_tf_performer_attention.py\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "import logging\r\n",
        "import math\r\n",
        "import random\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from typing import Callable, Sequence, Optional, Union\r\n",
        "\r\n",
        "from dataclasses import dataclass\r\n",
        "from enum import Enum\r\n",
        "\r\n",
        "from transformers.modeling_tf_utils import shape_list\r\n",
        "\r\n",
        "PerformerKernel = Enum('PerformerKernel', ['cosh', 'exp', 'elu', 'relu'])\r\n",
        "OrthogonalFeatureAlgorithm = Enum('OrthogonalFeatureAlgorithm', ['auto', 'kacs', 'qr'])\r\n",
        "\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class PerformerAttentionConfig:\r\n",
        "    r\"\"\"\r\n",
        "    This is the configuration class to store the configuration of a :class:`~transformers.PerformerAttention` module.\r\n",
        "    It is used to define the behavior of a Performer/FAVOR+ attention module when it is initialized.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        attention_dropout (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout ratio for the attention probabilities.\r\n",
        "        causal (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to apply causal attention, where positions are prevented from attending to positions to ahead\r\n",
        "            of themselves in the sequence, using the prefix-sum method.\r\n",
        "        kernel_type (:obj:`Enum(PerformerKernel)`, `optional`, defaults to :obj:`'exp'`):\r\n",
        "            The type of kernel function to use for comparing the queries and keys. Possible options are :obj:`'exp'`,\r\n",
        "            :obj:`'cosh'`, and :obj:`'relu'`. The :obj:`'cosh'` option approximates softmax attention with a smaller\r\n",
        "            variance than :obj:`'exp'`, but at the cost of using twice as many random features. :obj:`'relu'` may result\r\n",
        "            in better performance than :obj:`'exp'` and :obj:`'cosh'` in certain circumstances, but it is not an\r\n",
        "            unbiased estimator of softmax attention and thus should not be used with pretrained models that were\r\n",
        "            pretrained with softmax attention.\r\n",
        "        kernel_epsilon (:obj:`float`, `optional`, defaults to 1e-4):\r\n",
        "            Stabilizer term added to the output of the kernel function to avoid dividing by very small numbers.\r\n",
        "        normalize_output (:obj:`bool`, `optional`, defaults to True):\r\n",
        "            Whether to ensure that the output vectors are convex combinations of the input vectors; that is, that the\r\n",
        "            rows of the implicit attention map sum to 1.\r\n",
        "        normalization_stabilizer (:obj:`float`, `optional`, defaults to 1e-6):\r\n",
        "            Stabilizer term used when normalizing the output to avoid dividing by very small numbers.\r\n",
        "        num_random_features (:obj:`int`, `optional`, defaults to None):\r\n",
        "            The dimensionality of the random feature vectors to use. When None, the dimensionality is set to\r\n",
        "            D * log(D), where D is the dimensionality of each attention head.\r\n",
        "        orthogonal_feature_algorithm (:obj:`Enum(OrthogonalFeatureAlgorithm)`, defaults to 'auto'):\r\n",
        "            The algorithm to use for generating random orthogonal features. Possible values are 'kacs', which uses a\r\n",
        "            Kac's random walk Markov chain; 'qr', which performs QR decomposition on a random Gaussian matrix at each\r\n",
        "            redraw; and 'auto', which is equivalent to 'kacs' on PyTorch and 'qr' on TensorFlow, since the Kac's random\r\n",
        "            walk algorithm is not supported on TensorFlow. Kac's is generally faster than QR, but successive samples\r\n",
        "            are correlated with each other.\r\n",
        "        use_recurrent_decoding (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to use recurrent autoregressive decoding, as described in the 'Transformers are RNNs' paper. If\r\n",
        "            True, the PerformerAttention object will expect input tensors with a sequence length dimension of exactly 1,\r\n",
        "            and will output tensors with sequence length of 1. It will retain a recurrent hidden state between forward\r\n",
        "            passes that can be reset with the reset_recurrent_state() method.\r\n",
        "        use_thick_features (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to generate a random feature tensor that has a batch dimension.\r\n",
        "        use_orthogonal_features (:obj:`bool`, `optional`, defaults to True):\r\n",
        "            Whether to use strictly orthogonal random features, as opposed to features drawn from a standard Gaussian\r\n",
        "            distribution. Orthogonal features result in outputs that more closely approximate softmax attention, but at\r\n",
        "            the cost of doing QR decomposition on the CPU every time the features are redrawn. Best combined with a\r\n",
        "            reasonably large value of :obj:`feature_redraw_interval` (1-5k).\r\n",
        "        use_linear_layers (:obj:`bool`, `optional`, defaults to True):\r\n",
        "            Whether to transform the Q, K, and V inputs with a Linear layer before applying attention. Setting this\r\n",
        "            to False may be useful if you want to use PerformerAttention as one component of a more complex\r\n",
        "            attention mechanism.\r\n",
        "        regularize_feature_norms (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to ensure that the random feature vectors have a norm of sqrt(`d`), where `d` is the dimensionality\r\n",
        "            of each attention head.\r\n",
        "        feature_redraw_interval (:obj:`int`, `optional`, defaults to 100):\r\n",
        "            The number of forward passes after which the random feature matrix should be redrawn. If None, then the\r\n",
        "            feature matrix is never redrawn. When combined with :obj:`redraw_stochastically`, this parameter determines\r\n",
        "            the expected value of the redraw interval, rather than the interval itself.\r\n",
        "        redraw_stochastically (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            If true, PerformerAttention will redraw its random features each forward pass with a probability equal to\r\n",
        "            (1 / :obj:`feature_redraw_interval`), instead of deterministically redrawing once every N passes. This could\r\n",
        "            be desirable in large models to ensure that the attention layers don't all redraw their features at the same\r\n",
        "            time.\r\n",
        "        redraw_verbose (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to log a message when random features are redrawn during training.\r\n",
        "        dim (:obj:`int`, `optional`):\r\n",
        "            Dimensionality of the queries, keys, and values.\r\n",
        "        num_heads (:obj:`int`, `optional`):\r\n",
        "            Number of attention heads.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    attention_dropout: float = 0.1\r\n",
        "    kernel_type: Union[str, Callable, PerformerKernel] = PerformerKernel.exp\r\n",
        "\r\n",
        "    causal: bool = False\r\n",
        "    use_recurrent_decoding: bool = False\r\n",
        "\r\n",
        "    kernel_epsilon: float = 1e-4\r\n",
        "    normalize_output: bool = True\r\n",
        "    normalization_stabilizer: float = 1e-6\r\n",
        "\r\n",
        "    # The linear_layer_names parameter is needed to allow the PerformerAttention object to imitate the naming\r\n",
        "    # convention of arbitrary attention modules, and therefore load weights from pretrained models. It can either have\r\n",
        "    # 3 or 4 elements; if it has 3, then no output linear layer is used.\r\n",
        "    use_linear_layers: bool = True\r\n",
        "    linear_layer_names: Sequence[str] = ('q_linear', 'k_linear', 'v_linear', 'out_linear')\r\n",
        "\r\n",
        "    num_random_features: Optional[int] = None\r\n",
        "    use_thick_features: bool = False\r\n",
        "    regularize_feature_norms: bool = True\r\n",
        "\r\n",
        "    use_orthogonal_features: bool = True\r\n",
        "    orthogonal_feature_algorithm: Union[str, OrthogonalFeatureAlgorithm] = OrthogonalFeatureAlgorithm.auto\r\n",
        "\r\n",
        "    feature_redraw_interval: Optional[int] = 100\r\n",
        "    redraw_stochastically: bool = False\r\n",
        "    redraw_verbose: bool = False\r\n",
        "\r\n",
        "    # Optional here so the user doesn't have to set redundant parameters, but must be set by model before config is\r\n",
        "    # passed to PerformerAttention.__init__()\r\n",
        "    d_model: Optional[int] = None\r\n",
        "    num_heads: Optional[int] = None\r\n",
        "\r\n",
        "    # Make enums JSON serializable\r\n",
        "    def to_dict(self):\r\n",
        "        return {k: v.name if isinstance(v, Enum) else v for k, v in self.__dict__.items()}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "KERNEL_CALLABLES = {\r\n",
        "    PerformerKernel.cosh: lambda x, h: tf.concat((tf.exp(h + x), tf.exp(h - x)), axis=-1),\r\n",
        "    PerformerKernel.exp: lambda x, h: tf.exp(h + x),  # Default\r\n",
        "    PerformerKernel.elu: lambda x: tf.nn.elu(x) + 1,\r\n",
        "    PerformerKernel.relu: tf.nn.relu\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def resolve_enum(enum_class, value):\r\n",
        "    return enum_class[value] if isinstance(value, str) else value\r\n",
        "\r\n",
        "\r\n",
        "class TFPerformerAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config: Optional[Union[dict, PerformerAttentionConfig]] = None, **kwargs):\r\n",
        "        super().__init__(name=kwargs.pop('name', None), dtype=kwargs.pop('dtype', None))\r\n",
        "\r\n",
        "        if isinstance(config, dict):\r\n",
        "            config = PerformerAttentionConfig(**config)\r\n",
        "        else:\r\n",
        "            config = config or PerformerAttentionConfig()\r\n",
        "\r\n",
        "        # kwargs take precedence over the default values that might be stored in the config object\r\n",
        "        for k, v in kwargs.items():\r\n",
        "            assert hasattr(config, k), f\"'{k}' is an invalid config parameter\"\r\n",
        "            setattr(config, k, v)\r\n",
        "\r\n",
        "        self.__dict__.update(config.__dict__)\r\n",
        "\r\n",
        "        assert self.num_heads and self.d_model, \"Num_heads and d_model must be non-None\"\r\n",
        "        assert self.d_model % self.num_heads == 0, \"Num_heads must divide d_model evenly\"\r\n",
        "        assert self.d_model > self.num_heads, \"Number of dimensions per head must be greater than 1\"\r\n",
        "        \r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=self.attention_dropout)\r\n",
        "        self.calls_since_last_redraw = 0\r\n",
        "\r\n",
        "        self.orthogonal_feature_algorithm = resolve_enum(OrthogonalFeatureAlgorithm, self.orthogonal_feature_algorithm)\r\n",
        "        assert self.orthogonal_feature_algorithm != OrthogonalFeatureAlgorithm.kacs,\\\r\n",
        "            \"Kac's random walk is not supported in TensorFlow\"\r\n",
        "\r\n",
        "        # Create the feature matrix up front if we don't need to know what the batch dimension is;\r\n",
        "        # otherwise, lazily create it on the first forward pass\r\n",
        "        self.random_features = None\r\n",
        "        if not self.use_thick_features:\r\n",
        "            self._generate_feature_matrix(batch_size=1)\r\n",
        "\r\n",
        "        # Recurrent state\r\n",
        "        if self.use_recurrent_decoding:\r\n",
        "            self.s = None\r\n",
        "            self.z = None\r\n",
        "\r\n",
        "        if isinstance(self.kernel_type, Callable):\r\n",
        "            self.kernel_fn = self.kernel_type   # Allow for custom kernel types\r\n",
        "        else:\r\n",
        "            self.kernel_type = resolve_enum(PerformerKernel, self.kernel_type)\r\n",
        "            self.kernel_fn = KERNEL_CALLABLES[self.kernel_type]\r\n",
        "\r\n",
        "        if self.use_linear_layers:\r\n",
        "            for name in self.linear_layer_names:\r\n",
        "                setattr(self, name, tf.keras.layers.Dense(units=self.d_model))\r\n",
        "\r\n",
        "    def prune_heads(self, heads):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def redraw_features_now(self):\r\n",
        "        \"\"\"\r\n",
        "        Immediately redraws the random features.\r\n",
        "        \"\"\"\r\n",
        "        batch = self.random_features.shape[0]\r\n",
        "        self._generate_feature_matrix(batch)\r\n",
        "\r\n",
        "        if self.redraw_verbose:\r\n",
        "            logging.getLogger().info(\"TFPerformerAttention: Just redrew random features.\")\r\n",
        "\r\n",
        "        self.calls_since_last_redraw = 0\r\n",
        "\r\n",
        "    def reset_recurrent_state(self):\r\n",
        "        \"\"\"\r\n",
        "        Resets the recurrent state kept by the model when use_recurrent_decoding == True\r\n",
        "        \"\"\"\r\n",
        "        self.s = None\r\n",
        "        self.z = None\r\n",
        "\r\n",
        "    def call(self, query, key, value, mask=None, head_mask=None, output_attentions=False):\r\n",
        "        \"\"\"\r\n",
        "        Parameters:\r\n",
        "            query: torch.tensor(bs, seq_length, dim)\r\n",
        "            key: torch.tensor(bs, seq_length, dim)\r\n",
        "            value: torch.tensor(bs, seq_length, dim)\r\n",
        "            mask: torch.tensor(bs, seq_length)\r\n",
        "        Returns:\r\n",
        "            weights: tf.tensor(bs, num_heads, seq_length, seq_length) Attention weights context: tf.tensor(bs,\r\n",
        "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\r\n",
        "        \"\"\"\r\n",
        "        bs, q_length, dim = query.shape\r\n",
        "        dim_per_head = self.d_model // self.num_heads\r\n",
        "\r\n",
        "        def shape(x):\r\n",
        "            \"\"\" separate heads \"\"\"\r\n",
        "            # > Use shape_list instead of .shape\r\n",
        "            new_shape = tf.concat((shape_list(x)[:-1], tf.constant([self.num_heads, dim_per_head])), axis=0)\r\n",
        "            return tf.transpose(tf.reshape(x, new_shape), perm=[0, 2, 1, 3])\r\n",
        "\r\n",
        "        if self.use_linear_layers:\r\n",
        "            query, key, value = (getattr(self, name)(x) for name, x in\r\n",
        "                                 zip(self.linear_layer_names, (query, key, value)))\r\n",
        "        \r\n",
        "        # (bs, num_heads, q_length, dim_per_head)\r\n",
        "        query, key, value = (shape(x) for x in (query, key, value))\r\n",
        "\r\n",
        "        assert not output_attentions, \"Can't output attention maps when using Performer attention.\"\r\n",
        "        if self.use_recurrent_decoding:\r\n",
        "            assert q_length == 1, \"When use_recurrent_decoding == True, we only input and output one token at a time.\"\r\n",
        "        \r\n",
        "        self._redraw_features_if_needed(bs)\r\n",
        "        \r\n",
        "        # Get the transformed values of Q and K\r\n",
        "        q_prime, k_prime = self.get_projected_queries_and_keys(query, key)\r\n",
        "        return self.compute_attention_with_projected_queries_and_keys(q_prime, k_prime, value, mask, head_mask)\r\n",
        "\r\n",
        "    def get_projected_queries_and_keys(self, q, k):\r\n",
        "        \"\"\"\r\n",
        "        Turns Q into Q' and K into K' by multiplying them by the random feature tensor.\r\n",
        "        Parameters:\r\n",
        "            q: torch.tensor(bs, seq_length, dim)\r\n",
        "            k: torch.tensor(bs, seq_length, dim)\r\n",
        "        Returns:\r\n",
        "            q_prime: torch.tensor(bs, seq_length, num_features)\r\n",
        "            k_prime: torch.tensor(bs, seq_length, num_features)\r\n",
        "        \"\"\"\r\n",
        "        # Instead of dividing the product QK^T by sqrt(d), we divide Q and K by the 4th root of d.\r\n",
        "        q = q / (self.d_model ** 0.25)\r\n",
        "        k = k / (self.d_model ** 0.25)\r\n",
        "        \r\n",
        "        projected_q = q @ self.random_features\r\n",
        "        projected_k = k @ self.random_features\r\n",
        "        \r\n",
        "        # Special logic for kernels that attempt to approximate softmax\r\n",
        "        if self.kernel_type in (PerformerKernel.cosh, PerformerKernel.exp):\r\n",
        "            # The h(x) function is defined in Lemma 1 in Choromanski et al. pg. 4 as exp(-||x||**2 / 2). For numerical\r\n",
        "            # stability we leverage the fact that exp(x)*exp(y) = exp(x + y) here and delay computing the exp().\r\n",
        "            h_of_q = -tf.math.reduce_sum(q ** 2, axis=-1, keepdims=True) / 2\r\n",
        "            h_of_k = -tf.math.reduce_sum(k ** 2, axis=-1, keepdims=True) / 2\r\n",
        "            \r\n",
        "            # Compute the numerical stabilizer that we subtract from the input to exp(). For some reason the original\r\n",
        "            # Jax implementation uses different types of stabilizers for queries vs. keys, and we follow that here.\r\n",
        "            q_stabilizer = tf.math.reduce_max(h_of_q, axis=-1, keepdims=True)\r\n",
        "            \r\n",
        "            # This is just a scalar\r\n",
        "            k_stabilizer = tf.math.reduce_max(h_of_k)\r\n",
        "            \r\n",
        "            q_kernel_output = self.kernel_fn(projected_q - q_stabilizer, h_of_q)\r\n",
        "            k_kernel_output = self.kernel_fn(projected_k - k_stabilizer, h_of_k)\r\n",
        "            \r\n",
        "            # By multiplying by 1/sqrt(m), we ensure the final matrix product will contain a factor of 1/m. This means\r\n",
        "            # each row of Q'K'^T can be interpreted as an average over the exp(omega^T * q) * exp(omega^T * k) terms.\r\n",
        "            normalizing_constant = (q_kernel_output.shape[-1] ** -0.5)\r\n",
        "            \r\n",
        "            q_prime = normalizing_constant * (q_kernel_output + self.kernel_epsilon)\r\n",
        "            k_prime = normalizing_constant * (k_kernel_output + self.kernel_epsilon)\r\n",
        "            return q_prime, k_prime\r\n",
        "        \r\n",
        "        # Generalized attention (ReLU, ELU...)\r\n",
        "        else:\r\n",
        "            return tuple(self.kernel_fn(x) + self.kernel_epsilon for x in (projected_q, projected_k))\r\n",
        "\r\n",
        "    def compute_attention_with_projected_queries_and_keys(self, q_prime, k_prime, v, mask=None, head_mask=None):\r\n",
        "        \"\"\"\r\n",
        "        Computes the attention output given Q' and K' from the above get_projected_queries_and_keys method.\r\n",
        "        Parameters:\r\n",
        "            q_prime: tf.tensor(bs, seq_length, num_features)\r\n",
        "            k_prime: tf.tensor(bs, seq_length, num_features)\r\n",
        "            v: tf.tensor(bs, seq_length, dim)\r\n",
        "            mask: tf.tensor(bs, seq_length)\r\n",
        "        Returns:\r\n",
        "            V': tf.tensor(bs, seq_length, dim)\r\n",
        "        \"\"\"\r\n",
        "        # Apply the padding mask to K'. Also applying it to Q' would be redundant.\r\n",
        "        if mask is not None:\r\n",
        "            # If extended attention mask we need to reshape it to (bs, seq_len)\r\n",
        "            # Note: k_prime actual shape is (bs, ?, seq_length, num_features)\r\n",
        "            mask = tf.reshape(mask, shape=(shape_list(k_prime)[0], shape_list(k_prime)[2]))\r\n",
        "\r\n",
        "            k_prime *= tf.expand_dims(tf.expand_dims(mask, 1), -1)\r\n",
        "\r\n",
        "        k_prime_t = tf.linalg.matrix_transpose(k_prime)\r\n",
        "        output = self._numerator_for_projected_queries_and_keys(q_prime, k_prime_t, v)\r\n",
        "\r\n",
        "        if self.normalize_output:\r\n",
        "            output /= self._denominator_for_projected_queries_and_keys(q_prime, k_prime_t)\r\n",
        "\r\n",
        "        return self._finalize_attention_output(output, head_mask)\r\n",
        "\r\n",
        "    def _numerator_for_projected_queries_and_keys(self, q_prime, k_prime_t, v):\r\n",
        "        # Noncausal\r\n",
        "        if not self.causal:\r\n",
        "            return q_prime @ (k_prime_t @ v)\r\n",
        "\r\n",
        "        # Causal, during training\r\n",
        "        if not self.use_recurrent_decoding:\r\n",
        "            return _headwise_causal_numerator(q_prime, k_prime_t, v)\r\n",
        "\r\n",
        "        # Causal, at inference time\r\n",
        "        s_delta = k_prime_t @ v\r\n",
        "        self.s = s_delta if self.s is None else self.s + s_delta\r\n",
        "\r\n",
        "        return q_prime @ self.s\r\n",
        "\r\n",
        "    def _denominator_for_projected_queries_and_keys(self, q_prime, k_prime_t):\r\n",
        "        # Noncausal\r\n",
        "        if not self.causal:\r\n",
        "            denom = q_prime @ tf.math.reduce_sum(k_prime_t, axis=-1, keepdims=True)  # Sum over positions\r\n",
        "\r\n",
        "        # Causal, during training\r\n",
        "        elif not self.use_recurrent_decoding:\r\n",
        "            prefix_sums = tf.cumsum(k_prime_t, axis=-1)               # Cumsum over positions\r\n",
        "            denom = tf.einsum(\"bhlm,bhml->bhl\", q_prime, prefix_sums)\r\n",
        "            denom = tf.expand_dims(denom, axis=-1)\r\n",
        "\r\n",
        "        # Causal, at inference time\r\n",
        "        else:\r\n",
        "            self.z = k_prime_t if self.z is None else self.z + k_prime_t    # Incrementally sum over positions\r\n",
        "            denom = q_prime @ self.z\r\n",
        "\r\n",
        "        # Avoid dividing by very small numbers\r\n",
        "        extreme_vals = tf.cast(tf.math.abs(denom) <= self.normalization_stabilizer, denom.dtype)\r\n",
        "        return denom + 2 * self.normalization_stabilizer * extreme_vals\r\n",
        "    \r\n",
        "    def _finalize_attention_output(self, context, head_mask=None, att_map_to_output=None):\r\n",
        "        # Mask heads if we want to\r\n",
        "        if head_mask is not None:\r\n",
        "            context = context * head_mask\r\n",
        "\r\n",
        "        x = tf.transpose(context, perm=[0, 2, 1, 3])  # [...seq_len, num_heads, dim_per_head]\r\n",
        "        new_last_dim = shape_list(x)[-2] * shape_list(x)[-1]\r\n",
        "        context = tf.reshape(x, shape_list(x)[:-2] + [new_last_dim])  # (bs, q_length, dim)\r\n",
        "\r\n",
        "        if self.use_linear_layers and len(self.linear_layer_names) > 3:\r\n",
        "            context = getattr(self, self.linear_layer_names[3])(context)  # (bs, q_length, dim)\r\n",
        "\r\n",
        "        if att_map_to_output:\r\n",
        "            return context, att_map_to_output\r\n",
        "        else:\r\n",
        "            return context,\r\n",
        "\r\n",
        "    def _generate_feature_matrix(self, batch_size):\r\n",
        "        dim_per_head = self.d_model // self.num_heads\r\n",
        "        num_rows = self.num_random_features or round(dim_per_head * math.log(dim_per_head))\r\n",
        "        batch = batch_size if self.use_thick_features else 1\r\n",
        "        \r\n",
        "        if not self.use_orthogonal_features:\r\n",
        "            final_tensor = tf.random.normal((batch, num_rows, dim_per_head))\r\n",
        "        else:\r\n",
        "            total_num_blocks = int(math.ceil(num_rows / dim_per_head))\r\n",
        "            extra_rows = total_num_blocks * dim_per_head - num_rows\r\n",
        "\r\n",
        "            blocks = [_get_orthogonal_block(batch, dim_per_head) for _ in range(total_num_blocks)]\r\n",
        "            if extra_rows > 0:\r\n",
        "                blocks[-1] = blocks[-1][:, extra_rows:]\r\n",
        "\r\n",
        "            final_tensor = tf.concat(blocks, axis=1)\r\n",
        "        \r\n",
        "            # This option yields SMREG\r\n",
        "            if self.regularize_feature_norms:\r\n",
        "                final_tensor *= dim_per_head ** 0.5\r\n",
        "            else:\r\n",
        "                # Hack to make the matrix columns have the norm we would expect them to have if they were sampled\r\n",
        "                # straight from a Gaussian, instead of being all norm 1 since they went through QR decomposition\r\n",
        "                multiplier = tf.norm(tf.random.normal((batch, num_rows, dim_per_head)), axis=-1)\r\n",
        "                final_tensor = tf.linalg.diag(multiplier) @ final_tensor\r\n",
        "\r\n",
        "        final_tensor = tf.expand_dims(final_tensor, axis=1)     # Add an attention head dimension\r\n",
        "        final_tensor = tf.linalg.matrix_transpose(final_tensor)\r\n",
        "        self.random_features = final_tensor\r\n",
        "    \r\n",
        "    def _redraw_features_if_needed(self, batch):\r\n",
        "        # We haven't created the projection matrix yet, let's create it\r\n",
        "        if self.random_features is None:\r\n",
        "            self._generate_feature_matrix(batch)\r\n",
        "        \r\n",
        "        elif self.feature_redraw_interval is not None:\r\n",
        "            if self.redraw_stochastically:\r\n",
        "                # random.random() returns a float between 0.0 and 1.0, so this expression\r\n",
        "                # evaluates to True with probability 1. / self.feature_redraw_interval\r\n",
        "                if random.random() < 1. / self.feature_redraw_interval:\r\n",
        "                    self.redraw_features_now()\r\n",
        "            \r\n",
        "            # It's time to redraw the projection matrix\r\n",
        "            elif self.calls_since_last_redraw >= self.feature_redraw_interval:\r\n",
        "                self.redraw_features_now()\r\n",
        "        \r\n",
        "            # Keep track of how many forward passes we do before we redraw again\r\n",
        "            else:\r\n",
        "                self.calls_since_last_redraw += 1\r\n",
        "\r\n",
        "\r\n",
        "def _get_orthogonal_block(batch, size):\r\n",
        "    with tf.device('/CPU:0'):\r\n",
        "        unstructured_block = tf.random.normal((batch, size, size))\r\n",
        "        orthog, r = tf.linalg.qr(unstructured_block)\r\n",
        "\r\n",
        "    return tf.linalg.matrix_transpose(orthog)\r\n",
        "\r\n",
        "\r\n",
        "def _headwise_causal_numerator(q_prime, k_prime_t, v):\r\n",
        "    results = []\r\n",
        "\r\n",
        "    # Iterate over the attention heads to avoid allocating a very large tensor\r\n",
        "    for head in range(q_prime.shape[1]):\r\n",
        "        # Outer products- a sorta biggish tensor\r\n",
        "        outer_prods = tf.einsum('bml,bld->blmd', k_prime_t[:, head], v[:, head])\r\n",
        "        prefix_sums = tf.cumsum(outer_prods, axis=1)\r\n",
        "\r\n",
        "        query_prods = tf.einsum('blmd,blm->bld', prefix_sums, q_prime[:, head])\r\n",
        "        results.append(tf.expand_dims(query_prods, axis=1))\r\n",
        "\r\n",
        "    return tf.concat(results, axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccXBubyiL8Ud"
      },
      "source": [
        "##### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P1C9xdDL-Hg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "d2b70b62-eadf-48cc-baaa-aa9423bb94b6"
      },
      "source": [
        "import warnings\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from transformers.activations_tf import get_tf_activation\r\n",
        "from transformers.file_utils import (\r\n",
        "    MULTIPLE_CHOICE_DUMMY_INPUTS,\r\n",
        "    add_code_sample_docstrings,\r\n",
        "    add_start_docstrings,\r\n",
        "    add_start_docstrings_to_model_forward,\r\n",
        ")\r\n",
        "from transformers.modeling_tf_outputs import (\r\n",
        "    TFBaseModelOutput,\r\n",
        "    TFBaseModelOutputWithPooling,\r\n",
        "    TFMaskedLMOutput,\r\n",
        "    TFMultipleChoiceModelOutput,\r\n",
        "    TFQuestionAnsweringModelOutput,\r\n",
        "    TFSequenceClassifierOutput,\r\n",
        "    TFTokenClassifierOutput,\r\n",
        ")\r\n",
        "from transformers.modeling_tf_utils import (\r\n",
        "    TFMaskedLanguageModelingLoss,\r\n",
        "    TFMultipleChoiceLoss,\r\n",
        "    TFPreTrainedModel,\r\n",
        "    TFQuestionAnsweringLoss,\r\n",
        "    TFSequenceClassificationLoss,\r\n",
        "    TFTokenClassificationLoss,\r\n",
        "    get_initializer,\r\n",
        "    input_processing,\r\n",
        "    keras_serializable,\r\n",
        "    shape_list,\r\n",
        ")\r\n",
        "from transformers.utils import logging\r\n",
        "from transformers import RobertaConfig\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.get_logger(__name__)\r\n",
        "\r\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\r\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\r\n",
        "\r\n",
        "TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\r\n",
        "    \"roberta-base\",\r\n",
        "    \"roberta-large\",\r\n",
        "    \"roberta-large-mnli\",\r\n",
        "    \"distilroberta-base\",\r\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertWordEmbeddings\r\n",
        "class TFRobertaWordEmbeddings(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, initializer_range: float, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.weight = self.add_weight(\r\n",
        "            name=\"weight\",\r\n",
        "            shape=[self.vocab_size, self.hidden_size],\r\n",
        "            initializer=get_initializer(initializer_range=self.initializer_range),\r\n",
        "        )\r\n",
        "\r\n",
        "        super().build(input_shape=input_shape)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\r\n",
        "            \"vocab_size\": self.vocab_size,\r\n",
        "            \"hidden_size\": self.hidden_size,\r\n",
        "            \"initializer_range\": self.initializer_range,\r\n",
        "        }\r\n",
        "        base_config = super().get_config()\r\n",
        "\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, input_ids):\r\n",
        "        flat_input_ids = tf.reshape(tensor=input_ids, shape=[-1])\r\n",
        "        embeddings = tf.gather(params=self.weight, indices=flat_input_ids)\r\n",
        "        embeddings = tf.reshape(\r\n",
        "            tensor=embeddings, shape=tf.concat(values=[shape_list(tensor=input_ids), [self.hidden_size]], axis=0)\r\n",
        "        )\r\n",
        "\r\n",
        "        embeddings.set_shape(shape=input_ids.shape.as_list() + [self.hidden_size])\r\n",
        "\r\n",
        "        return embeddings\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertTokenTypeEmbeddings\r\n",
        "class TFRobertaTokenTypeEmbeddings(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, type_vocab_size: int, hidden_size: int, initializer_range: float, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.type_vocab_size = type_vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.token_type_embeddings = self.add_weight(\r\n",
        "            name=\"embeddings\",\r\n",
        "            shape=[self.type_vocab_size, self.hidden_size],\r\n",
        "            initializer=get_initializer(initializer_range=self.initializer_range),\r\n",
        "        )\r\n",
        "\r\n",
        "        super().build(input_shape=input_shape)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\r\n",
        "            \"type_vocab_size\": self.type_vocab_size,\r\n",
        "            \"hidden_size\": self.hidden_size,\r\n",
        "            \"initializer_range\": self.initializer_range,\r\n",
        "        }\r\n",
        "        base_config = super().get_config()\r\n",
        "\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, token_type_ids):\r\n",
        "        flat_token_type_ids = tf.reshape(tensor=token_type_ids, shape=[-1])\r\n",
        "        one_hot_data = tf.one_hot(indices=flat_token_type_ids, depth=self.type_vocab_size, dtype=self._compute_dtype)\r\n",
        "        embeddings = tf.matmul(a=one_hot_data, b=self.token_type_embeddings)\r\n",
        "        embeddings = tf.reshape(\r\n",
        "            tensor=embeddings, shape=tf.concat(values=[shape_list(tensor=token_type_ids), [self.hidden_size]], axis=0)\r\n",
        "        )\r\n",
        "\r\n",
        "        embeddings.set_shape(shape=token_type_ids.shape.as_list() + [self.hidden_size])\r\n",
        "\r\n",
        "        return embeddings\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.longformer.modeling_tf_longformer.TFLongformerPositionEmbeddings\r\n",
        "class TFRobertaPositionEmbeddings(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, max_position_embeddings: int, hidden_size: int, initializer_range: float, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.max_position_embeddings = max_position_embeddings\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.position_embeddings = self.add_weight(\r\n",
        "            name=\"embeddings\",\r\n",
        "            shape=[self.max_position_embeddings, self.hidden_size],\r\n",
        "            initializer=get_initializer(initializer_range=self.initializer_range),\r\n",
        "        )\r\n",
        "\r\n",
        "        super().build(input_shape)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\r\n",
        "            \"max_position_embeddings\": self.max_position_embeddings,\r\n",
        "            \"hidden_size\": self.hidden_size,\r\n",
        "            \"initializer_range\": self.initializer_range,\r\n",
        "        }\r\n",
        "        base_config = super().get_config()\r\n",
        "\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, position_ids):\r\n",
        "        flat_position_ids = tf.reshape(tensor=position_ids, shape=[-1])\r\n",
        "        embeddings = tf.gather(params=self.position_embeddings, indices=flat_position_ids)\r\n",
        "        embeddings = tf.reshape(\r\n",
        "            tensor=embeddings, shape=tf.concat(values=[shape_list(tensor=position_ids), [self.hidden_size]], axis=0)\r\n",
        "        )\r\n",
        "\r\n",
        "        embeddings.set_shape(shape=position_ids.shape.as_list() + [self.hidden_size])\r\n",
        "\r\n",
        "        return embeddings\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaEmbeddings(tf.keras.layers.Layer):\r\n",
        "    \"\"\"\r\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.padding_idx = 1\r\n",
        "        self.word_embeddings = TFRobertaWordEmbeddings(\r\n",
        "            vocab_size=config.vocab_size,\r\n",
        "            hidden_size=config.hidden_size,\r\n",
        "            initializer_range=config.initializer_range,\r\n",
        "            name=\"word_embeddings\",\r\n",
        "        )\r\n",
        "        self.position_embeddings = TFRobertaPositionEmbeddings(\r\n",
        "            max_position_embeddings=config.max_position_embeddings,\r\n",
        "            hidden_size=config.hidden_size,\r\n",
        "            initializer_range=config.initializer_range,\r\n",
        "            name=\"position_embeddings\",\r\n",
        "        )\r\n",
        "        self.token_type_embeddings = TFRobertaTokenTypeEmbeddings(\r\n",
        "            type_vocab_size=config.type_vocab_size,\r\n",
        "            hidden_size=config.hidden_size,\r\n",
        "            initializer_range=config.initializer_range,\r\n",
        "            name=\"token_type_embeddings\",\r\n",
        "        )\r\n",
        "        self.embeddings_sum = tf.keras.layers.Add()\r\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\r\n",
        "\r\n",
        "    def create_position_ids_from_input_ids(self, input_ids):\r\n",
        "        \"\"\"\r\n",
        "        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding\r\n",
        "        symbols are ignored. This is modified from fairseq's `utils.make_positions`.\r\n",
        "        Args:\r\n",
        "            input_ids: tf.Tensor\r\n",
        "        Returns: tf.Tensor\r\n",
        "        \"\"\"\r\n",
        "        input_ids_shape = shape_list(tensor=input_ids)\r\n",
        "\r\n",
        "        # multiple choice has 3 dimensions\r\n",
        "        if len(input_ids_shape) == 3:\r\n",
        "            input_ids = tf.reshape(\r\n",
        "                tensor=input_ids, shape=(input_ids_shape[0] * input_ids_shape[1], input_ids_shape[2])\r\n",
        "            )\r\n",
        "\r\n",
        "        mask = tf.cast(x=tf.math.not_equal(x=input_ids, y=self.padding_idx), dtype=input_ids.dtype)\r\n",
        "        incremental_indices = tf.math.cumsum(x=mask, axis=1) * mask\r\n",
        "\r\n",
        "        return incremental_indices + self.padding_idx\r\n",
        "\r\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\r\n",
        "        \"\"\"\r\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\r\n",
        "        Args:\r\n",
        "            inputs_embeds: tf.Tensor\r\n",
        "        Returns: tf.Tensor\r\n",
        "        \"\"\"\r\n",
        "        batch_size, seq_length = shape_list(tensor=inputs_embeds)[:2]\r\n",
        "        position_ids = tf.range(start=self.padding_idx + 1, limit=seq_length + self.padding_idx + 1)[tf.newaxis, :]\r\n",
        "\r\n",
        "        return tf.tile(input=position_ids, multiples=(batch_size, 1))\r\n",
        "\r\n",
        "    def call(self, input_ids=None, position_ids=None, token_type_ids=None, inputs_embeds=None, training=False):\r\n",
        "        \"\"\"\r\n",
        "        Applies embedding based on inputs tensor.\r\n",
        "        Returns:\r\n",
        "            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\r\n",
        "        \"\"\"\r\n",
        "        assert not (input_ids is None and inputs_embeds is None)\r\n",
        "\r\n",
        "        if input_ids is not None:\r\n",
        "            inputs_embeds = self.word_embeddings(input_ids=input_ids)\r\n",
        "\r\n",
        "        if token_type_ids is None:\r\n",
        "            input_shape = shape_list(tensor=inputs_embeds)[:-1]\r\n",
        "            token_type_ids = tf.fill(dims=input_shape, value=0)\r\n",
        "\r\n",
        "        if position_ids is None:\r\n",
        "            if input_ids is not None:\r\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\r\n",
        "                position_ids = self.create_position_ids_from_input_ids(input_ids=input_ids)\r\n",
        "            else:\r\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds=inputs_embeds)\r\n",
        "\r\n",
        "        position_embeds = self.position_embeddings(position_ids=position_ids)\r\n",
        "        token_type_embeds = self.token_type_embeddings(token_type_ids=token_type_ids)\r\n",
        "        final_embeddings = self.embeddings_sum(inputs=[inputs_embeds, position_embeds, token_type_embeds])\r\n",
        "        final_embeddings = self.LayerNorm(inputs=final_embeddings)\r\n",
        "        final_embeddings = self.dropout(inputs=final_embeddings, training=training)\r\n",
        "\r\n",
        "        return final_embeddings\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertPooler\r\n",
        "class TFRobertaPooler(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.Dense(\r\n",
        "            config.hidden_size,\r\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\r\n",
        "            activation=\"tanh\",\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\r\n",
        "        # to the first token.\r\n",
        "        first_token_tensor = hidden_states[:, 0]\r\n",
        "        pooled_output = self.dense(first_token_tensor)\r\n",
        "\r\n",
        "        return pooled_output\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfAttention with Bert->Roberta\r\n",
        "class TFRobertaSelfAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\r\n",
        "            raise ValueError(\r\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\r\n",
        "                f\"of attention heads ({config.num_attention_heads})\"\r\n",
        "            )\r\n",
        "\r\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n",
        "\r\n",
        "        self.query = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cde->abde\",\r\n",
        "            output_shape=(None, config.num_attention_heads, self.attention_head_size),\r\n",
        "            bias_axes=\"de\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"query\",\r\n",
        "        )\r\n",
        "        self.key = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cde->abde\",\r\n",
        "            output_shape=(None, config.num_attention_heads, self.attention_head_size),\r\n",
        "            bias_axes=\"de\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"key\",\r\n",
        "        )\r\n",
        "        self.value = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cde->abde\",\r\n",
        "            output_shape=(None, config.num_attention_heads, self.attention_head_size),\r\n",
        "            bias_axes=\"de\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"value\",\r\n",
        "        )\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)\r\n",
        "\r\n",
        "    def call(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False, training=False):\r\n",
        "        query_layer = self.query(inputs=hidden_states)\r\n",
        "        key_layer = self.key(inputs=hidden_states)\r\n",
        "        value_layer = self.value(inputs=hidden_states)\r\n",
        "\r\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw\r\n",
        "        # attention scores.\r\n",
        "        dk = tf.cast(x=self.attention_head_size, dtype=query_layer.dtype)\r\n",
        "        query_layer = tf.multiply(x=query_layer, y=tf.math.rsqrt(x=dk))\r\n",
        "        attention_scores = tf.einsum(\"aecd,abcd->acbe\", key_layer, query_layer)\r\n",
        "\r\n",
        "        if attention_mask is not None:\r\n",
        "            # Apply the attention mask is (precomputed for all layers in TFRobertaModel call() function)\r\n",
        "            attention_scores = attention_scores + attention_mask\r\n",
        "\r\n",
        "        # Normalize the attention scores to probabilities.\r\n",
        "        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)\r\n",
        "\r\n",
        "        # This is actually dropping out entire tokens to attend to, which might\r\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\r\n",
        "        attention_probs = self.dropout(attention_probs, training=training)\r\n",
        "\r\n",
        "        # Mask heads if we want to\r\n",
        "        if head_mask is not None:\r\n",
        "            attention_scores = attention_scores * head_mask\r\n",
        "\r\n",
        "        attention_output = tf.einsum(\"acbe,aecd->abcd\", attention_probs, value_layer)\r\n",
        "        outputs = (attention_output, attention_probs) if output_attentions else (attention_output,)\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfOutput\r\n",
        "class TFRobertaSelfOutput(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\r\n",
        "            raise ValueError(\r\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\r\n",
        "                f\"of attention heads ({config.num_attention_heads})\"\r\n",
        "            )\r\n",
        "\r\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n",
        "        self.all_head_size = config.num_attention_heads * self.attention_head_size\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abcd,cde->abe\",\r\n",
        "            output_shape=(None, self.all_head_size),\r\n",
        "            bias_axes=\"e\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\r\n",
        "\r\n",
        "    def call(self, hidden_states, input_tensor, training=False):\r\n",
        "        hidden_states = self.dense(inputs=hidden_states)\r\n",
        "        hidden_states = self.dropout(inputs=hidden_states, training=training)\r\n",
        "        hidden_states = self.LayerNorm(inputs=hidden_states + input_tensor)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertAttention with Bert->Roberta\r\n",
        "class TFRobertaAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        #self.self_attention = TFRobertaSelfAttention(config, name=\"self\")\r\n",
        "        \r\n",
        "        # Either merge Performer Config w/ normal config or enable choosing Performer\r\n",
        "        # Currently manually add performer == True to config\r\n",
        "        self.perf_attn = config.performer\r\n",
        "        if self.perf_attn == True:\r\n",
        "\r\n",
        "            self.num_attention_heads = config.num_attention_heads\r\n",
        "\r\n",
        "            performer_config = PerformerAttentionConfig(\r\n",
        "                                num_heads=config.num_attention_heads,\r\n",
        "                                d_model=config.hidden_size,\r\n",
        "                                kernel_type='exp',\r\n",
        "                                num_random_features=300,\r\n",
        "                                use_linear_layers=False\r\n",
        "                                )\r\n",
        "\r\n",
        "            self.self_attention = TFPerformerAttention(performer_config, name=\"self\")\r\n",
        "\r\n",
        "        else: \r\n",
        "            self.self_attention = TFRobertaSelfAttention(config, name=\"self\")\r\n",
        "\r\n",
        "\r\n",
        "        self.dense_output = TFRobertaSelfOutput(config, name=\"output\")\r\n",
        "\r\n",
        "    def prune_heads(self, heads):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\r\n",
        "\r\n",
        "\r\n",
        "        if self.perf_attn:\r\n",
        "            self_outputs = self.self_attention(\r\n",
        "                input_tensor, input_tensor, input_tensor, attention_mask, head_mask, output_attentions\r\n",
        "            )\r\n",
        "\r\n",
        "            # Reshape to (bs, q_length, num_h, rest)\r\n",
        "            self_outputs = list(self_outputs)\r\n",
        "            self_outputs[0] = tf.reshape(self_outputs[0], shape_list(self_outputs[0])[:2] + [self.num_attention_heads] + [-1])\r\n",
        "            self_outputs = tuple(self_outputs)\r\n",
        "        \r\n",
        "        else:\r\n",
        "            self_outputs = self.self_attention(\r\n",
        "            input_tensor, attention_mask, head_mask, output_attentions, training=training\r\n",
        "            )\r\n",
        "\r\n",
        "\r\n",
        "        attention_output = self.dense_output(self_outputs[0], input_tensor, training=training)\r\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertIntermediate\r\n",
        "class TFRobertaIntermediate(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cd->abd\",\r\n",
        "            output_shape=(None, config.intermediate_size),\r\n",
        "            bias_axes=\"d\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "\r\n",
        "        if isinstance(config.hidden_act, str):\r\n",
        "            self.intermediate_act_fn = get_tf_activation(activation_string=config.hidden_act)\r\n",
        "        else:\r\n",
        "            self.intermediate_act_fn = config.hidden_act\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        hidden_states = self.dense(inputs=hidden_states)\r\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertOutput\r\n",
        "class TFRobertaOutput(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cd->abd\",\r\n",
        "            bias_axes=\"d\",\r\n",
        "            output_shape=(None, config.hidden_size),\r\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\r\n",
        "\r\n",
        "    def call(self, hidden_states, input_tensor, training=False):\r\n",
        "        hidden_states = self.dense(inputs=hidden_states)\r\n",
        "        hidden_states = self.dropout(inputs=hidden_states, training=training)\r\n",
        "        hidden_states = self.LayerNorm(inputs=hidden_states + input_tensor)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertLayer with Bert->Roberta\r\n",
        "class TFRobertaLayer(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.attention = TFRobertaAttention(config, name=\"attention\")\r\n",
        "        self.intermediate = TFRobertaIntermediate(config, name=\"intermediate\")\r\n",
        "        self.bert_output = TFRobertaOutput(config, name=\"output\")\r\n",
        "\r\n",
        "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\r\n",
        "        attention_outputs = self.attention(\r\n",
        "            hidden_states, attention_mask, head_mask, output_attentions, training=training\r\n",
        "        )\r\n",
        "        attention_output = attention_outputs[0]\r\n",
        "        intermediate_output = self.intermediate(attention_output)\r\n",
        "        layer_output = self.bert_output(intermediate_output, attention_output, training=training)\r\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertEncoder with Bert->Roberta\r\n",
        "class TFRobertaEncoder(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.layer = [TFRobertaLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\r\n",
        "\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        hidden_states,\r\n",
        "        attention_mask,\r\n",
        "        head_mask,\r\n",
        "        output_attentions,\r\n",
        "        output_hidden_states,\r\n",
        "        return_dict,\r\n",
        "        training=False,\r\n",
        "    ):\r\n",
        "        all_hidden_states = () if output_hidden_states else None\r\n",
        "        all_attentions = () if output_attentions else None\r\n",
        "\r\n",
        "        for i, layer_module in enumerate(self.layer):\r\n",
        "            if output_hidden_states:\r\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\r\n",
        "\r\n",
        "            layer_outputs = layer_module(\r\n",
        "                hidden_states, attention_mask, head_mask[i], output_attentions, training=training\r\n",
        "            )\r\n",
        "            hidden_states = layer_outputs[0]\r\n",
        "\r\n",
        "            if output_attentions:\r\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\r\n",
        "\r\n",
        "        # Add last layer\r\n",
        "        if output_hidden_states:\r\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\r\n",
        "\r\n",
        "        if not return_dict:\r\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\r\n",
        "\r\n",
        "        return TFBaseModelOutput(\r\n",
        "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "@keras_serializable\r\n",
        "class TFRobertaMainLayer(tf.keras.layers.Layer):\r\n",
        "    config_class = RobertaConfig\r\n",
        "\r\n",
        "    def __init__(self, config, add_pooling_layer=True, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.config = config\r\n",
        "        self.num_hidden_layers = config.num_hidden_layers\r\n",
        "        self.initializer_range = config.initializer_range\r\n",
        "        self.output_attentions = config.output_attentions\r\n",
        "        self.output_hidden_states = config.output_hidden_states\r\n",
        "        self.return_dict = config.use_return_dict\r\n",
        "        self.encoder = TFRobertaEncoder(config, name=\"encoder\")\r\n",
        "        self.pooler = TFRobertaPooler(config, name=\"pooler\") if add_pooling_layer else None\r\n",
        "        # The embeddings must be the last declaration in order to follow the weights order\r\n",
        "        self.embeddings = TFRobertaEmbeddings(config, name=\"embeddings\")\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer.get_input_embeddings\r\n",
        "    def get_input_embeddings(self):\r\n",
        "        return self.embeddings.word_embeddings\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer.set_input_embeddings\r\n",
        "    def set_input_embeddings(self, value):\r\n",
        "        self.embeddings.word_embeddings.weight = value\r\n",
        "        self.embeddings.word_embeddings.vocab_size = shape_list(value)[0]\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer._prune_heads\r\n",
        "    def _prune_heads(self, heads_to_prune):\r\n",
        "        \"\"\"\r\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\r\n",
        "        class PreTrainedModel\r\n",
        "        \"\"\"\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer.call\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "\r\n",
        "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\r\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\r\n",
        "        elif inputs[\"input_ids\"] is not None:\r\n",
        "            input_shape = shape_list(inputs[\"input_ids\"])\r\n",
        "        elif inputs[\"inputs_embeds\"] is not None:\r\n",
        "            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\r\n",
        "        else:\r\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\r\n",
        "\r\n",
        "        if inputs[\"attention_mask\"] is None:\r\n",
        "            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)\r\n",
        "\r\n",
        "        if inputs[\"token_type_ids\"] is None:\r\n",
        "            inputs[\"token_type_ids\"] = tf.fill(input_shape, 0)\r\n",
        "\r\n",
        "        embedding_output = self.embeddings(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            inputs[\"position_ids\"],\r\n",
        "            inputs[\"token_type_ids\"],\r\n",
        "            inputs[\"inputs_embeds\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\r\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\r\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\r\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\r\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\r\n",
        "        extended_attention_mask = inputs[\"attention_mask\"][:, tf.newaxis, tf.newaxis, :]\r\n",
        "\r\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\r\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\r\n",
        "        # positions we want to attend and -10000.0 for masked positions.\r\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\r\n",
        "        # effectively the same as removing these entirely.\r\n",
        "        extended_attention_mask = tf.cast(extended_attention_mask, embedding_output.dtype)\r\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\r\n",
        "\r\n",
        "        # Prepare head mask if needed\r\n",
        "        # 1.0 in head_mask indicate we keep the head\r\n",
        "        # attention_probs has shape bsz x n_heads x N x N\r\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\r\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\r\n",
        "        if inputs[\"head_mask\"] is not None:\r\n",
        "            raise NotImplementedError\r\n",
        "        else:\r\n",
        "            inputs[\"head_mask\"] = [None] * self.num_hidden_layers\r\n",
        "            # head_mask = tf.constant([0] * self.num_hidden_layers)\r\n",
        "\r\n",
        "        encoder_outputs = self.encoder(\r\n",
        "            embedding_output,\r\n",
        "            extended_attention_mask,\r\n",
        "            inputs[\"head_mask\"],\r\n",
        "            inputs[\"output_attentions\"],\r\n",
        "            inputs[\"output_hidden_states\"],\r\n",
        "            inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        sequence_output = encoder_outputs[0]\r\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            return (\r\n",
        "                sequence_output,\r\n",
        "                pooled_output,\r\n",
        "            ) + encoder_outputs[1:]\r\n",
        "\r\n",
        "        return TFBaseModelOutputWithPooling(\r\n",
        "            last_hidden_state=sequence_output,\r\n",
        "            pooler_output=pooled_output,\r\n",
        "            hidden_states=encoder_outputs.hidden_states,\r\n",
        "            attentions=encoder_outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaPreTrainedModel(TFPreTrainedModel):\r\n",
        "    \"\"\"\r\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\r\n",
        "    models.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    config_class = RobertaConfig\r\n",
        "    base_model_prefix = \"roberta\"\r\n",
        "\r\n",
        "    @tf.function(\r\n",
        "        input_signature=[\r\n",
        "            {\r\n",
        "                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\r\n",
        "                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\r\n",
        "            }\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    def serving(self, inputs):\r\n",
        "        output = self.call(inputs)\r\n",
        "\r\n",
        "        return self.serving_output(output)\r\n",
        "\r\n",
        "\r\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\r\n",
        "    This model inherits from :class:`~transformers.TFPreTrainedModel`. Check the superclass documentation for the\r\n",
        "    generic methods the library implements for all its model (such as downloading or saving, resizing the input\r\n",
        "    embeddings, pruning heads etc.)\r\n",
        "    This model is also a `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. Use\r\n",
        "    it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage\r\n",
        "    and behavior.\r\n",
        "    .. note::\r\n",
        "        TF 2.0 models accepts two formats as inputs:\r\n",
        "        - having all inputs as keyword arguments (like PyTorch models), or\r\n",
        "        - having all inputs as a list, tuple or dict in the first positional arguments.\r\n",
        "        This second option is useful when using :meth:`tf.keras.Model.fit` method which currently requires having all\r\n",
        "        the tensors in the first argument of the model call function: :obj:`model(inputs)`.\r\n",
        "        If you choose this second option, there are three possibilities you can use to gather all the input Tensors in\r\n",
        "        the first positional argument :\r\n",
        "        - a single Tensor with :obj:`input_ids` only and nothing else: :obj:`model(inputs_ids)`\r\n",
        "        - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:\r\n",
        "          :obj:`model([input_ids, attention_mask])` or :obj:`model([input_ids, attention_mask, token_type_ids])`\r\n",
        "        - a dictionary with one or several input Tensors associated to the input names given in the docstring:\r\n",
        "          :obj:`model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids})`\r\n",
        "    Parameters:\r\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\r\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\r\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\r\n",
        "            weights.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\r\n",
        "    Args:\r\n",
        "        input_ids (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`):\r\n",
        "            Indices of input sequence tokens in the vocabulary.\r\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\r\n",
        "            :func:`transformers.PreTrainedTokenizer.__call__` and :func:`transformers.PreTrainedTokenizer.encode` for\r\n",
        "            details.\r\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\r\n",
        "        attention_mask (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`, `optional`):\r\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\r\n",
        "            - 1 for tokens that are **not masked**,\r\n",
        "            - 0 for tokens that are **masked**.\r\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\r\n",
        "        token_type_ids (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`, `optional`):\r\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\r\n",
        "            1]``:\r\n",
        "            - 0 corresponds to a `sentence A` token,\r\n",
        "            - 1 corresponds to a `sentence B` token.\r\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`__\r\n",
        "        position_ids (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`, `optional`):\r\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\r\n",
        "            config.max_position_embeddings - 1]``.\r\n",
        "            `What are position IDs? <../glossary.html#position-ids>`__\r\n",
        "        head_mask (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\r\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\r\n",
        "            - 1 indicates the head is **not masked**,\r\n",
        "            - 0 indicates the head is **masked**.\r\n",
        "        inputs_embeds (:obj:`tf.Tensor` of shape :obj:`({0}, hidden_size)`, `optional`):\r\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\r\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\r\n",
        "            vectors than the model's internal embedding lookup matrix.\r\n",
        "        output_attentions (:obj:`bool`, `optional`):\r\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\r\n",
        "            tensors for more detail.\r\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\r\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\r\n",
        "            more detail.\r\n",
        "        return_dict (:obj:`bool`, `optional`):\r\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\r\n",
        "        training (:obj:`bool`, `optional`, defaults to :obj:`False`):\r\n",
        "            Whether or not to use the model in training mode (some modules like dropout modules have different\r\n",
        "            behaviors between training and evaluation).\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaModel(TFRobertaPreTrainedModel):\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFBaseModelOutputWithPooling,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            input_ids=inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertModel.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFBaseModelOutputWithPooling(\r\n",
        "            last_hidden_state=output.last_hidden_state,\r\n",
        "            pooler_output=output.pooler_output,\r\n",
        "            hidden_states=hs,\r\n",
        "            attentions=attns,\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaLMHead(tf.keras.layers.Layer):\r\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, config, input_embeddings, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = config.vocab_size\r\n",
        "        self.hidden_size = config.hidden_size\r\n",
        "        self.dense = tf.keras.layers.Dense(\r\n",
        "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n",
        "        )\r\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\r\n",
        "        self.act = get_tf_activation(\"gelu\")\r\n",
        "\r\n",
        "        # The output weights are the same as the input embeddings, but there is\r\n",
        "        # an output-only bias for each token.\r\n",
        "        self.decoder = input_embeddings\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\r\n",
        "\r\n",
        "        super().build(input_shape)\r\n",
        "\r\n",
        "    def get_output_embeddings(self):\r\n",
        "        return self.decoder\r\n",
        "\r\n",
        "    def set_output_embeddings(self, value):\r\n",
        "        self.decoder.weight = value\r\n",
        "        self.decoder.vocab_size = shape_list(value)[0]\r\n",
        "\r\n",
        "    def get_bias(self):\r\n",
        "        return {\"bias\": self.bias}\r\n",
        "\r\n",
        "    def set_bias(self, value):\r\n",
        "        self.bias = value[\"bias\"]\r\n",
        "        self.vocab_size = shape_list(value[\"bias\"])[0]\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        hidden_states = self.dense(hidden_states)\r\n",
        "        hidden_states = self.act(hidden_states)\r\n",
        "        hidden_states = self.layer_norm(hidden_states)\r\n",
        "\r\n",
        "        # project back to size of vocabulary with bias\r\n",
        "        seq_length = shape_list(tensor=hidden_states)[1]\r\n",
        "        hidden_states = tf.reshape(tensor=hidden_states, shape=[-1, self.hidden_size])\r\n",
        "        hidden_states = tf.matmul(a=hidden_states, b=self.decoder.weight, transpose_b=True)\r\n",
        "        hidden_states = tf.reshape(tensor=hidden_states, shape=[-1, seq_length, self.vocab_size])\r\n",
        "        hidden_states = tf.nn.bias_add(value=hidden_states, bias=self.bias)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\r\n",
        "class TFRobertaForMaskedLM(TFRobertaPreTrainedModel, TFMaskedLanguageModelingLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head.decoder.weight\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.lm_head = TFRobertaLMHead(config, self.roberta.embeddings.word_embeddings, name=\"lm_head\")\r\n",
        "\r\n",
        "    def get_lm_head(self):\r\n",
        "        return self.lm_head\r\n",
        "\r\n",
        "    def get_prefix_bias_name(self):\r\n",
        "        warnings.warn(\"The method get_prefix_bias_name is deprecated. Please use `get_bias` instead.\", FutureWarning)\r\n",
        "        return self.name + \"/\" + self.lm_head.name\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFMaskedLMOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\r\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\r\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\r\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        sequence_output = outputs[0]\r\n",
        "        prediction_scores = self.lm_head(sequence_output)\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], prediction_scores)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (prediction_scores,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFMaskedLMOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=prediction_scores,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFMaskedLMOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaClassificationHead(tf.keras.layers.Layer):\r\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.dense = tf.keras.layers.Dense(\r\n",
        "            config.hidden_size,\r\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\r\n",
        "            activation=\"tanh\",\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.out_proj = tf.keras.layers.Dense(\r\n",
        "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"out_proj\"\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, features, training=False):\r\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\r\n",
        "        x = self.dropout(x, training=training)\r\n",
        "        x = self.dense(x)\r\n",
        "        x = self.dropout(x, training=training)\r\n",
        "        x = self.out_proj(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\r\n",
        "    pooled output) e.g. for GLUE tasks.\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForSequenceClassification(TFRobertaPreTrainedModel, TFSequenceClassificationLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.classifier = TFRobertaClassificationHead(config, name=\"classifier\")\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFSequenceClassifierOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\r\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\r\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        sequence_output = outputs[0]\r\n",
        "        logits = self.classifier(sequence_output, training=inputs[\"training\"])\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (logits,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFSequenceClassifierOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFSequenceClassifierOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\r\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForMultipleChoice(TFRobertaPreTrainedModel, TFMultipleChoiceLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"lm_head\"]\r\n",
        "    _keys_to_ignore_on_load_missing = [r\"dropout\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\r\n",
        "        )\r\n",
        "\r\n",
        "    @property\r\n",
        "    def dummy_inputs(self):\r\n",
        "        \"\"\"\r\n",
        "        Dummy inputs to build the network.\r\n",
        "        Returns:\r\n",
        "            tf.Tensor with dummy inputs\r\n",
        "        \"\"\"\r\n",
        "        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFMultipleChoiceModelOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\r\n",
        "            num_choices]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\r\n",
        "            :obj:`input_ids` above)\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "\r\n",
        "        if inputs[\"input_ids\"] is not None:\r\n",
        "            num_choices = shape_list(inputs[\"input_ids\"])[1]\r\n",
        "            seq_length = shape_list(inputs[\"input_ids\"])[2]\r\n",
        "        else:\r\n",
        "            num_choices = shape_list(inputs_embeds)[1]\r\n",
        "            seq_length = shape_list(inputs_embeds)[2]\r\n",
        "\r\n",
        "        flat_input_ids = tf.reshape(inputs[\"input_ids\"], (-1, seq_length)) if inputs[\"input_ids\"] is not None else None\r\n",
        "        flat_attention_mask = (\r\n",
        "            tf.reshape(inputs[\"attention_mask\"], (-1, seq_length)) if inputs[\"attention_mask\"] is not None else None\r\n",
        "        )\r\n",
        "        flat_token_type_ids = (\r\n",
        "            tf.reshape(inputs[\"token_type_ids\"], (-1, seq_length)) if inputs[\"token_type_ids\"] is not None else None\r\n",
        "        )\r\n",
        "        flat_position_ids = (\r\n",
        "            tf.reshape(inputs[\"position_ids\"], (-1, seq_length)) if inputs[\"position_ids\"] is not None else None\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            flat_input_ids,\r\n",
        "            flat_attention_mask,\r\n",
        "            flat_token_type_ids,\r\n",
        "            flat_position_ids,\r\n",
        "            inputs[\"head_mask\"],\r\n",
        "            inputs[\"inputs_embeds\"],\r\n",
        "            inputs[\"output_attentions\"],\r\n",
        "            inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        pooled_output = outputs[1]\r\n",
        "        pooled_output = self.dropout(pooled_output, training=inputs[\"training\"])\r\n",
        "        logits = self.classifier(pooled_output)\r\n",
        "        reshaped_logits = tf.reshape(logits, (-1, num_choices))\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], reshaped_logits)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (reshaped_logits,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFMultipleChoiceModelOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=reshaped_logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    @tf.function(\r\n",
        "        input_signature=[\r\n",
        "            {\r\n",
        "                \"input_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"input_ids\"),\r\n",
        "                \"attention_mask\": tf.TensorSpec((None, None, None), tf.int32, name=\"attention_mask\"),\r\n",
        "            }\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    def serving(self, inputs):\r\n",
        "        output = self.call(inputs)\r\n",
        "\r\n",
        "        return self.serving_output(output)\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFMultipleChoiceModelOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\r\n",
        "    Named-Entity-Recognition (NER) tasks.\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForTokenClassification(TFRobertaPreTrainedModel, TFTokenClassificationLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\r\n",
        "    _keys_to_ignore_on_load_missing = [r\"dropout\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\r\n",
        "        )\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFTokenClassifierOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\r\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\r\n",
        "            1]``.\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        sequence_output = outputs[0]\r\n",
        "\r\n",
        "        sequence_output = self.dropout(sequence_output, training=inputs[\"training\"])\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (logits,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFTokenClassifierOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFTokenClassifierOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    RoBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\r\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForQuestionAnswering(TFRobertaPreTrainedModel, TFQuestionAnsweringLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.qa_outputs = tf.keras.layers.Dense(\r\n",
        "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\r\n",
        "        )\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFQuestionAnsweringModelOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        start_positions=None,\r\n",
        "        end_positions=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        start_positions (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\r\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\r\n",
        "            sequence are not taken into account for computing the loss.\r\n",
        "        end_positions (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\r\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\r\n",
        "            sequence are not taken into account for computing the loss.\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            start_positions=start_positions,\r\n",
        "            end_positions=end_positions,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        sequence_output = outputs[0]\r\n",
        "\r\n",
        "        logits = self.qa_outputs(sequence_output)\r\n",
        "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\r\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\r\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\r\n",
        "\r\n",
        "        loss = None\r\n",
        "        if inputs[\"start_positions\"] is not None and inputs[\"end_positions\"] is not None:\r\n",
        "            labels = {\"start_position\": inputs[\"start_positions\"]}\r\n",
        "            labels[\"end_position\"] = inputs[\"end_positions\"]\r\n",
        "            loss = self.compute_loss(labels, (start_logits, end_logits))\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (start_logits, end_logits) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFQuestionAnsweringModelOutput(\r\n",
        "            loss=loss,\r\n",
        "            start_logits=start_logits,\r\n",
        "            end_logits=end_logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFQuestionAnsweringModelOutput(\r\n",
        "            start_logits=output.start_logits, end_logits=output.end_logits, hidden_states=hs, attentions=attns\r\n",
        "        )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c5d1381270b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_serializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTFRobertaMainLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mkeras_serializable\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_serializable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"register_keras_serializable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_keras_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    152\u001b[0m       raise ValueError(\n\u001b[1;32m    153\u001b[0m           \u001b[0;34m'%s has already been registered to %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m           (registered_name, _GLOBAL_CUSTOM_OBJECTS[registered_name]))\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GLOBAL_CUSTOM_NAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Custom>TFRobertaMainLayer has already been registered to <class '__main__.TFRobertaMainLayer'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sdQWaoYTiDm"
      },
      "source": [
        "### Data & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldw2uXlYTjez",
        "outputId": "67079848-ba01-423b-c7f9-bf000512ba57"
      },
      "source": [
        "from datasets import load_dataset\r\n",
        "\r\n",
        "train_ds = load_dataset('c3', 'mixed', split='train')\r\n",
        "\r\n",
        "print(\"Example: \", next(iter(train_ds)))\r\n",
        "\r\n",
        "def prepare_dict(example):\r\n",
        "    example[\"text\"] = example[\"documents\"][0]\r\n",
        "    return example\r\n",
        "\r\n",
        "train_ds = train_ds.map(prepare_dict)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset c3 (/root/.cache/huggingface/datasets/c3/mixed/1.0.0/6bcfb26ae1bd77bd57d300c2504900834cc29aaa092eb87c4d91d7960a3c2d8c)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/c3/mixed/1.0.0/6bcfb26ae1bd77bd57d300c2504900834cc29aaa092eb87c4d91d7960a3c2d8c/cache-63135c94bbd0d9ad.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Example:  {'document_id': 'm13-70', 'documents': ['许多动物的某些器官感觉特别灵敏，它们能比人类提前知道一些灾害事件的发生，例如，海洋中的水母能预报风暴，老鼠能事先躲避矿井崩塌或有害气体，等等。地震往往能使一些动物的某些感觉器官受到刺激而发生异常反应。如一个地区的重力发生变异，某些动物可能通过它们的平衡器官感觉到；一种振动异常，某些动物的听觉器官也许能够察觉出来。地震前地下岩层早已在逐日缓慢活动，而断层面之间又具有强大的摩擦力。这种摩擦力会产生一种低于人的听觉所能感觉到的低频声波。人对每秒20次以上的声波才能感觉到，而动物则不然。那些感觉十分灵敏的动物，在感触到这种低声波时，便会惊恐万状，以至出现冬蛇出洞、鱼跃水面等异常现象。'], 'questions': {'answer': ['比人的灵敏', '水母', '20次以上', '害怕'], 'choice': [['没有人的灵敏', '和人的差不多', '和人的一样好', '比人的灵敏'], ['蛇', '老鼠', '水母', '鱼'], ['20次', '20次以上', '20次以下', '以上都对'], ['兴奋', '逃跑', '跳跃', '害怕']], 'question': ['动物的器官感觉与人的相比有什么不同?', '录音中提到能预报风暴的动物是什么?', '低频声波至少要达到每秒多少次才能被人感觉到?', '动物感觉到低频声波时会有怎样的表现?']}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "54ea54fd67e0408c82d0811d2346fbc7",
            "46ab8c81fc244aee9743d884ec920eda",
            "324486834e594012846856a2b9487cd1",
            "7679593b31d44523825c5ee9ed01e4b6",
            "6e2d6467414045f0bd0c209966ab76b2",
            "c535869add4b4e4e91eb17c154e657a0",
            "a3cb9332cf1c4dbaa8e25ad6aba73886",
            "1fd9c3ea003d49f886ebbe452c9fa210"
          ]
        },
        "id": "kXy-89ivTzV1",
        "outputId": "939a0016-18aa-4b00-d92b-6c1407d8ab75"
      },
      "source": [
        "# Check Average Length - Not a \"performer optimized dataset for now\"\r\n",
        "class AverageLen():\r\n",
        "    def __init__(self):\r\n",
        "        self.avg_len = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "        self.last_examples = ''\r\n",
        "    \r\n",
        "    def concat(self, example):\r\n",
        "        self.last_examples += example[\"text\"]\r\n",
        "        if len(self.last_examples) > (4096*2):\r\n",
        "            example[\"text\"] = self.last_examples\r\n",
        "            self.last_examples = ''\r\n",
        "            return True\r\n",
        "        else:\r\n",
        "            return False\r\n",
        "\r\n",
        "    def filter(self, example):\r\n",
        "        if example[\"text\"] == '':\r\n",
        "            return False\r\n",
        "        return True\r\n",
        "\r\n",
        "    def average_length(self, example):\r\n",
        "        self.avg_len += len(example[\"text\"])\r\n",
        "        self.count += 1\r\n",
        "\r\n",
        "    def return_avg(self):\r\n",
        "        return self.avg_len / self.count\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "avg_len = AverageLen()\r\n",
        "# train_ds = train_ds.filter(avg_len.concat) # This turns it into a long sequence dataset, too high on memory for 1 GPU\r\n",
        "train_ds.map(avg_len.average_length)\r\n",
        "avg_len.return_avg()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54ea54fd67e0408c82d0811d2346fbc7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3138.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180.02994584262504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eedf79eee9554a9f9d446f190a8ff8a0",
            "4669decc3a0948b2a33c0d7122c63f24",
            "85d157c18cc34ef68bcdc19f9cc97faf",
            "c3aafdc18a35434a8c5761e66b28be5f",
            "5d288de1d22b4036bc7154403ee6a2c9",
            "9785b438d9284a4d97a19f21367049dc",
            "a45b1fb82e4c4694b94e4189f12d39d7",
            "7fcb946afaf3421cbc5d2f164577ee85"
          ]
        },
        "id": "W4AXpzxaT3HE",
        "outputId": "cedb9ea6-076c-4dbe-f250-d0f6b2a5855a"
      },
      "source": [
        "tokenizer = WWMTokenizer(col=\"text\", seq_len=512)\r\n",
        "\r\n",
        "train_ds = train_ds.map(tokenizer.tokenize_pretraining)\r\n",
        "\r\n",
        "train_ds = tokenizer.to_tf_dataset(train_ds)\r\n",
        "\r\n",
        "train_ds = train_ds.shuffle(1000).batch(1)\r\n",
        "\r\n",
        "print(\"Example: \", next(iter(train_ds)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eedf79eee9554a9f9d446f190a8ff8a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3138.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Example:  {'input_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
            "array([[  101,  3300,   671,   855,  1957,  3952, 16864,   103, 14277,\n",
            "          103,   103,  1961,  4638,  4415, 15739,   103,  3221,  3952,\n",
            "         6814,  1961, 13869,  1744, 15214,  3297,  2160,  4638,   671,\n",
            "         3340,  3736,   103,   103,   103,  6809, 14225,  6821, 13759,\n",
            "         4680, 16460,  8024,  1961,   679, 16228,  1765,  5298, 13796,\n",
            "         8024,   103,  2245,  1765,   711,  5632, 15403,  4638,  4415,\n",
            "        15739,  5445,  1939, 16216,   511,   676,  2399,  2678,   103,\n",
            "         8024,  1961, 15148,   103,  1114, 14963,  1962,   749,  8024,\n",
            "         1762,  6381, 18499,   103,  6225, 13887,  4638,  4867, 17943,\n",
            "        14955,   704,  8024,  1961,  1041, 17064, 19795,   103,  1765,\n",
            "         6663,   782,  3736,   704,  8024,  6813, 19919,  3308,  2190,\n",
            "        15336,  3952,  1343,   511,  1157,  2458, 15050,  3198,   103,\n",
            "         1921, 16755,  7478, 15439,   103,   103,   103,  3952,  2533,\n",
            "         2523, 10446,   103,   103,  2552, 15715,   738,  2523,   103,\n",
            "          103,   511,   852, 16278,  1762, 17759,  6632, 16398, 19689,\n",
            "         2970, 19875,   103, 16460,  4638,  3198, 14009,  8024,  3736,\n",
            "          677,  4960, 17254,  6629,   749,  1920, 20500,  8024,  5445,\n",
            "        13741,  6632,   103,   103,  3849,  8024,  6814,   749,   671,\n",
            "        13890, 14093,  8024,   103,   103,   103,  1168,   749,   847,\n",
            "        15854,   679, 19281,   758, 15957,  4638,   103,   103,   511,\n",
            "         6821,   855,  6817, 14277, 14504,  1762,  3736,   704,  2130,\n",
            "        14116,  1927, 14400,   749,  3175, 14460, 15754,  8024,  1961,\n",
            "          103,  4802, 15194,  6820, 19263,  3952,  1914,  6823,  2798,\n",
            "         5543,   103,  2279, 19861,   511,  1961,  6632,   103,   103,\n",
            "          103, 15859,  8024,  6632, 18193,  6632,  4558, 14284,   103,\n",
            "         3297, 14457,  8024,  1961,  5303, 13811,  3123, 15518,   103,\n",
            "         8024,  4260,   677,   749,   671, 17741,  1762,  1961,  6716,\n",
            "        19861,   924, 15901,  1961,  4638,  5670,   511,  6821, 16255,\n",
            "         8024,  1920, 20500,  3933, 16990,  6814, 14400,   749,  8024,\n",
            "          103,  1777,  1762,  5670, 13734,  8024,  1355, 17442,  1071,\n",
            "        15198, 18182,  4895,  2279, 19861,  1372,  3300,   671, 17693,\n",
            "         1914,  5101,   749,   511,  1920, 15214,  6963,  6230, 15590,\n",
            "          103,   103,  1377, 15724,  8024,  1961,  5632, 15403,   738,\n",
            "         6230, 15590,  7478, 15439,  6890, 15799,   511,  1961,  2190,\n",
            "         6381, 18499,  6432,  8038,   100,  2769,   679,  3221,  2682,\n",
            "          711,  5632, 15403,  2823,   955, 14423,  8024,   852, 16278,\n",
            "         1963, 16419,  2769,  4761, 19944,  2769,  6655, 17952,  4680,\n",
            "        16460,  1372,   103,   671, 17693,  1914,  5101,  4638,  6413,\n",
            "         8024,  2769,  3221,   103,   103,  1377, 13866,  1780, 15955,\n",
            "         1168,  3297, 14457,  4638,   511,   100,  1762,  6878, 14225,\n",
            "         1737, 20467,  4638,  6281,   103,  8024,  2769, 13869,  2382,\n",
            "        15439,   833,  3123, 15518,  8024,   103,  2769, 13869,   679,\n",
            "          103,   103,  4638,  3221,  8024,  1071, 15198,  1372, 19263,\n",
            "         1086,  1780, 15955,   103,   103,  8024, 16603,  5543,  5815,\n",
            "         2533,  2768, 14273,   511,  2768, 14273,  3300,  3198, 14009,\n",
            "          103,  1762,  1184, 20538,  8024,  4895,  2769, 13869,  1372,\n",
            "         3300,   671, 17693,  5101,  4638,  6655, 17952,   103,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0]], dtype=int32)>, 'lm_label_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
            "array([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  6817, 14277,\n",
            "        14504,  8024,  -100,  -100,  -100,  -100,  2218,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,   511,   711, 13806,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,   679, 16228,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   722, 14457,\n",
            "         -100,  -100,  5303, 13811,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  1469,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   928, 15609,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  8024,\n",
            "         -100,  -100,  -100,  -100,  1962,  8024,  1961,  -100,  -100,\n",
            "         -100,  3123, 16408,  8024,  -100,  -100,  -100,  -100,  2690,\n",
            "        15628,  -100,  -100,  -100,  -100,  1961,  -100,  -100,  -100,\n",
            "         -100,  -100,  4680, 16460,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  6632, 16398, 19689,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  1126, 13782,  2347,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  4923, 15485,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          679,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  1168,  -100,  -100,  -100,  -100,  -100,  3952,  6632,\n",
            "         2154, 15643,  -100,  -100,  3952,  -100,  -100,  -100,   511,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   749,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         1961,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  1961,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         7478, 15439,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  1197,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,   671, 15194,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  3198, 14009,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  5445,  -100,  -100,  -100,\n",
            "         4761, 19944,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,   671, 13735,  -100,  2218,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         2218,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   511,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],\n",
            "      dtype=int32)>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7QmTERGUREm",
        "outputId": "918f9ce5-0a4d-4373-bd94-e95dc93f1a3c"
      },
      "source": [
        "### Modelling - Eager ###\r\n",
        "\r\n",
        "from transformers import RobertaConfig\r\n",
        "\r\n",
        "learning_rate = 1e-5\r\n",
        "\r\n",
        "# Borrow Vocab.txt from https://huggingface.co/hfl/chinese-roberta-wwm-ext\r\n",
        "config = RobertaConfig.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")    #(\"bert-base-chinese\")\r\n",
        "config.performer = True\r\n",
        "model = TFRobertaForMaskedLM(config)\r\n",
        "print(config)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
        "\r\n",
        "loss_history_train = []\r\n",
        "loss_history_val = []\r\n",
        "\r\n",
        "def compute_loss(labels, logits):\r\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\r\n",
        "    )\r\n",
        "    # make sure only labels that are not equal to -100 do affect loss\r\n",
        "    active_loss = tf.not_equal(tf.reshape(labels, (-1,)), -100)\r\n",
        "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\r\n",
        "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\r\n",
        "\r\n",
        "    return loss_fn(labels, reduced_logits)\r\n",
        "\r\n",
        "def train_step(data):\r\n",
        "    y = data[\"lm_label_ids\"]\r\n",
        "    x = data\r\n",
        "\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "        # > Feeds it just into TFT5ForConditionalGeneration; training=True turns on dropout\r\n",
        "        outputs = model(x, training=True)\r\n",
        "        y_pred = outputs.logits\r\n",
        "        loss = compute_loss(y, y_pred)\r\n",
        "\r\n",
        "        # Reduce loss to single digit\r\n",
        "        loss = tf.reduce_mean(loss)\r\n",
        "\r\n",
        "    loss_history_train.append(loss.numpy().mean())\r\n",
        "    # Calculate grads & update\r\n",
        "    grads = tape.gradient(loss, model.trainable_variables)    \r\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
        "    \r\n",
        "def test_step(data):\r\n",
        "    y = data[\"lm_label_ids\"]\r\n",
        "    x = data\r\n",
        "    outputs = model(x, training=False)\r\n",
        "    y_pred = outputs.logits\r\n",
        "    loss = compute_loss(y, y_pred)\r\n",
        "    loss = tf.reduce_mean(loss)\r\n",
        "    loss_history_val.append(loss.numpy().mean())\r\n",
        "\r\n",
        "\r\n",
        "def train(epochs, steps=-1):\r\n",
        "    for epoch in range(epochs):\r\n",
        "        for batch, train in enumerate(train_ds):\r\n",
        "\r\n",
        "          train_step(train)\r\n",
        "\r\n",
        "          if batch % 100 == 0:\r\n",
        "              #test_step(val)\r\n",
        "              print('Batch {}, Last Train Loss {}'.format(batch, loss_history_train[-1])) #, loss_history_val[-1]))\r\n",
        "\r\n",
        "          if batch == steps:\r\n",
        "              break \r\n",
        "\r\n",
        "        print('Epoch {} finished'.format(epoch))\r\n",
        "\r\n",
        "train(epochs=10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"performer\": true,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "Batch 0, Last Train Loss 10.01305866241455\n",
            "Batch 100, Last Train Loss 8.065813064575195\n",
            "Batch 200, Last Train Loss 8.739216804504395\n",
            "Batch 300, Last Train Loss 8.386575698852539\n",
            "Batch 400, Last Train Loss 8.023459434509277\n",
            "Batch 500, Last Train Loss 6.782862186431885\n",
            "Batch 600, Last Train Loss 7.575124740600586\n",
            "Batch 700, Last Train Loss 6.286157608032227\n",
            "Batch 800, Last Train Loss 7.41764497756958\n",
            "Batch 900, Last Train Loss 8.807690620422363\n",
            "Batch 1000, Last Train Loss 7.3342485427856445\n",
            "Batch 1100, Last Train Loss 7.310985088348389\n",
            "Batch 1200, Last Train Loss 5.436051845550537\n",
            "Batch 1300, Last Train Loss 5.800329685211182\n",
            "Batch 1400, Last Train Loss 5.886070251464844\n",
            "Batch 1500, Last Train Loss 8.153827667236328\n",
            "Batch 1600, Last Train Loss 6.805720329284668\n",
            "Batch 1700, Last Train Loss 6.4014177322387695\n",
            "Batch 1800, Last Train Loss 6.278635501861572\n",
            "Batch 1900, Last Train Loss 7.073243618011475\n",
            "Batch 2000, Last Train Loss 6.612656593322754\n",
            "Batch 2100, Last Train Loss 3.7209370136260986\n",
            "Batch 2200, Last Train Loss 7.928494453430176\n",
            "Batch 2300, Last Train Loss 7.0428876876831055\n",
            "Batch 2400, Last Train Loss 7.773281574249268\n",
            "Batch 2500, Last Train Loss 6.553858757019043\n",
            "Batch 2600, Last Train Loss 6.574962615966797\n",
            "Batch 2700, Last Train Loss 6.4968366622924805\n",
            "Batch 2800, Last Train Loss 8.190140724182129\n",
            "Batch 2900, Last Train Loss 4.517754554748535\n",
            "Batch 3000, Last Train Loss 7.601536273956299\n",
            "Batch 3100, Last Train Loss 6.972167491912842\n",
            "Epoch 0 finished\n",
            "Batch 0, Last Train Loss 7.129791736602783\n",
            "Batch 100, Last Train Loss 7.738062381744385\n",
            "Batch 200, Last Train Loss 5.824728012084961\n",
            "Batch 300, Last Train Loss 6.404394626617432\n",
            "Batch 400, Last Train Loss 6.785905361175537\n",
            "Batch 500, Last Train Loss 6.721826553344727\n",
            "Batch 600, Last Train Loss 8.149370193481445\n",
            "Batch 700, Last Train Loss 7.879330158233643\n",
            "Batch 800, Last Train Loss 6.395676612854004\n",
            "Batch 900, Last Train Loss 6.1902055740356445\n",
            "Batch 1000, Last Train Loss 6.221255302429199\n",
            "Batch 1100, Last Train Loss 6.61215353012085\n",
            "Batch 1200, Last Train Loss 6.135186672210693\n",
            "Batch 1300, Last Train Loss 7.3285231590271\n",
            "Batch 1400, Last Train Loss 5.185935974121094\n",
            "Batch 1500, Last Train Loss 6.72980260848999\n",
            "Batch 1600, Last Train Loss 7.169397354125977\n",
            "Batch 1700, Last Train Loss 7.866446495056152\n",
            "Batch 1800, Last Train Loss 4.8522796630859375\n",
            "Batch 1900, Last Train Loss 7.183880805969238\n",
            "Batch 2000, Last Train Loss 6.342766761779785\n",
            "Batch 2100, Last Train Loss 5.3903093338012695\n",
            "Batch 2200, Last Train Loss 6.790374279022217\n",
            "Batch 2300, Last Train Loss 6.969010829925537\n",
            "Batch 2400, Last Train Loss 8.294821739196777\n",
            "Batch 2500, Last Train Loss 7.16201639175415\n",
            "Batch 2600, Last Train Loss 5.618669033050537\n",
            "Batch 2700, Last Train Loss 6.5617218017578125\n",
            "Batch 2800, Last Train Loss 5.439329624176025\n",
            "Batch 2900, Last Train Loss 6.263369083404541\n",
            "Batch 3000, Last Train Loss 7.893511772155762\n",
            "Batch 3100, Last Train Loss 7.307892322540283\n",
            "Epoch 1 finished\n",
            "Batch 0, Last Train Loss 6.308974742889404\n",
            "Batch 100, Last Train Loss 6.595973968505859\n",
            "Batch 200, Last Train Loss 6.508427619934082\n",
            "Batch 300, Last Train Loss 6.680193901062012\n",
            "Batch 400, Last Train Loss 5.442663192749023\n",
            "Batch 500, Last Train Loss 6.317714214324951\n",
            "Batch 600, Last Train Loss 6.173846244812012\n",
            "Batch 700, Last Train Loss 6.895950794219971\n",
            "Batch 800, Last Train Loss 7.232892990112305\n",
            "Batch 900, Last Train Loss 5.463114261627197\n",
            "Batch 1000, Last Train Loss 6.874776840209961\n",
            "Batch 1100, Last Train Loss 6.439493179321289\n",
            "Batch 1200, Last Train Loss 4.016664028167725\n",
            "Batch 1300, Last Train Loss 6.8528337478637695\n",
            "Batch 1400, Last Train Loss 6.347734451293945\n",
            "Batch 1500, Last Train Loss 7.2079949378967285\n",
            "Batch 1600, Last Train Loss 6.849777698516846\n",
            "Batch 1700, Last Train Loss 5.856703281402588\n",
            "Batch 1800, Last Train Loss 6.124783992767334\n",
            "Batch 1900, Last Train Loss 5.99050760269165\n",
            "Batch 2000, Last Train Loss 6.879035472869873\n",
            "Batch 2100, Last Train Loss 6.676570415496826\n",
            "Batch 2200, Last Train Loss 5.612576007843018\n",
            "Batch 2300, Last Train Loss 6.496153831481934\n",
            "Batch 2400, Last Train Loss 5.953967094421387\n",
            "Batch 2500, Last Train Loss 7.798093795776367\n",
            "Batch 2600, Last Train Loss 3.825467348098755\n",
            "Batch 2700, Last Train Loss 5.294064044952393\n",
            "Batch 2800, Last Train Loss 6.4647979736328125\n",
            "Batch 2900, Last Train Loss 6.276163578033447\n",
            "Batch 3000, Last Train Loss 7.600394248962402\n",
            "Batch 3100, Last Train Loss 6.387923240661621\n",
            "Epoch 2 finished\n",
            "Batch 0, Last Train Loss 7.123232841491699\n",
            "Batch 100, Last Train Loss 6.825240135192871\n",
            "Batch 200, Last Train Loss 5.781578540802002\n",
            "Batch 300, Last Train Loss 6.668585777282715\n",
            "Batch 400, Last Train Loss 6.620718955993652\n",
            "Batch 500, Last Train Loss 6.224221706390381\n",
            "Batch 600, Last Train Loss 6.701578140258789\n",
            "Batch 700, Last Train Loss 8.220803260803223\n",
            "Batch 800, Last Train Loss 6.604506015777588\n",
            "Batch 900, Last Train Loss 6.568035125732422\n",
            "Batch 1000, Last Train Loss 6.22856330871582\n",
            "Batch 1100, Last Train Loss 6.443106174468994\n",
            "Batch 1200, Last Train Loss 4.951851844787598\n",
            "Batch 1300, Last Train Loss 6.201090335845947\n",
            "Batch 1400, Last Train Loss 6.453426837921143\n",
            "Batch 1500, Last Train Loss 6.01198148727417\n",
            "Batch 1600, Last Train Loss 6.724023342132568\n",
            "Batch 1700, Last Train Loss 6.7993483543396\n",
            "Batch 1800, Last Train Loss 4.133657455444336\n",
            "Batch 1900, Last Train Loss 5.785479545593262\n",
            "Batch 2000, Last Train Loss 5.711467742919922\n",
            "Batch 2100, Last Train Loss 4.647820949554443\n",
            "Batch 2200, Last Train Loss 5.616990089416504\n",
            "Batch 2300, Last Train Loss 5.330291271209717\n",
            "Batch 2400, Last Train Loss 6.701851844787598\n",
            "Batch 2500, Last Train Loss 5.993262767791748\n",
            "Batch 2600, Last Train Loss 7.043309211730957\n",
            "Batch 2700, Last Train Loss 5.822855472564697\n",
            "Batch 2800, Last Train Loss 6.357746601104736\n",
            "Batch 2900, Last Train Loss 6.614368438720703\n",
            "Batch 3000, Last Train Loss 5.234385967254639\n",
            "Batch 3100, Last Train Loss 5.5044426918029785\n",
            "Epoch 3 finished\n",
            "Batch 0, Last Train Loss 5.807601451873779\n",
            "Batch 100, Last Train Loss 5.256110668182373\n",
            "Batch 200, Last Train Loss 5.90764045715332\n",
            "Batch 300, Last Train Loss 6.421492099761963\n",
            "Batch 400, Last Train Loss 4.963687419891357\n",
            "Batch 500, Last Train Loss 6.387262344360352\n",
            "Batch 600, Last Train Loss 6.455779552459717\n",
            "Batch 700, Last Train Loss 5.275057315826416\n",
            "Batch 800, Last Train Loss 4.001105308532715\n",
            "Batch 900, Last Train Loss 5.258576393127441\n",
            "Batch 1000, Last Train Loss 6.362533092498779\n",
            "Batch 1100, Last Train Loss 5.810433387756348\n",
            "Batch 1200, Last Train Loss 6.579606056213379\n",
            "Batch 1300, Last Train Loss 6.273081302642822\n",
            "Batch 1400, Last Train Loss 5.802717208862305\n",
            "Batch 1500, Last Train Loss 5.808831691741943\n",
            "Batch 1600, Last Train Loss 5.8696980476379395\n",
            "Batch 1700, Last Train Loss 5.621417999267578\n",
            "Batch 1800, Last Train Loss 6.439644813537598\n",
            "Batch 1900, Last Train Loss 6.889506816864014\n",
            "Batch 2000, Last Train Loss 6.917819499969482\n",
            "Batch 2100, Last Train Loss 6.7742438316345215\n",
            "Batch 2200, Last Train Loss 5.204185962677002\n",
            "Batch 2300, Last Train Loss 6.401907920837402\n",
            "Batch 2400, Last Train Loss 4.546618461608887\n",
            "Batch 2500, Last Train Loss 6.973772048950195\n",
            "Batch 2600, Last Train Loss 6.103154182434082\n",
            "Batch 2700, Last Train Loss 6.523642539978027\n",
            "Batch 2800, Last Train Loss 5.624768257141113\n",
            "Batch 2900, Last Train Loss 6.195801258087158\n",
            "Batch 3000, Last Train Loss 7.418616771697998\n",
            "Batch 3100, Last Train Loss 4.4096174240112305\n",
            "Epoch 4 finished\n",
            "Batch 0, Last Train Loss 5.7908034324646\n",
            "Batch 100, Last Train Loss 6.392549991607666\n",
            "Batch 200, Last Train Loss 5.792017459869385\n",
            "Batch 300, Last Train Loss 3.973402500152588\n",
            "Batch 400, Last Train Loss 6.248178482055664\n",
            "Batch 500, Last Train Loss 5.920013427734375\n",
            "Batch 600, Last Train Loss 6.125626087188721\n",
            "Batch 700, Last Train Loss 6.234297752380371\n",
            "Batch 800, Last Train Loss 6.3917388916015625\n",
            "Batch 900, Last Train Loss 6.400976657867432\n",
            "Batch 1000, Last Train Loss 6.656538963317871\n",
            "Batch 1100, Last Train Loss 5.858322620391846\n",
            "Batch 1200, Last Train Loss 6.112612724304199\n",
            "Batch 1300, Last Train Loss 5.487565040588379\n",
            "Batch 1400, Last Train Loss 6.192251682281494\n",
            "Batch 1500, Last Train Loss 1.601890206336975\n",
            "Batch 1600, Last Train Loss 6.598923683166504\n",
            "Batch 1700, Last Train Loss 3.9804673194885254\n",
            "Batch 1800, Last Train Loss 4.63099479675293\n",
            "Batch 1900, Last Train Loss 7.999545574188232\n",
            "Batch 2000, Last Train Loss 6.335219383239746\n",
            "Batch 2100, Last Train Loss 4.187882900238037\n",
            "Batch 2200, Last Train Loss 6.448779582977295\n",
            "Batch 2300, Last Train Loss 5.334262847900391\n",
            "Batch 2400, Last Train Loss 6.690080642700195\n",
            "Batch 2500, Last Train Loss 6.080026626586914\n",
            "Batch 2600, Last Train Loss 5.978629112243652\n",
            "Batch 2700, Last Train Loss 6.1673760414123535\n",
            "Batch 2800, Last Train Loss 5.2017741203308105\n",
            "Batch 2900, Last Train Loss 6.33361291885376\n",
            "Batch 3000, Last Train Loss 6.511897087097168\n",
            "Batch 3100, Last Train Loss 5.381141662597656\n",
            "Epoch 5 finished\n",
            "Batch 0, Last Train Loss 4.777760028839111\n",
            "Batch 100, Last Train Loss 3.9501237869262695\n",
            "Batch 200, Last Train Loss 5.713250160217285\n",
            "Batch 300, Last Train Loss 5.702483654022217\n",
            "Batch 400, Last Train Loss 6.77571964263916\n",
            "Batch 500, Last Train Loss 6.569110870361328\n",
            "Batch 600, Last Train Loss 6.52065896987915\n",
            "Batch 700, Last Train Loss 4.48590087890625\n",
            "Batch 800, Last Train Loss 6.320645332336426\n",
            "Batch 900, Last Train Loss 6.221048831939697\n",
            "Batch 1000, Last Train Loss 4.808013439178467\n",
            "Batch 1100, Last Train Loss 5.538735866546631\n",
            "Batch 1200, Last Train Loss 4.118221282958984\n",
            "Batch 1300, Last Train Loss 6.272615432739258\n",
            "Batch 1400, Last Train Loss 7.5385308265686035\n",
            "Batch 1500, Last Train Loss 4.577393054962158\n",
            "Batch 1600, Last Train Loss 6.587945461273193\n",
            "Batch 1700, Last Train Loss 5.287532329559326\n",
            "Batch 1800, Last Train Loss 6.009703636169434\n",
            "Batch 1900, Last Train Loss 6.208389759063721\n",
            "Batch 2000, Last Train Loss 6.4372124671936035\n",
            "Batch 2100, Last Train Loss 5.278053283691406\n",
            "Batch 2200, Last Train Loss 6.431750774383545\n",
            "Batch 2300, Last Train Loss 5.10740852355957\n",
            "Batch 2400, Last Train Loss 5.271737575531006\n",
            "Batch 2500, Last Train Loss 6.606556415557861\n",
            "Batch 2600, Last Train Loss 5.441740036010742\n",
            "Batch 2700, Last Train Loss 5.62224006652832\n",
            "Batch 2800, Last Train Loss 5.688159942626953\n",
            "Batch 2900, Last Train Loss 4.737239837646484\n",
            "Batch 3000, Last Train Loss 6.275838851928711\n",
            "Batch 3100, Last Train Loss 6.950216770172119\n",
            "Epoch 6 finished\n",
            "Batch 0, Last Train Loss 4.814881801605225\n",
            "Batch 100, Last Train Loss 2.669433832168579\n",
            "Batch 200, Last Train Loss 6.346761703491211\n",
            "Batch 300, Last Train Loss 5.164907455444336\n",
            "Batch 400, Last Train Loss 5.388126850128174\n",
            "Batch 500, Last Train Loss 5.003997325897217\n",
            "Batch 600, Last Train Loss 6.3651652336120605\n",
            "Batch 700, Last Train Loss 4.052145004272461\n",
            "Batch 800, Last Train Loss 5.3485798835754395\n",
            "Batch 900, Last Train Loss 6.5281147956848145\n",
            "Batch 1000, Last Train Loss 5.066577434539795\n",
            "Batch 1100, Last Train Loss 5.172120571136475\n",
            "Batch 1200, Last Train Loss 4.636216163635254\n",
            "Batch 1300, Last Train Loss 6.600427627563477\n",
            "Batch 1400, Last Train Loss 4.4983367919921875\n",
            "Batch 1500, Last Train Loss 3.436854600906372\n",
            "Batch 1600, Last Train Loss 4.811767578125\n",
            "Batch 1700, Last Train Loss 6.5853657722473145\n",
            "Batch 1800, Last Train Loss 5.779216766357422\n",
            "Batch 1900, Last Train Loss 5.887803554534912\n",
            "Batch 2000, Last Train Loss 5.738761901855469\n",
            "Batch 2100, Last Train Loss 6.808402061462402\n",
            "Batch 2200, Last Train Loss 5.417128562927246\n",
            "Batch 2300, Last Train Loss 5.117286205291748\n",
            "Batch 2400, Last Train Loss 5.62932825088501\n",
            "Batch 2500, Last Train Loss 4.346803665161133\n",
            "Batch 2600, Last Train Loss 7.079285144805908\n",
            "Batch 2700, Last Train Loss 6.184044361114502\n",
            "Batch 2800, Last Train Loss 4.449731349945068\n",
            "Batch 2900, Last Train Loss 3.101375102996826\n",
            "Batch 3000, Last Train Loss 5.374163627624512\n",
            "Batch 3100, Last Train Loss 6.292483329772949\n",
            "Epoch 7 finished\n",
            "Batch 0, Last Train Loss 4.871033191680908\n",
            "Batch 100, Last Train Loss 5.312069416046143\n",
            "Batch 200, Last Train Loss 6.25742769241333\n",
            "Batch 300, Last Train Loss 5.916421890258789\n",
            "Batch 400, Last Train Loss 6.196547985076904\n",
            "Batch 500, Last Train Loss 4.949454307556152\n",
            "Batch 600, Last Train Loss 5.833718776702881\n",
            "Batch 700, Last Train Loss 4.936514854431152\n",
            "Batch 800, Last Train Loss 4.6057047843933105\n",
            "Batch 900, Last Train Loss 5.746297836303711\n",
            "Batch 1000, Last Train Loss 5.800589084625244\n",
            "Batch 1100, Last Train Loss 5.391928672790527\n",
            "Batch 1200, Last Train Loss 5.81292724609375\n",
            "Batch 1300, Last Train Loss 5.971370697021484\n",
            "Batch 1400, Last Train Loss 5.838384628295898\n",
            "Batch 1500, Last Train Loss 6.4179534912109375\n",
            "Batch 1600, Last Train Loss 6.434347629547119\n",
            "Batch 1700, Last Train Loss 6.657838344573975\n",
            "Batch 1800, Last Train Loss 5.066362380981445\n",
            "Batch 1900, Last Train Loss 5.852817058563232\n",
            "Batch 2000, Last Train Loss 4.1700944900512695\n",
            "Batch 2100, Last Train Loss 7.096297264099121\n",
            "Batch 2200, Last Train Loss 5.960175037384033\n",
            "Batch 2300, Last Train Loss 6.3796515464782715\n",
            "Batch 2400, Last Train Loss 3.0634918212890625\n",
            "Batch 2500, Last Train Loss 6.519474029541016\n",
            "Batch 2600, Last Train Loss 4.755239009857178\n",
            "Batch 2700, Last Train Loss 3.1048495769500732\n",
            "Batch 2800, Last Train Loss 5.296679496765137\n",
            "Batch 2900, Last Train Loss 2.9275834560394287\n",
            "Batch 3000, Last Train Loss 5.868353366851807\n",
            "Batch 3100, Last Train Loss 5.5207953453063965\n",
            "Epoch 8 finished\n",
            "Batch 0, Last Train Loss 4.823385238647461\n",
            "Batch 100, Last Train Loss 5.617051124572754\n",
            "Batch 200, Last Train Loss 4.853823184967041\n",
            "Batch 300, Last Train Loss 5.036814212799072\n",
            "Batch 400, Last Train Loss 5.178238391876221\n",
            "Batch 500, Last Train Loss 6.239534378051758\n",
            "Batch 600, Last Train Loss 6.125502586364746\n",
            "Batch 700, Last Train Loss 4.193202972412109\n",
            "Batch 800, Last Train Loss 6.106689929962158\n",
            "Batch 900, Last Train Loss 5.683180332183838\n",
            "Batch 1000, Last Train Loss 5.62790060043335\n",
            "Batch 1100, Last Train Loss 1.9243810176849365\n",
            "Batch 1200, Last Train Loss 6.0130839347839355\n",
            "Batch 1300, Last Train Loss 5.2981438636779785\n",
            "Batch 1400, Last Train Loss 5.943637847900391\n",
            "Batch 1500, Last Train Loss 1.5218079090118408\n",
            "Batch 1600, Last Train Loss 4.133486747741699\n",
            "Batch 1700, Last Train Loss 6.080559730529785\n",
            "Batch 1800, Last Train Loss 6.757869720458984\n",
            "Batch 1900, Last Train Loss 4.836735725402832\n",
            "Batch 2000, Last Train Loss 6.1966142654418945\n",
            "Batch 2100, Last Train Loss 4.322340965270996\n",
            "Batch 2200, Last Train Loss 3.9745192527770996\n",
            "Batch 2300, Last Train Loss 4.893491744995117\n",
            "Batch 2400, Last Train Loss 5.162299156188965\n",
            "Batch 2500, Last Train Loss 5.897579193115234\n",
            "Batch 2600, Last Train Loss 6.141583442687988\n",
            "Batch 2700, Last Train Loss 3.667778968811035\n",
            "Batch 2800, Last Train Loss 6.64293098449707\n",
            "Batch 2900, Last Train Loss 5.852972507476807\n",
            "Batch 3000, Last Train Loss 4.956453800201416\n",
            "Batch 3100, Last Train Loss 3.7948296070098877\n",
            "Epoch 9 finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NnncvBYVUgJ",
        "outputId": "987d9009-18f0-4140-c125-531027762294"
      },
      "source": [
        "### Evaluation ###\r\n",
        "\r\n",
        "example = next(iter(train_ds)) \r\n",
        "input_ids = example[\"input_ids\"]\r\n",
        "\r\n",
        "print(\"Inputs: \", tokenizer.tokenizer_cn.decode(input_ids.numpy()[0]))\r\n",
        "\r\n",
        "logits = model.predict(example).logits\r\n",
        "\r\n",
        "tensor = tf.math.argmax(\r\n",
        "              logits, axis=-1, output_type=tf.dtypes.int64, name=None\r\n",
        "          )\r\n",
        "\r\n",
        "preds = tokenizer.tokenizer_cn.decode(tensor[0])\r\n",
        "\r\n",
        "print(\"Preds: \", preds)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs:  [CLS] 昆明 [MASK] [MASK] 城市 冬季 温和 ， 最 冷 时 的 平均 气温 约 8℃ [MASK] 夏季 凉爽 ， 最 [MASK] 的 温度 不 高于 32℃ 。 昆明 [MASK] [MASK] 云 贵 高原 ， 海拔 足 [MASK] [MASK] 米 。 [MASK] [MASK] articles [MASK] ， 在 离 地面 10 千 米 的 高度 内 ， 海拔 越 [MASK] ， 气温 就 越 [MASK] [MASK] 因此 ， 昆明 的 夏季 特别 凉快 ， 被 称为 [UNK] 春城 [UNK] 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdc2cd3b590>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fdc4a6ebd90> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdc2cd3b590>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fdc4a6ebd90> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fdc480749d8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function wrap at 0x7fdc480749d8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Preds:  的 我明 ， ， 城市 冬季 温 地 ， 最 冷 时 的 所 我 气温 约 。子 ， 夏季 在果 ， 最 ， 的 温度 不 高于 能子 。 知明 。 的 段 来 高时 ， 海 的 在 ， ， 米 。 ， ，皆 ， ， 在 离 地面 10 一 米 的 高度 内 ， 海 这 越 ， ， 气温 就 越 ， 的 因此 ，动明 的 夏季 特别 在快 ， 被 称为 [UNK] 春城 [UNK] 。 的 ， 的 ， ， ， ， ， 的 ， 他 ，们 了 [UNK] ， 的 的 一 了 ， 的 小 可 [UNK] 的 ， 的 。 的 ， ， ， ， ， 发 的 [UNK] 的 一 的 的 、 ， 一 的 的 ， ， ， ， ， ， 的 人 发 ， [UNK] ， 的 的 的 。 ， 的 生 的 的 的 的 有 的 ， [UNK] ， 的 这 的 ， ， 的 他 一 一 人 的 的 的 一 ， 到 ， 的 ， ， ， [UNK] 的 了 。 [UNK] 的 ， ， 了 的 的 的 ， 上 ， ， ， ， 的 ， ， 是 ， 。 ， 的 ， ， 的 ， 人 的个 的 ， 说 的 的 的 一 一 的 的 ， ， ， 的 一 比 ， 的 ， ， ， ， 。 的 了 的 。 ， ， 的 是 [UNK] ， ， 的 ， 在 的 的 是 ， ， 的 一 一 的 在 ， ， 一 ， 的 的 在 的 的 ， ， 。 的 的 的 ， 的 的 [UNK] ， 一 吃 ， 的 我 的 一 ， 的个 一 ， 这 ， ， ， 有 ， [UNK] 。 。 ， ， 的 的 ， 我 人 的 ， 的 的 一 的 ， ， ， 的 ， 了 。 是 ， 一 的 的 上 了 的 人 ， ， ， ， ， 的 ， 了 的 可个 ， 。 上 ， ， 了 ， ， 了 ， 。 ， ， 人 ， 的为 我 ， ， ， ， 的 ， 的 ， 了 [UNK] 是 上 ， ， 的 为 ， ， 了 ， ， ， 。 我 人 ， 。 ， 一 的 人 ， ， 一 我 。 ， ， ， 的 了 在 [UNK] 中 ， 的 ，物 这 有 上 在 的 都 ， 在 有 。 的 ， 的 ， 能 ， ， ， 的 ， 了 的 ， ， 的 学 的 。 的 的 ， ， ， 吃 这 ， 的 ， 一 是 了 是 了 人 ， ， 我 ， 的 的 的 ， 是 ， ， 的 ， ， [UNK] ， 。 在 ， 的 人 一 ， [UNK] 这 ， 的 ， ， 都 ， 这 的 这 ， [UNK] ， 的 的 的\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBtbbuV9kGty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}