{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PerformerMetrics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IW0_DO17MLT0",
        "4lzKWR3FMJVQ",
        "ccXBubyiL8Ud",
        "R-y3EpDRJcCB",
        "gcnt_AVoqoFX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RSXAat3wMLb"
      },
      "source": [
        "!pip install -q transformers\r\n",
        "!pip install -q datasets\r\n",
        "!pip install -q ltp"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qqWZKaXSWgz",
        "outputId": "a214130a-cd13-4bcc-ccfc-ac96602e2bf4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0_DO17MLT0"
      },
      "source": [
        "##### WWM Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qIxq8O7MRIK"
      },
      "source": [
        "## Whole Word Masking\r\n",
        "\r\n",
        "### WWM for CN\r\n",
        "\r\n",
        "# References:\r\n",
        "# https://github.com/brightmart/roberta_zh/blob/13f7849f0cb0e11573e032acddb35b83b096224e/create_pretraining_data.py\r\n",
        "# https://github.com/huggingface/transformers/blob/9152f16023b59d262b51573714b40325c8e49370/examples/legacy/run_chinese_ref.py#L78\r\n",
        "\r\n",
        "\r\n",
        "# 1) Generate ref ids based on LTP tokenizer > prepare_ref\r\n",
        "# 2) Generate mask for whole words\r\n",
        "# 3) Implement the masking\r\n",
        "\r\n",
        "from transformers import AutoTokenizer\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "from ltp import LTP\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "\r\n",
        "def random_word(tokens, ref_ids, tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\r\n",
        "    :param tokens: list of str, tokenized sentence.\r\n",
        "    :param ref_ids: list of int, 1 is where to place a mask\r\n",
        "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\r\n",
        "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\r\n",
        "    \"\"\"\r\n",
        "    output_label = []\r\n",
        "\r\n",
        "    for i, (token, ref_id) in enumerate(zip(tokens, ref_ids)):\r\n",
        "\r\n",
        "        prob = random.random()\r\n",
        "\r\n",
        "        if ref_id == 1:\r\n",
        "\r\n",
        "            # 80% randomly change token to mask token\r\n",
        "            if prob < 0.8:\r\n",
        "                tokens[i] = \"[MASK]\"\r\n",
        "\r\n",
        "            # 10% randomly change token to random token\r\n",
        "            elif prob < 0.9:\r\n",
        "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\r\n",
        "\r\n",
        "            # -> rest 10% randomly keep current token\r\n",
        "\r\n",
        "            # append current token to output (we will predict these later)\r\n",
        "            try:\r\n",
        "                output_label.append(tokenizer.vocab[token])\r\n",
        "            except KeyError:\r\n",
        "                # For unknown words (should not occur with BPE vocab)\r\n",
        "                output_label.append(tokenizer.vocab[\"[UNK]\"])\r\n",
        "        else:\r\n",
        "            # no masking token (will be ignored by loss function later)\r\n",
        "            output_label.append(-100)\r\n",
        "\r\n",
        "    return tokens, output_label\r\n",
        "\r\n",
        "def _is_chinese_char(cp):\r\n",
        "    \"\"\"\r\n",
        "    Checks whether CP is the codepoint of a CJK character.\r\n",
        "    \"\"\"\r\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\r\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\r\n",
        "    #\r\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\r\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\r\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\r\n",
        "    # space-separated words, so they are not treated specially and handled\r\n",
        "    # like the all of the other languages.\r\n",
        "    if (\r\n",
        "        (cp >= 0x4E00 and cp <= 0x9FFF)\r\n",
        "        or (cp >= 0x3400 and cp <= 0x4DBF)  #\r\n",
        "        or (cp >= 0x20000 and cp <= 0x2A6DF)  #\r\n",
        "        or (cp >= 0x2A700 and cp <= 0x2B73F)  #\r\n",
        "        or (cp >= 0x2B740 and cp <= 0x2B81F)  #\r\n",
        "        or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\r\n",
        "        or (cp >= 0xF900 and cp <= 0xFAFF)\r\n",
        "        or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\r\n",
        "    ):  #\r\n",
        "        return True\r\n",
        "\r\n",
        "    return False\r\n",
        "\r\n",
        "\r\n",
        "def is_chinese(word):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "      word: str\r\n",
        "    \"\"\"\r\n",
        "    # word like '180' or '身高' or '神'\r\n",
        "    for char in word:\r\n",
        "        char = ord(char)\r\n",
        "        if not _is_chinese_char(char):\r\n",
        "            return 0\r\n",
        "    return 1\r\n",
        "\r\n",
        "\r\n",
        "def get_chinese_word(tokens):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "      List[str]\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    word_set = set()\r\n",
        "\r\n",
        "    for token in tokens:\r\n",
        "        chinese_word = len(token) > 1 and is_chinese(token)\r\n",
        "        if chinese_word:\r\n",
        "            word_set.add(token)\r\n",
        "    word_list = list(word_set)\r\n",
        "    return word_list\r\n",
        "\r\n",
        "\r\n",
        "def add_sub_symbol(bert_tokens, chinese_word_set):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "      bert_tokens: List[str]\r\n",
        "      chinese_word_set: set\r\n",
        "    \"\"\"\r\n",
        "    if not chinese_word_set:\r\n",
        "        return bert_tokens\r\n",
        "    max_word_len = max([len(w) for w in chinese_word_set])\r\n",
        "\r\n",
        "    bert_word = bert_tokens\r\n",
        "    start, end = 0, len(bert_word)\r\n",
        "    while start < end:\r\n",
        "        single_word = True\r\n",
        "        if is_chinese(bert_word[start]):\r\n",
        "            l = min(end - start, max_word_len)\r\n",
        "            for i in range(l, 1, -1):\r\n",
        "                whole_word = \"\".join(bert_word[start : start + i])\r\n",
        "                if whole_word in chinese_word_set:\r\n",
        "                    for j in range(start + 1, start + i):\r\n",
        "                        bert_word[j] = \"##\" + bert_word[j]\r\n",
        "                    start = start + i\r\n",
        "                    single_word = False\r\n",
        "                    break\r\n",
        "        if single_word:\r\n",
        "            start += 1\r\n",
        "    return bert_word\r\n",
        "\r\n",
        "def prepare_ref(lines, ltp_tokenizer, bert_tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    Args:\r\n",
        "      lines: List[str] - e.g. [text1, text2]\r\n",
        "      ltp_tokenizer\r\n",
        "      bert_tokenizer\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      ref_ids: List[List[int], ...]\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    ltp_res = []\r\n",
        "\r\n",
        "    for i in range(0, len(lines), 100):\r\n",
        "        res = ltp_tokenizer.seg(lines[i : i + 100])[0]\r\n",
        "        res = [get_chinese_word(r) for r in res]\r\n",
        "        ltp_res.extend(res)\r\n",
        "    assert len(ltp_res) == len(lines)\r\n",
        "\r\n",
        "    bert_res = []\r\n",
        "    for i in range(0, len(lines), 100):\r\n",
        "        res = bert_tokenizer(lines[i : i + 100], add_special_tokens=True, truncation=True, max_length=512)\r\n",
        "        bert_res.extend(res[\"input_ids\"])\r\n",
        "    assert len(bert_res) == len(lines)\r\n",
        "\r\n",
        "    ref_ids = []\r\n",
        "    for input_ids, chinese_word in zip(bert_res, ltp_res):\r\n",
        "\r\n",
        "        input_tokens = []\r\n",
        "        for id in input_ids:\r\n",
        "            token = bert_tokenizer._convert_id_to_token(id)\r\n",
        "            input_tokens.append(token)\r\n",
        "        input_tokens = add_sub_symbol(input_tokens, chinese_word)\r\n",
        "        ref_id = []\r\n",
        "        # We only save pos of chinese subwords start with ##, which mean is part of a whole word.\r\n",
        "        for i, token in enumerate(input_tokens):\r\n",
        "            if token[:2] == \"##\":\r\n",
        "                clean_token = token[2:]\r\n",
        "                # save chinese tokens' pos\r\n",
        "                if len(clean_token) == 1 and _is_chinese_char(ord(clean_token)):\r\n",
        "                    ref_id.append(i)\r\n",
        "        ref_ids.append(ref_id)\r\n",
        "\r\n",
        "    assert len(ref_ids) == len(bert_res)\r\n",
        "\r\n",
        "    return ref_ids\r\n",
        "\r\n",
        "\r\n",
        "def cn_whole_word_mask(input_tokens, ref_ids, max_predictions=512, mlm_probability=0.15):\r\n",
        "    \"\"\"\r\n",
        "    Masks whole words in CN based on the reference ids & the standard _whole_word_mask for BERT for one individual example.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      input_tokens: List[str]\r\n",
        "      ref_tokens: List[int]\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      input_tokens: List[int]\r\n",
        "\r\n",
        "    TODO:\r\n",
        "      We could save the LTP dependency by copying the function from: https://github.com/HIT-SCIR/ltp/blob/c47b3f455c07c5dcc186f2b674efde8c67612baf/ltp/algorithms/maximum_forward_matching.py#L75\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    for i in range(len(input_tokens)):\r\n",
        "        if i in ref_ids:\r\n",
        "            # We move it back by -1 as the ref_ids start at 1, not 0\r\n",
        "            input_tokens[i-1] = \"##\" + input_tokens[i-1]\r\n",
        "\r\n",
        "    input_tokens = _whole_word_mask(input_tokens)\r\n",
        "\r\n",
        "    return input_tokens\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def _whole_word_mask(input_tokens, max_predictions=512, mlm_probability=0.15):\r\n",
        "    \"\"\"\r\n",
        "    Get 0/1 labels for masked tokens with whole word mask proxy\r\n",
        "\r\n",
        "    Args:\r\n",
        "      input_tokens: List[str]\r\n",
        "\r\n",
        "    Outputs:\r\n",
        "      input_tokens: List[int]\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cand_indexes = []\r\n",
        "    for (i, token) in enumerate(input_tokens):\r\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\r\n",
        "            continue\r\n",
        "\r\n",
        "        if len(cand_indexes) >= 1 and token.startswith(\"##\"):\r\n",
        "            cand_indexes[-1].append(i)\r\n",
        "\r\n",
        "        else:\r\n",
        "            cand_indexes.append([i])\r\n",
        "\r\n",
        "    random.shuffle(cand_indexes)\r\n",
        "    num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * mlm_probability))))\r\n",
        "    masked_lms = []\r\n",
        "    covered_indexes = set()\r\n",
        "    for index_set in cand_indexes:\r\n",
        "        if len(masked_lms) >= num_to_predict:\r\n",
        "            break\r\n",
        "        # If adding a whole-word mask would exceed the maximum number of\r\n",
        "        # predictions, then just skip this candidate.\r\n",
        "        if len(masked_lms) + len(index_set) > num_to_predict:\r\n",
        "            continue\r\n",
        "        is_any_index_covered = False\r\n",
        "        for index in index_set:\r\n",
        "            if index in covered_indexes:\r\n",
        "                is_any_index_covered = True\r\n",
        "                break\r\n",
        "        if is_any_index_covered:\r\n",
        "            continue\r\n",
        "        for index in index_set:\r\n",
        "            covered_indexes.add(index)\r\n",
        "            masked_lms.append(index)\r\n",
        "\r\n",
        "    assert len(covered_indexes) == len(masked_lms)\r\n",
        "    mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\r\n",
        "    return mask_labels\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class WWMTokenizer():\r\n",
        "    def __init__(self, col=\"text\", seq_len=512):\r\n",
        "        \"\"\"\r\n",
        "        Constructs Huggingface CN tokenizer & other\r\n",
        "\r\n",
        "            col: What column to tokenize if pretraining\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.tokenizer_cn = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\r\n",
        "        self.tokenizer_ltp = LTP(\"small\")\r\n",
        "        self.max_seq_length = seq_len\r\n",
        "        self.col = col\r\n",
        "\r\n",
        "    def tokenize_pretraining(self, example):\r\n",
        "        \"\"\"\r\n",
        "        Takes in an example & returns pretraining data\r\n",
        "\r\n",
        "        Args:\r\n",
        "            Example: dict with entry \"text\"\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            Dict of TF Tensors\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        inputs = example[self.col]\r\n",
        "    \r\n",
        "\r\n",
        "        ref_ids = prepare_ref([inputs], self.tokenizer_ltp, self.tokenizer_cn)\r\n",
        "\r\n",
        "        tokens = self.tokenizer_cn.tokenize(inputs)\r\n",
        "\r\n",
        "        if len(tokens) > self.max_seq_length - 2:\r\n",
        "            tokens = tokens[:(self.max_seq_length - 2)]\r\n",
        "            ref_ids = ref_ids[:(self.max_seq_length - 2)]\r\n",
        "\r\n",
        "        ref_ids = cn_whole_word_mask(tokens, ref_ids[0])\r\n",
        "        tokens, labels = random_word(tokens, ref_ids, self.tokenizer_cn)\r\n",
        "\r\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\r\n",
        "        lm_label_ids = ([-100] + labels + [-100])\r\n",
        "\r\n",
        "        input_ids = self.tokenizer_cn.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "        attention_mask = [1] * len(input_ids)\r\n",
        "        token_type_ids = [0] * len(input_ids)\r\n",
        "\r\n",
        "        while len(input_ids) < self.max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            attention_mask.append(0)\r\n",
        "            token_type_ids.append(0)\r\n",
        "            lm_label_ids.append(-100)\r\n",
        "\r\n",
        "        assert len(input_ids) == self.max_seq_length\r\n",
        "        assert len(attention_mask) == self.max_seq_length\r\n",
        "        assert len(token_type_ids) == self.max_seq_length\r\n",
        "        assert len(lm_label_ids) == self.max_seq_length\r\n",
        "\r\n",
        "\r\n",
        "        outputs = {'input_ids': tf.constant(input_ids), 'attention_mask': tf.constant(attention_mask), \r\n",
        "                'token_type_ids': tf.constant(attention_mask), 'lm_label_ids': tf.constant(lm_label_ids)}\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def to_tf_dataset(self, dataset): \r\n",
        "        \"\"\"\r\n",
        "        Turns dataset into a TF compatible dataset\r\n",
        "        \"\"\"\r\n",
        "        columns = ['input_ids', 'attention_mask', 'token_type_ids', 'lm_label_ids']\r\n",
        "        dataset.set_format(type='tensorflow', columns=columns)\r\n",
        "\r\n",
        "        return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \r\n",
        "                      'token_type_ids':tf.int32, 'lm_label_ids':tf.int32}\r\n",
        "\r\n",
        "        return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \r\n",
        "                        'token_type_ids': tf.TensorShape([None]), 'lm_label_ids':tf.TensorShape([None])}\r\n",
        "\r\n",
        "        ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\r\n",
        "        return ds"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lzKWR3FMJVQ"
      },
      "source": [
        "##### Performer Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZollJirsMRr-"
      },
      "source": [
        "# Config + Module for Performer Attention\r\n",
        "\r\n",
        "# Resources:\r\n",
        "# https://github.com/xl402/performer/blob/master/performer/networks/linear_attention.py\r\n",
        "# https://github.com/huggingface/transformers/blob/1c4236d8ef884f9f4cb1bf807ef622199c56df80/src/transformers/modeling_tf_performer_attention.py\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "import logging\r\n",
        "import math\r\n",
        "import random\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from typing import Callable, Sequence, Optional, Union\r\n",
        "\r\n",
        "from dataclasses import dataclass\r\n",
        "from enum import Enum\r\n",
        "\r\n",
        "from transformers.modeling_tf_utils import shape_list\r\n",
        "\r\n",
        "PerformerKernel = Enum('PerformerKernel', ['cosh', 'exp', 'elu', 'relu'])\r\n",
        "OrthogonalFeatureAlgorithm = Enum('OrthogonalFeatureAlgorithm', ['auto', 'kacs', 'qr'])\r\n",
        "\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class PerformerAttentionConfig:\r\n",
        "    r\"\"\"\r\n",
        "    This is the configuration class to store the configuration of a :class:`~transformers.PerformerAttention` module.\r\n",
        "    It is used to define the behavior of a Performer/FAVOR+ attention module when it is initialized.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        attention_dropout (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout ratio for the attention probabilities.\r\n",
        "        causal (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to apply causal attention, where positions are prevented from attending to positions to ahead\r\n",
        "            of themselves in the sequence, using the prefix-sum method.\r\n",
        "        kernel_type (:obj:`Enum(PerformerKernel)`, `optional`, defaults to :obj:`'exp'`):\r\n",
        "            The type of kernel function to use for comparing the queries and keys. Possible options are :obj:`'exp'`,\r\n",
        "            :obj:`'cosh'`, and :obj:`'relu'`. The :obj:`'cosh'` option approximates softmax attention with a smaller\r\n",
        "            variance than :obj:`'exp'`, but at the cost of using twice as many random features. :obj:`'relu'` may result\r\n",
        "            in better performance than :obj:`'exp'` and :obj:`'cosh'` in certain circumstances, but it is not an\r\n",
        "            unbiased estimator of softmax attention and thus should not be used with pretrained models that were\r\n",
        "            pretrained with softmax attention.\r\n",
        "        kernel_epsilon (:obj:`float`, `optional`, defaults to 1e-4):\r\n",
        "            Stabilizer term added to the output of the kernel function to avoid dividing by very small numbers.\r\n",
        "        normalize_output (:obj:`bool`, `optional`, defaults to True):\r\n",
        "            Whether to ensure that the output vectors are convex combinations of the input vectors; that is, that the\r\n",
        "            rows of the implicit attention map sum to 1.\r\n",
        "        normalization_stabilizer (:obj:`float`, `optional`, defaults to 1e-6):\r\n",
        "            Stabilizer term used when normalizing the output to avoid dividing by very small numbers.\r\n",
        "        num_random_features (:obj:`int`, `optional`, defaults to None):\r\n",
        "            The dimensionality of the random feature vectors to use. When None, the dimensionality is set to\r\n",
        "            D * log(D), where D is the dimensionality of each attention head.\r\n",
        "        orthogonal_feature_algorithm (:obj:`Enum(OrthogonalFeatureAlgorithm)`, defaults to 'auto'):\r\n",
        "            The algorithm to use for generating random orthogonal features. Possible values are 'kacs', which uses a\r\n",
        "            Kac's random walk Markov chain; 'qr', which performs QR decomposition on a random Gaussian matrix at each\r\n",
        "            redraw; and 'auto', which is equivalent to 'kacs' on PyTorch and 'qr' on TensorFlow, since the Kac's random\r\n",
        "            walk algorithm is not supported on TensorFlow. Kac's is generally faster than QR, but successive samples\r\n",
        "            are correlated with each other.\r\n",
        "        use_recurrent_decoding (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to use recurrent autoregressive decoding, as described in the 'Transformers are RNNs' paper. If\r\n",
        "            True, the PerformerAttention object will expect input tensors with a sequence length dimension of exactly 1,\r\n",
        "            and will output tensors with sequence length of 1. It will retain a recurrent hidden state between forward\r\n",
        "            passes that can be reset with the reset_recurrent_state() method.\r\n",
        "        use_thick_features (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to generate a random feature tensor that has a batch dimension.\r\n",
        "        use_orthogonal_features (:obj:`bool`, `optional`, defaults to True):\r\n",
        "            Whether to use strictly orthogonal random features, as opposed to features drawn from a standard Gaussian\r\n",
        "            distribution. Orthogonal features result in outputs that more closely approximate softmax attention, but at\r\n",
        "            the cost of doing QR decomposition on the CPU every time the features are redrawn. Best combined with a\r\n",
        "            reasonably large value of :obj:`feature_redraw_interval` (1-5k).\r\n",
        "        use_linear_layers (:obj:`bool`, `optional`, defaults to True):\r\n",
        "            Whether to transform the Q, K, and V inputs with a Linear layer before applying attention. Setting this\r\n",
        "            to False may be useful if you want to use PerformerAttention as one component of a more complex\r\n",
        "            attention mechanism.\r\n",
        "        regularize_feature_norms (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to ensure that the random feature vectors have a norm of sqrt(`d`), where `d` is the dimensionality\r\n",
        "            of each attention head.\r\n",
        "        feature_redraw_interval (:obj:`int`, `optional`, defaults to 100):\r\n",
        "            The number of forward passes after which the random feature matrix should be redrawn. If None, then the\r\n",
        "            feature matrix is never redrawn. When combined with :obj:`redraw_stochastically`, this parameter determines\r\n",
        "            the expected value of the redraw interval, rather than the interval itself.\r\n",
        "        redraw_stochastically (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            If true, PerformerAttention will redraw its random features each forward pass with a probability equal to\r\n",
        "            (1 / :obj:`feature_redraw_interval`), instead of deterministically redrawing once every N passes. This could\r\n",
        "            be desirable in large models to ensure that the attention layers don't all redraw their features at the same\r\n",
        "            time.\r\n",
        "        redraw_verbose (:obj:`bool`, `optional`, defaults to False):\r\n",
        "            Whether to log a message when random features are redrawn during training.\r\n",
        "        dim (:obj:`int`, `optional`):\r\n",
        "            Dimensionality of the queries, keys, and values.\r\n",
        "        num_heads (:obj:`int`, `optional`):\r\n",
        "            Number of attention heads.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    attention_dropout: float = 0.1\r\n",
        "    kernel_type: Union[str, Callable, PerformerKernel] = PerformerKernel.exp\r\n",
        "\r\n",
        "    causal: bool = False\r\n",
        "    use_recurrent_decoding: bool = False\r\n",
        "\r\n",
        "    kernel_epsilon: float = 1e-4\r\n",
        "    normalize_output: bool = True\r\n",
        "    normalization_stabilizer: float = 1e-6\r\n",
        "\r\n",
        "    # The linear_layer_names parameter is needed to allow the PerformerAttention object to imitate the naming\r\n",
        "    # convention of arbitrary attention modules, and therefore load weights from pretrained models. It can either have\r\n",
        "    # 3 or 4 elements; if it has 3, then no output linear layer is used.\r\n",
        "    use_linear_layers: bool = True\r\n",
        "    linear_layer_names: Sequence[str] = ('q_linear', 'k_linear', 'v_linear', 'out_linear')\r\n",
        "\r\n",
        "    num_random_features: Optional[int] = None\r\n",
        "    use_thick_features: bool = False\r\n",
        "    regularize_feature_norms: bool = True\r\n",
        "\r\n",
        "    use_orthogonal_features: bool = True\r\n",
        "    orthogonal_feature_algorithm: Union[str, OrthogonalFeatureAlgorithm] = OrthogonalFeatureAlgorithm.auto\r\n",
        "\r\n",
        "    feature_redraw_interval: Optional[int] = 100\r\n",
        "    redraw_stochastically: bool = False\r\n",
        "    redraw_verbose: bool = False\r\n",
        "\r\n",
        "    # Optional here so the user doesn't have to set redundant parameters, but must be set by model before config is\r\n",
        "    # passed to PerformerAttention.__init__()\r\n",
        "    d_model: Optional[int] = None\r\n",
        "    num_heads: Optional[int] = None\r\n",
        "\r\n",
        "    # Make enums JSON serializable\r\n",
        "    def to_dict(self):\r\n",
        "        return {k: v.name if isinstance(v, Enum) else v for k, v in self.__dict__.items()}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "KERNEL_CALLABLES = {\r\n",
        "    PerformerKernel.cosh: lambda x, h: tf.concat((tf.exp(h + x), tf.exp(h - x)), axis=-1),\r\n",
        "    PerformerKernel.exp: lambda x, h: tf.exp(h + x),  # Default\r\n",
        "    PerformerKernel.elu: lambda x: tf.nn.elu(x) + 1,\r\n",
        "    PerformerKernel.relu: tf.nn.relu\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def resolve_enum(enum_class, value):\r\n",
        "    return enum_class[value] if isinstance(value, str) else value\r\n",
        "\r\n",
        "\r\n",
        "class TFPerformerAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config: Optional[Union[dict, PerformerAttentionConfig]] = None, **kwargs):\r\n",
        "        super().__init__(name=kwargs.pop('name', None), dtype=kwargs.pop('dtype', None))\r\n",
        "\r\n",
        "        if isinstance(config, dict):\r\n",
        "            config = PerformerAttentionConfig(**config)\r\n",
        "        else:\r\n",
        "            config = config or PerformerAttentionConfig()\r\n",
        "\r\n",
        "        # kwargs take precedence over the default values that might be stored in the config object\r\n",
        "        for k, v in kwargs.items():\r\n",
        "            assert hasattr(config, k), f\"'{k}' is an invalid config parameter\"\r\n",
        "            setattr(config, k, v)\r\n",
        "\r\n",
        "        self.__dict__.update(config.__dict__)\r\n",
        "\r\n",
        "        assert self.num_heads and self.d_model, \"Num_heads and d_model must be non-None\"\r\n",
        "        assert self.d_model % self.num_heads == 0, \"Num_heads must divide d_model evenly\"\r\n",
        "        assert self.d_model > self.num_heads, \"Number of dimensions per head must be greater than 1\"\r\n",
        "        \r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=self.attention_dropout)\r\n",
        "        self.calls_since_last_redraw = 0\r\n",
        "\r\n",
        "        self.orthogonal_feature_algorithm = resolve_enum(OrthogonalFeatureAlgorithm, self.orthogonal_feature_algorithm)\r\n",
        "        assert self.orthogonal_feature_algorithm != OrthogonalFeatureAlgorithm.kacs,\\\r\n",
        "            \"Kac's random walk is not supported in TensorFlow\"\r\n",
        "\r\n",
        "        # Create the feature matrix up front if we don't need to know what the batch dimension is;\r\n",
        "        # otherwise, lazily create it on the first forward pass\r\n",
        "        self.random_features = None\r\n",
        "        if not self.use_thick_features:\r\n",
        "            self._generate_feature_matrix(batch_size=1)\r\n",
        "\r\n",
        "        # Recurrent state\r\n",
        "        if self.use_recurrent_decoding:\r\n",
        "            self.s = None\r\n",
        "            self.z = None\r\n",
        "\r\n",
        "        if isinstance(self.kernel_type, Callable):\r\n",
        "            self.kernel_fn = self.kernel_type   # Allow for custom kernel types\r\n",
        "        else:\r\n",
        "            self.kernel_type = resolve_enum(PerformerKernel, self.kernel_type)\r\n",
        "            self.kernel_fn = KERNEL_CALLABLES[self.kernel_type]\r\n",
        "\r\n",
        "        if self.use_linear_layers:\r\n",
        "            for name in self.linear_layer_names:\r\n",
        "                setattr(self, name, tf.keras.layers.Dense(units=self.d_model))\r\n",
        "\r\n",
        "    def prune_heads(self, heads):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def redraw_features_now(self):\r\n",
        "        \"\"\"\r\n",
        "        Immediately redraws the random features.\r\n",
        "        \"\"\"\r\n",
        "        batch = self.random_features.shape[0]\r\n",
        "        self._generate_feature_matrix(batch)\r\n",
        "\r\n",
        "        if self.redraw_verbose:\r\n",
        "            logging.getLogger().info(\"TFPerformerAttention: Just redrew random features.\")\r\n",
        "\r\n",
        "        self.calls_since_last_redraw = 0\r\n",
        "\r\n",
        "    def reset_recurrent_state(self):\r\n",
        "        \"\"\"\r\n",
        "        Resets the recurrent state kept by the model when use_recurrent_decoding == True\r\n",
        "        \"\"\"\r\n",
        "        self.s = None\r\n",
        "        self.z = None\r\n",
        "\r\n",
        "    def call(self, query, key, value, mask=None, head_mask=None, output_attentions=False):\r\n",
        "        \"\"\"\r\n",
        "        Parameters:\r\n",
        "            query: torch.tensor(bs, seq_length, dim)\r\n",
        "            key: torch.tensor(bs, seq_length, dim)\r\n",
        "            value: torch.tensor(bs, seq_length, dim)\r\n",
        "            mask: torch.tensor(bs, seq_length)\r\n",
        "        Returns:\r\n",
        "            weights: tf.tensor(bs, num_heads, seq_length, seq_length) Attention weights context: tf.tensor(bs,\r\n",
        "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\r\n",
        "        \"\"\"\r\n",
        "        bs, q_length, dim = query.shape\r\n",
        "        dim_per_head = self.d_model // self.num_heads\r\n",
        "\r\n",
        "        def shape(x):\r\n",
        "            \"\"\" separate heads \"\"\"\r\n",
        "            # > Use shape_list instead of .shape\r\n",
        "            new_shape = tf.concat((shape_list(x)[:-1], tf.constant([self.num_heads, dim_per_head])), axis=0)\r\n",
        "            return tf.transpose(tf.reshape(x, new_shape), perm=[0, 2, 1, 3])\r\n",
        "\r\n",
        "        if self.use_linear_layers:\r\n",
        "            query, key, value = (getattr(self, name)(x) for name, x in\r\n",
        "                                 zip(self.linear_layer_names, (query, key, value)))\r\n",
        "        \r\n",
        "        # (bs, num_heads, q_length, dim_per_head)\r\n",
        "        query, key, value = (shape(x) for x in (query, key, value))\r\n",
        "\r\n",
        "        assert not output_attentions, \"Can't output attention maps when using Performer attention.\"\r\n",
        "        if self.use_recurrent_decoding:\r\n",
        "            assert q_length == 1, \"When use_recurrent_decoding == True, we only input and output one token at a time.\"\r\n",
        "        \r\n",
        "        self._redraw_features_if_needed(bs)\r\n",
        "        \r\n",
        "        # Get the transformed values of Q and K\r\n",
        "        q_prime, k_prime = self.get_projected_queries_and_keys(query, key)\r\n",
        "        return self.compute_attention_with_projected_queries_and_keys(q_prime, k_prime, value, mask, head_mask)\r\n",
        "\r\n",
        "    def get_projected_queries_and_keys(self, q, k):\r\n",
        "        \"\"\"\r\n",
        "        Turns Q into Q' and K into K' by multiplying them by the random feature tensor.\r\n",
        "        Parameters:\r\n",
        "            q: torch.tensor(bs, seq_length, dim)\r\n",
        "            k: torch.tensor(bs, seq_length, dim)\r\n",
        "        Returns:\r\n",
        "            q_prime: torch.tensor(bs, seq_length, num_features)\r\n",
        "            k_prime: torch.tensor(bs, seq_length, num_features)\r\n",
        "        \"\"\"\r\n",
        "        # Instead of dividing the product QK^T by sqrt(d), we divide Q and K by the 4th root of d.\r\n",
        "        q = q / (self.d_model ** 0.25)\r\n",
        "        k = k / (self.d_model ** 0.25)\r\n",
        "        \r\n",
        "        projected_q = q @ self.random_features\r\n",
        "        projected_k = k @ self.random_features\r\n",
        "        \r\n",
        "        # Special logic for kernels that attempt to approximate softmax\r\n",
        "        if self.kernel_type in (PerformerKernel.cosh, PerformerKernel.exp):\r\n",
        "            # The h(x) function is defined in Lemma 1 in Choromanski et al. pg. 4 as exp(-||x||**2 / 2). For numerical\r\n",
        "            # stability we leverage the fact that exp(x)*exp(y) = exp(x + y) here and delay computing the exp().\r\n",
        "            h_of_q = -tf.math.reduce_sum(q ** 2, axis=-1, keepdims=True) / 2\r\n",
        "            h_of_k = -tf.math.reduce_sum(k ** 2, axis=-1, keepdims=True) / 2\r\n",
        "            \r\n",
        "            # Compute the numerical stabilizer that we subtract from the input to exp(). For some reason the original\r\n",
        "            # Jax implementation uses different types of stabilizers for queries vs. keys, and we follow that here.\r\n",
        "            q_stabilizer = tf.math.reduce_max(h_of_q, axis=-1, keepdims=True)\r\n",
        "            \r\n",
        "            # This is just a scalar\r\n",
        "            k_stabilizer = tf.math.reduce_max(h_of_k)\r\n",
        "            \r\n",
        "            q_kernel_output = self.kernel_fn(projected_q - q_stabilizer, h_of_q)\r\n",
        "            k_kernel_output = self.kernel_fn(projected_k - k_stabilizer, h_of_k)\r\n",
        "            \r\n",
        "            # By multiplying by 1/sqrt(m), we ensure the final matrix product will contain a factor of 1/m. This means\r\n",
        "            # each row of Q'K'^T can be interpreted as an average over the exp(omega^T * q) * exp(omega^T * k) terms.\r\n",
        "            normalizing_constant = (q_kernel_output.shape[-1] ** -0.5)\r\n",
        "            \r\n",
        "            q_prime = normalizing_constant * (q_kernel_output + self.kernel_epsilon)\r\n",
        "            k_prime = normalizing_constant * (k_kernel_output + self.kernel_epsilon)\r\n",
        "            return q_prime, k_prime\r\n",
        "        \r\n",
        "        # Generalized attention (ReLU, ELU...)\r\n",
        "        else:\r\n",
        "            return tuple(self.kernel_fn(x) + self.kernel_epsilon for x in (projected_q, projected_k))\r\n",
        "\r\n",
        "    def compute_attention_with_projected_queries_and_keys(self, q_prime, k_prime, v, mask=None, head_mask=None):\r\n",
        "        \"\"\"\r\n",
        "        Computes the attention output given Q' and K' from the above get_projected_queries_and_keys method.\r\n",
        "        Parameters:\r\n",
        "            q_prime: tf.tensor(bs, seq_length, num_features)\r\n",
        "            k_prime: tf.tensor(bs, seq_length, num_features)\r\n",
        "            v: tf.tensor(bs, seq_length, dim)\r\n",
        "            mask: tf.tensor(bs, seq_length)\r\n",
        "        Returns:\r\n",
        "            V': tf.tensor(bs, seq_length, dim)\r\n",
        "        \"\"\"\r\n",
        "        # Apply the padding mask to K'. Also applying it to Q' would be redundant.\r\n",
        "        if mask is not None:\r\n",
        "            # If extended attention mask we need to reshape it to (bs, seq_len)\r\n",
        "            # Note: k_prime actual shape is (bs, ?, seq_length, num_features)\r\n",
        "            mask = tf.reshape(mask, shape=(shape_list(k_prime)[0], shape_list(k_prime)[2]))\r\n",
        "\r\n",
        "            k_prime *= tf.expand_dims(tf.expand_dims(mask, 1), -1)\r\n",
        "\r\n",
        "        k_prime_t = tf.linalg.matrix_transpose(k_prime)\r\n",
        "        output = self._numerator_for_projected_queries_and_keys(q_prime, k_prime_t, v)\r\n",
        "\r\n",
        "        if self.normalize_output:\r\n",
        "            output /= self._denominator_for_projected_queries_and_keys(q_prime, k_prime_t)\r\n",
        "\r\n",
        "        return self._finalize_attention_output(output, head_mask)\r\n",
        "\r\n",
        "    def _numerator_for_projected_queries_and_keys(self, q_prime, k_prime_t, v):\r\n",
        "        # Noncausal\r\n",
        "        if not self.causal:\r\n",
        "            return q_prime @ (k_prime_t @ v)\r\n",
        "\r\n",
        "        # Causal, during training\r\n",
        "        if not self.use_recurrent_decoding:\r\n",
        "            return _headwise_causal_numerator(q_prime, k_prime_t, v)\r\n",
        "\r\n",
        "        # Causal, at inference time\r\n",
        "        s_delta = k_prime_t @ v\r\n",
        "        self.s = s_delta if self.s is None else self.s + s_delta\r\n",
        "\r\n",
        "        return q_prime @ self.s\r\n",
        "\r\n",
        "    def _denominator_for_projected_queries_and_keys(self, q_prime, k_prime_t):\r\n",
        "        # Noncausal\r\n",
        "        if not self.causal:\r\n",
        "            denom = q_prime @ tf.math.reduce_sum(k_prime_t, axis=-1, keepdims=True)  # Sum over positions\r\n",
        "\r\n",
        "        # Causal, during training\r\n",
        "        elif not self.use_recurrent_decoding:\r\n",
        "            prefix_sums = tf.cumsum(k_prime_t, axis=-1)               # Cumsum over positions\r\n",
        "            denom = tf.einsum(\"bhlm,bhml->bhl\", q_prime, prefix_sums)\r\n",
        "            denom = tf.expand_dims(denom, axis=-1)\r\n",
        "\r\n",
        "        # Causal, at inference time\r\n",
        "        else:\r\n",
        "            self.z = k_prime_t if self.z is None else self.z + k_prime_t    # Incrementally sum over positions\r\n",
        "            denom = q_prime @ self.z\r\n",
        "\r\n",
        "        # Avoid dividing by very small numbers\r\n",
        "        extreme_vals = tf.cast(tf.math.abs(denom) <= self.normalization_stabilizer, denom.dtype)\r\n",
        "        return denom + 2 * self.normalization_stabilizer * extreme_vals\r\n",
        "    \r\n",
        "    def _finalize_attention_output(self, context, head_mask=None, att_map_to_output=None):\r\n",
        "        # Mask heads if we want to\r\n",
        "        if head_mask is not None:\r\n",
        "            context = context * head_mask\r\n",
        "\r\n",
        "        x = tf.transpose(context, perm=[0, 2, 1, 3])  # [...seq_len, num_heads, dim_per_head]\r\n",
        "        new_last_dim = shape_list(x)[-2] * shape_list(x)[-1]\r\n",
        "        context = tf.reshape(x, shape_list(x)[:-2] + [new_last_dim])  # (bs, q_length, dim)\r\n",
        "\r\n",
        "        if self.use_linear_layers and len(self.linear_layer_names) > 3:\r\n",
        "            context = getattr(self, self.linear_layer_names[3])(context)  # (bs, q_length, dim)\r\n",
        "\r\n",
        "        if att_map_to_output:\r\n",
        "            return context, att_map_to_output\r\n",
        "        else:\r\n",
        "            return context,\r\n",
        "\r\n",
        "    def _generate_feature_matrix(self, batch_size):\r\n",
        "        dim_per_head = self.d_model // self.num_heads\r\n",
        "        num_rows = self.num_random_features or round(dim_per_head * math.log(dim_per_head))\r\n",
        "        batch = batch_size if self.use_thick_features else 1\r\n",
        "        \r\n",
        "        if not self.use_orthogonal_features:\r\n",
        "            final_tensor = tf.random.normal((batch, num_rows, dim_per_head))\r\n",
        "        else:\r\n",
        "            total_num_blocks = int(math.ceil(num_rows / dim_per_head))\r\n",
        "            extra_rows = total_num_blocks * dim_per_head - num_rows\r\n",
        "\r\n",
        "            blocks = [_get_orthogonal_block(batch, dim_per_head) for _ in range(total_num_blocks)]\r\n",
        "            if extra_rows > 0:\r\n",
        "                blocks[-1] = blocks[-1][:, extra_rows:]\r\n",
        "\r\n",
        "            final_tensor = tf.concat(blocks, axis=1)\r\n",
        "        \r\n",
        "            # This option yields SMREG\r\n",
        "            if self.regularize_feature_norms:\r\n",
        "                final_tensor *= dim_per_head ** 0.5\r\n",
        "            else:\r\n",
        "                # Hack to make the matrix columns have the norm we would expect them to have if they were sampled\r\n",
        "                # straight from a Gaussian, instead of being all norm 1 since they went through QR decomposition\r\n",
        "                multiplier = tf.norm(tf.random.normal((batch, num_rows, dim_per_head)), axis=-1)\r\n",
        "                final_tensor = tf.linalg.diag(multiplier) @ final_tensor\r\n",
        "\r\n",
        "        final_tensor = tf.expand_dims(final_tensor, axis=1)     # Add an attention head dimension\r\n",
        "        final_tensor = tf.linalg.matrix_transpose(final_tensor)\r\n",
        "        self.random_features = final_tensor\r\n",
        "    \r\n",
        "    def _redraw_features_if_needed(self, batch):\r\n",
        "        # We haven't created the projection matrix yet, let's create it\r\n",
        "        if self.random_features is None:\r\n",
        "            self._generate_feature_matrix(batch)\r\n",
        "        \r\n",
        "        elif self.feature_redraw_interval is not None:\r\n",
        "            if self.redraw_stochastically:\r\n",
        "                # random.random() returns a float between 0.0 and 1.0, so this expression\r\n",
        "                # evaluates to True with probability 1. / self.feature_redraw_interval\r\n",
        "                if random.random() < 1. / self.feature_redraw_interval:\r\n",
        "                    self.redraw_features_now()\r\n",
        "            \r\n",
        "            # It's time to redraw the projection matrix\r\n",
        "            elif self.calls_since_last_redraw >= self.feature_redraw_interval:\r\n",
        "                self.redraw_features_now()\r\n",
        "        \r\n",
        "            # Keep track of how many forward passes we do before we redraw again\r\n",
        "            else:\r\n",
        "                self.calls_since_last_redraw += 1\r\n",
        "\r\n",
        "\r\n",
        "def _get_orthogonal_block(batch, size):\r\n",
        "    with tf.device('/CPU:0'):\r\n",
        "        unstructured_block = tf.random.normal((batch, size, size))\r\n",
        "        orthog, r = tf.linalg.qr(unstructured_block)\r\n",
        "\r\n",
        "    return tf.linalg.matrix_transpose(orthog)\r\n",
        "\r\n",
        "\r\n",
        "def _headwise_causal_numerator(q_prime, k_prime_t, v):\r\n",
        "    results = []\r\n",
        "\r\n",
        "    # Iterate over the attention heads to avoid allocating a very large tensor\r\n",
        "    for head in range(q_prime.shape[1]):\r\n",
        "        # Outer products- a sorta biggish tensor\r\n",
        "        outer_prods = tf.einsum('bml,bld->blmd', k_prime_t[:, head], v[:, head])\r\n",
        "        prefix_sums = tf.cumsum(outer_prods, axis=1)\r\n",
        "\r\n",
        "        query_prods = tf.einsum('blmd,blm->bld', prefix_sums, q_prime[:, head])\r\n",
        "        results.append(tf.expand_dims(query_prods, axis=1))\r\n",
        "\r\n",
        "    return tf.concat(results, axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccXBubyiL8Ud"
      },
      "source": [
        "##### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P1C9xdDL-Hg"
      },
      "source": [
        "import warnings\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from transformers.activations_tf import get_tf_activation\r\n",
        "from transformers.file_utils import (\r\n",
        "    MULTIPLE_CHOICE_DUMMY_INPUTS,\r\n",
        "    add_code_sample_docstrings,\r\n",
        "    add_start_docstrings,\r\n",
        "    add_start_docstrings_to_model_forward,\r\n",
        ")\r\n",
        "from transformers.modeling_tf_outputs import (\r\n",
        "    TFBaseModelOutput,\r\n",
        "    TFBaseModelOutputWithPooling,\r\n",
        "    TFMaskedLMOutput,\r\n",
        "    TFMultipleChoiceModelOutput,\r\n",
        "    TFQuestionAnsweringModelOutput,\r\n",
        "    TFSequenceClassifierOutput,\r\n",
        "    TFTokenClassifierOutput,\r\n",
        ")\r\n",
        "from transformers.modeling_tf_utils import (\r\n",
        "    TFMaskedLanguageModelingLoss,\r\n",
        "    TFMultipleChoiceLoss,\r\n",
        "    TFPreTrainedModel,\r\n",
        "    TFQuestionAnsweringLoss,\r\n",
        "    TFSequenceClassificationLoss,\r\n",
        "    TFTokenClassificationLoss,\r\n",
        "    get_initializer,\r\n",
        "    input_processing,\r\n",
        "    keras_serializable,\r\n",
        "    shape_list,\r\n",
        ")\r\n",
        "from transformers.utils import logging\r\n",
        "from transformers import RobertaConfig\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.get_logger(__name__)\r\n",
        "\r\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\r\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\r\n",
        "\r\n",
        "TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\r\n",
        "    \"roberta-base\",\r\n",
        "    \"roberta-large\",\r\n",
        "    \"roberta-large-mnli\",\r\n",
        "    \"distilroberta-base\",\r\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertWordEmbeddings\r\n",
        "class TFRobertaWordEmbeddings(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, initializer_range: float, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.weight = self.add_weight(\r\n",
        "            name=\"weight\",\r\n",
        "            shape=[self.vocab_size, self.hidden_size],\r\n",
        "            initializer=get_initializer(initializer_range=self.initializer_range),\r\n",
        "        )\r\n",
        "\r\n",
        "        super().build(input_shape=input_shape)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\r\n",
        "            \"vocab_size\": self.vocab_size,\r\n",
        "            \"hidden_size\": self.hidden_size,\r\n",
        "            \"initializer_range\": self.initializer_range,\r\n",
        "        }\r\n",
        "        base_config = super().get_config()\r\n",
        "\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, input_ids):\r\n",
        "        flat_input_ids = tf.reshape(tensor=input_ids, shape=[-1])\r\n",
        "        embeddings = tf.gather(params=self.weight, indices=flat_input_ids)\r\n",
        "        embeddings = tf.reshape(\r\n",
        "            tensor=embeddings, shape=tf.concat(values=[shape_list(tensor=input_ids), [self.hidden_size]], axis=0)\r\n",
        "        )\r\n",
        "\r\n",
        "        embeddings.set_shape(shape=input_ids.shape.as_list() + [self.hidden_size])\r\n",
        "\r\n",
        "        return embeddings\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertTokenTypeEmbeddings\r\n",
        "class TFRobertaTokenTypeEmbeddings(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, type_vocab_size: int, hidden_size: int, initializer_range: float, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.type_vocab_size = type_vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.token_type_embeddings = self.add_weight(\r\n",
        "            name=\"embeddings\",\r\n",
        "            shape=[self.type_vocab_size, self.hidden_size],\r\n",
        "            initializer=get_initializer(initializer_range=self.initializer_range),\r\n",
        "        )\r\n",
        "\r\n",
        "        super().build(input_shape=input_shape)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\r\n",
        "            \"type_vocab_size\": self.type_vocab_size,\r\n",
        "            \"hidden_size\": self.hidden_size,\r\n",
        "            \"initializer_range\": self.initializer_range,\r\n",
        "        }\r\n",
        "        base_config = super().get_config()\r\n",
        "\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, token_type_ids):\r\n",
        "        flat_token_type_ids = tf.reshape(tensor=token_type_ids, shape=[-1])\r\n",
        "        one_hot_data = tf.one_hot(indices=flat_token_type_ids, depth=self.type_vocab_size, dtype=self._compute_dtype)\r\n",
        "        embeddings = tf.matmul(a=one_hot_data, b=self.token_type_embeddings)\r\n",
        "        embeddings = tf.reshape(\r\n",
        "            tensor=embeddings, shape=tf.concat(values=[shape_list(tensor=token_type_ids), [self.hidden_size]], axis=0)\r\n",
        "        )\r\n",
        "\r\n",
        "        embeddings.set_shape(shape=token_type_ids.shape.as_list() + [self.hidden_size])\r\n",
        "\r\n",
        "        return embeddings\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.longformer.modeling_tf_longformer.TFLongformerPositionEmbeddings\r\n",
        "class TFRobertaPositionEmbeddings(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, max_position_embeddings: int, hidden_size: int, initializer_range: float, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.max_position_embeddings = max_position_embeddings\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.position_embeddings = self.add_weight(\r\n",
        "            name=\"embeddings\",\r\n",
        "            shape=[self.max_position_embeddings, self.hidden_size],\r\n",
        "            initializer=get_initializer(initializer_range=self.initializer_range),\r\n",
        "        )\r\n",
        "\r\n",
        "        super().build(input_shape)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {\r\n",
        "            \"max_position_embeddings\": self.max_position_embeddings,\r\n",
        "            \"hidden_size\": self.hidden_size,\r\n",
        "            \"initializer_range\": self.initializer_range,\r\n",
        "        }\r\n",
        "        base_config = super().get_config()\r\n",
        "\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, position_ids):\r\n",
        "        flat_position_ids = tf.reshape(tensor=position_ids, shape=[-1])\r\n",
        "        embeddings = tf.gather(params=self.position_embeddings, indices=flat_position_ids)\r\n",
        "        embeddings = tf.reshape(\r\n",
        "            tensor=embeddings, shape=tf.concat(values=[shape_list(tensor=position_ids), [self.hidden_size]], axis=0)\r\n",
        "        )\r\n",
        "\r\n",
        "        embeddings.set_shape(shape=position_ids.shape.as_list() + [self.hidden_size])\r\n",
        "\r\n",
        "        return embeddings\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaEmbeddings(tf.keras.layers.Layer):\r\n",
        "    \"\"\"\r\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.padding_idx = 1\r\n",
        "        self.word_embeddings = TFRobertaWordEmbeddings(\r\n",
        "            vocab_size=config.vocab_size,\r\n",
        "            hidden_size=config.hidden_size,\r\n",
        "            initializer_range=config.initializer_range,\r\n",
        "            name=\"word_embeddings\",\r\n",
        "        )\r\n",
        "        self.position_embeddings = TFRobertaPositionEmbeddings(\r\n",
        "            max_position_embeddings=config.max_position_embeddings,\r\n",
        "            hidden_size=config.hidden_size,\r\n",
        "            initializer_range=config.initializer_range,\r\n",
        "            name=\"position_embeddings\",\r\n",
        "        )\r\n",
        "        self.token_type_embeddings = TFRobertaTokenTypeEmbeddings(\r\n",
        "            type_vocab_size=config.type_vocab_size,\r\n",
        "            hidden_size=config.hidden_size,\r\n",
        "            initializer_range=config.initializer_range,\r\n",
        "            name=\"token_type_embeddings\",\r\n",
        "        )\r\n",
        "        self.embeddings_sum = tf.keras.layers.Add()\r\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\r\n",
        "\r\n",
        "    def create_position_ids_from_input_ids(self, input_ids):\r\n",
        "        \"\"\"\r\n",
        "        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding\r\n",
        "        symbols are ignored. This is modified from fairseq's `utils.make_positions`.\r\n",
        "        Args:\r\n",
        "            input_ids: tf.Tensor\r\n",
        "        Returns: tf.Tensor\r\n",
        "        \"\"\"\r\n",
        "        input_ids_shape = shape_list(tensor=input_ids)\r\n",
        "\r\n",
        "        # multiple choice has 3 dimensions\r\n",
        "        if len(input_ids_shape) == 3:\r\n",
        "            input_ids = tf.reshape(\r\n",
        "                tensor=input_ids, shape=(input_ids_shape[0] * input_ids_shape[1], input_ids_shape[2])\r\n",
        "            )\r\n",
        "\r\n",
        "        mask = tf.cast(x=tf.math.not_equal(x=input_ids, y=self.padding_idx), dtype=input_ids.dtype)\r\n",
        "        incremental_indices = tf.math.cumsum(x=mask, axis=1) * mask\r\n",
        "\r\n",
        "        return incremental_indices + self.padding_idx\r\n",
        "\r\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\r\n",
        "        \"\"\"\r\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\r\n",
        "        Args:\r\n",
        "            inputs_embeds: tf.Tensor\r\n",
        "        Returns: tf.Tensor\r\n",
        "        \"\"\"\r\n",
        "        batch_size, seq_length = shape_list(tensor=inputs_embeds)[:2]\r\n",
        "        position_ids = tf.range(start=self.padding_idx + 1, limit=seq_length + self.padding_idx + 1)[tf.newaxis, :]\r\n",
        "\r\n",
        "        return tf.tile(input=position_ids, multiples=(batch_size, 1))\r\n",
        "\r\n",
        "    def call(self, input_ids=None, position_ids=None, token_type_ids=None, inputs_embeds=None, training=False):\r\n",
        "        \"\"\"\r\n",
        "        Applies embedding based on inputs tensor.\r\n",
        "        Returns:\r\n",
        "            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\r\n",
        "        \"\"\"\r\n",
        "        assert not (input_ids is None and inputs_embeds is None)\r\n",
        "\r\n",
        "        if input_ids is not None:\r\n",
        "            inputs_embeds = self.word_embeddings(input_ids=input_ids)\r\n",
        "\r\n",
        "        if token_type_ids is None:\r\n",
        "            input_shape = shape_list(tensor=inputs_embeds)[:-1]\r\n",
        "            token_type_ids = tf.fill(dims=input_shape, value=0)\r\n",
        "\r\n",
        "        if position_ids is None:\r\n",
        "            if input_ids is not None:\r\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\r\n",
        "                position_ids = self.create_position_ids_from_input_ids(input_ids=input_ids)\r\n",
        "            else:\r\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds=inputs_embeds)\r\n",
        "\r\n",
        "        position_embeds = self.position_embeddings(position_ids=position_ids)\r\n",
        "        token_type_embeds = self.token_type_embeddings(token_type_ids=token_type_ids)\r\n",
        "        final_embeddings = self.embeddings_sum(inputs=[inputs_embeds, position_embeds, token_type_embeds])\r\n",
        "        final_embeddings = self.LayerNorm(inputs=final_embeddings)\r\n",
        "        final_embeddings = self.dropout(inputs=final_embeddings, training=training)\r\n",
        "\r\n",
        "        return final_embeddings\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertPooler\r\n",
        "class TFRobertaPooler(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.Dense(\r\n",
        "            config.hidden_size,\r\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\r\n",
        "            activation=\"tanh\",\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\r\n",
        "        # to the first token.\r\n",
        "        first_token_tensor = hidden_states[:, 0]\r\n",
        "        pooled_output = self.dense(first_token_tensor)\r\n",
        "\r\n",
        "        return pooled_output\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfAttention with Bert->Roberta\r\n",
        "class TFRobertaSelfAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\r\n",
        "            raise ValueError(\r\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\r\n",
        "                f\"of attention heads ({config.num_attention_heads})\"\r\n",
        "            )\r\n",
        "\r\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n",
        "\r\n",
        "        self.query = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cde->abde\",\r\n",
        "            output_shape=(None, config.num_attention_heads, self.attention_head_size),\r\n",
        "            bias_axes=\"de\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"query\",\r\n",
        "        )\r\n",
        "        self.key = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cde->abde\",\r\n",
        "            output_shape=(None, config.num_attention_heads, self.attention_head_size),\r\n",
        "            bias_axes=\"de\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"key\",\r\n",
        "        )\r\n",
        "        self.value = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cde->abde\",\r\n",
        "            output_shape=(None, config.num_attention_heads, self.attention_head_size),\r\n",
        "            bias_axes=\"de\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"value\",\r\n",
        "        )\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)\r\n",
        "\r\n",
        "    def call(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False, training=False):\r\n",
        "        query_layer = self.query(inputs=hidden_states)\r\n",
        "        key_layer = self.key(inputs=hidden_states)\r\n",
        "        value_layer = self.value(inputs=hidden_states)\r\n",
        "\r\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw\r\n",
        "        # attention scores.\r\n",
        "        dk = tf.cast(x=self.attention_head_size, dtype=query_layer.dtype)\r\n",
        "        query_layer = tf.multiply(x=query_layer, y=tf.math.rsqrt(x=dk))\r\n",
        "        attention_scores = tf.einsum(\"aecd,abcd->acbe\", key_layer, query_layer)\r\n",
        "\r\n",
        "        if attention_mask is not None:\r\n",
        "            # Apply the attention mask is (precomputed for all layers in TFRobertaModel call() function)\r\n",
        "            attention_scores = attention_scores + attention_mask\r\n",
        "\r\n",
        "        # Normalize the attention scores to probabilities.\r\n",
        "        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)\r\n",
        "\r\n",
        "        # This is actually dropping out entire tokens to attend to, which might\r\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\r\n",
        "        attention_probs = self.dropout(attention_probs, training=training)\r\n",
        "\r\n",
        "        # Mask heads if we want to\r\n",
        "        if head_mask is not None:\r\n",
        "            attention_scores = attention_scores * head_mask\r\n",
        "\r\n",
        "        attention_output = tf.einsum(\"acbe,aecd->abcd\", attention_probs, value_layer)\r\n",
        "        outputs = (attention_output, attention_probs) if output_attentions else (attention_output,)\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfOutput\r\n",
        "class TFRobertaSelfOutput(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\r\n",
        "            raise ValueError(\r\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\r\n",
        "                f\"of attention heads ({config.num_attention_heads})\"\r\n",
        "            )\r\n",
        "\r\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n",
        "        self.all_head_size = config.num_attention_heads * self.attention_head_size\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abcd,cde->abe\",\r\n",
        "            output_shape=(None, self.all_head_size),\r\n",
        "            bias_axes=\"e\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\r\n",
        "\r\n",
        "    def call(self, hidden_states, input_tensor, training=False):\r\n",
        "        hidden_states = self.dense(inputs=hidden_states)\r\n",
        "        hidden_states = self.dropout(inputs=hidden_states, training=training)\r\n",
        "        hidden_states = self.LayerNorm(inputs=hidden_states + input_tensor)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertAttention with Bert->Roberta\r\n",
        "class TFRobertaAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        #self.self_attention = TFRobertaSelfAttention(config, name=\"self\")\r\n",
        "        \r\n",
        "        # Either merge Performer Config w/ normal config or enable choosing Performer\r\n",
        "        # Currently manually add performer == True to config\r\n",
        "        self.perf_attn = config.performer\r\n",
        "        if self.perf_attn == True:\r\n",
        "\r\n",
        "            self.num_attention_heads = config.num_attention_heads\r\n",
        "\r\n",
        "            performer_config = PerformerAttentionConfig(\r\n",
        "                                num_heads=config.num_attention_heads,\r\n",
        "                                d_model=config.hidden_size,\r\n",
        "                                kernel_type='exp',\r\n",
        "                                num_random_features=300,\r\n",
        "                                use_linear_layers=False\r\n",
        "                                )\r\n",
        "\r\n",
        "            self.self_attention = TFPerformerAttention(performer_config, name=\"self\")\r\n",
        "\r\n",
        "        else: \r\n",
        "            self.self_attention = TFRobertaSelfAttention(config, name=\"self\")\r\n",
        "\r\n",
        "\r\n",
        "        self.dense_output = TFRobertaSelfOutput(config, name=\"output\")\r\n",
        "\r\n",
        "    def prune_heads(self, heads):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\r\n",
        "\r\n",
        "\r\n",
        "        if self.perf_attn:\r\n",
        "            self_outputs = self.self_attention(\r\n",
        "                input_tensor, input_tensor, input_tensor, attention_mask, head_mask, output_attentions\r\n",
        "            )\r\n",
        "\r\n",
        "            # Reshape to (bs, q_length, num_h, rest)\r\n",
        "            self_outputs = list(self_outputs)\r\n",
        "            self_outputs[0] = tf.reshape(self_outputs[0], shape_list(self_outputs[0])[:2] + [self.num_attention_heads] + [-1])\r\n",
        "            self_outputs = tuple(self_outputs)\r\n",
        "        \r\n",
        "        else:\r\n",
        "            self_outputs = self.self_attention(\r\n",
        "            input_tensor, attention_mask, head_mask, output_attentions, training=training\r\n",
        "            )\r\n",
        "\r\n",
        "\r\n",
        "        attention_output = self.dense_output(self_outputs[0], input_tensor, training=training)\r\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertIntermediate\r\n",
        "class TFRobertaIntermediate(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cd->abd\",\r\n",
        "            output_shape=(None, config.intermediate_size),\r\n",
        "            bias_axes=\"d\",\r\n",
        "            kernel_initializer=get_initializer(initializer_range=config.initializer_range),\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "\r\n",
        "        if isinstance(config.hidden_act, str):\r\n",
        "            self.intermediate_act_fn = get_tf_activation(activation_string=config.hidden_act)\r\n",
        "        else:\r\n",
        "            self.intermediate_act_fn = config.hidden_act\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        hidden_states = self.dense(inputs=hidden_states)\r\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertOutput\r\n",
        "class TFRobertaOutput(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.experimental.EinsumDense(\r\n",
        "            equation=\"abc,cd->abd\",\r\n",
        "            bias_axes=\"d\",\r\n",
        "            output_shape=(None, config.hidden_size),\r\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\r\n",
        "\r\n",
        "    def call(self, hidden_states, input_tensor, training=False):\r\n",
        "        hidden_states = self.dense(inputs=hidden_states)\r\n",
        "        hidden_states = self.dropout(inputs=hidden_states, training=training)\r\n",
        "        hidden_states = self.LayerNorm(inputs=hidden_states + input_tensor)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertLayer with Bert->Roberta\r\n",
        "class TFRobertaLayer(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.attention = TFRobertaAttention(config, name=\"attention\")\r\n",
        "        self.intermediate = TFRobertaIntermediate(config, name=\"intermediate\")\r\n",
        "        self.bert_output = TFRobertaOutput(config, name=\"output\")\r\n",
        "\r\n",
        "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\r\n",
        "        attention_outputs = self.attention(\r\n",
        "            hidden_states, attention_mask, head_mask, output_attentions, training=training\r\n",
        "        )\r\n",
        "        attention_output = attention_outputs[0]\r\n",
        "        intermediate_output = self.intermediate(attention_output)\r\n",
        "        layer_output = self.bert_output(intermediate_output, attention_output, training=training)\r\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "\r\n",
        "# Copied from transformers.models.bert.modeling_tf_bert.TFBertEncoder with Bert->Roberta\r\n",
        "class TFRobertaEncoder(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.layer = [TFRobertaLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\r\n",
        "\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        hidden_states,\r\n",
        "        attention_mask,\r\n",
        "        head_mask,\r\n",
        "        output_attentions,\r\n",
        "        output_hidden_states,\r\n",
        "        return_dict,\r\n",
        "        training=False,\r\n",
        "    ):\r\n",
        "        all_hidden_states = () if output_hidden_states else None\r\n",
        "        all_attentions = () if output_attentions else None\r\n",
        "\r\n",
        "        for i, layer_module in enumerate(self.layer):\r\n",
        "            if output_hidden_states:\r\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\r\n",
        "\r\n",
        "            layer_outputs = layer_module(\r\n",
        "                hidden_states, attention_mask, head_mask[i], output_attentions, training=training\r\n",
        "            )\r\n",
        "            hidden_states = layer_outputs[0]\r\n",
        "\r\n",
        "            if output_attentions:\r\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\r\n",
        "\r\n",
        "        # Add last layer\r\n",
        "        if output_hidden_states:\r\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\r\n",
        "\r\n",
        "        if not return_dict:\r\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\r\n",
        "\r\n",
        "        return TFBaseModelOutput(\r\n",
        "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "@keras_serializable\r\n",
        "class TFRobertaMainLayer(tf.keras.layers.Layer):\r\n",
        "    config_class = RobertaConfig\r\n",
        "\r\n",
        "    def __init__(self, config, add_pooling_layer=True, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.config = config\r\n",
        "        self.num_hidden_layers = config.num_hidden_layers\r\n",
        "        self.initializer_range = config.initializer_range\r\n",
        "        self.output_attentions = config.output_attentions\r\n",
        "        self.output_hidden_states = config.output_hidden_states\r\n",
        "        self.return_dict = config.use_return_dict\r\n",
        "        self.encoder = TFRobertaEncoder(config, name=\"encoder\")\r\n",
        "        self.pooler = TFRobertaPooler(config, name=\"pooler\") if add_pooling_layer else None\r\n",
        "        # The embeddings must be the last declaration in order to follow the weights order\r\n",
        "        self.embeddings = TFRobertaEmbeddings(config, name=\"embeddings\")\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer.get_input_embeddings\r\n",
        "    def get_input_embeddings(self):\r\n",
        "        return self.embeddings.word_embeddings\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer.set_input_embeddings\r\n",
        "    def set_input_embeddings(self, value):\r\n",
        "        self.embeddings.word_embeddings.weight = value\r\n",
        "        self.embeddings.word_embeddings.vocab_size = shape_list(value)[0]\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer._prune_heads\r\n",
        "    def _prune_heads(self, heads_to_prune):\r\n",
        "        \"\"\"\r\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\r\n",
        "        class PreTrainedModel\r\n",
        "        \"\"\"\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer.call\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "\r\n",
        "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\r\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\r\n",
        "        elif inputs[\"input_ids\"] is not None:\r\n",
        "            input_shape = shape_list(inputs[\"input_ids\"])\r\n",
        "        elif inputs[\"inputs_embeds\"] is not None:\r\n",
        "            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\r\n",
        "        else:\r\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\r\n",
        "\r\n",
        "        if inputs[\"attention_mask\"] is None:\r\n",
        "            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)\r\n",
        "\r\n",
        "        if inputs[\"token_type_ids\"] is None:\r\n",
        "            inputs[\"token_type_ids\"] = tf.fill(input_shape, 0)\r\n",
        "\r\n",
        "        embedding_output = self.embeddings(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            inputs[\"position_ids\"],\r\n",
        "            inputs[\"token_type_ids\"],\r\n",
        "            inputs[\"inputs_embeds\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\r\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\r\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\r\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\r\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\r\n",
        "        extended_attention_mask = inputs[\"attention_mask\"][:, tf.newaxis, tf.newaxis, :]\r\n",
        "\r\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\r\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\r\n",
        "        # positions we want to attend and -10000.0 for masked positions.\r\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\r\n",
        "        # effectively the same as removing these entirely.\r\n",
        "        extended_attention_mask = tf.cast(extended_attention_mask, embedding_output.dtype)\r\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\r\n",
        "\r\n",
        "        # Prepare head mask if needed\r\n",
        "        # 1.0 in head_mask indicate we keep the head\r\n",
        "        # attention_probs has shape bsz x n_heads x N x N\r\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\r\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\r\n",
        "        if inputs[\"head_mask\"] is not None:\r\n",
        "            raise NotImplementedError\r\n",
        "        else:\r\n",
        "            inputs[\"head_mask\"] = [None] * self.num_hidden_layers\r\n",
        "            # head_mask = tf.constant([0] * self.num_hidden_layers)\r\n",
        "\r\n",
        "        encoder_outputs = self.encoder(\r\n",
        "            embedding_output,\r\n",
        "            extended_attention_mask,\r\n",
        "            inputs[\"head_mask\"],\r\n",
        "            inputs[\"output_attentions\"],\r\n",
        "            inputs[\"output_hidden_states\"],\r\n",
        "            inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        sequence_output = encoder_outputs[0]\r\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            return (\r\n",
        "                sequence_output,\r\n",
        "                pooled_output,\r\n",
        "            ) + encoder_outputs[1:]\r\n",
        "\r\n",
        "        return TFBaseModelOutputWithPooling(\r\n",
        "            last_hidden_state=sequence_output,\r\n",
        "            pooler_output=pooled_output,\r\n",
        "            hidden_states=encoder_outputs.hidden_states,\r\n",
        "            attentions=encoder_outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaPreTrainedModel(TFPreTrainedModel):\r\n",
        "    \"\"\"\r\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\r\n",
        "    models.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    config_class = RobertaConfig\r\n",
        "    base_model_prefix = \"roberta\"\r\n",
        "\r\n",
        "    @tf.function(\r\n",
        "        input_signature=[\r\n",
        "            {\r\n",
        "                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\r\n",
        "                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\r\n",
        "            }\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    def serving(self, inputs):\r\n",
        "        output = self.call(inputs)\r\n",
        "\r\n",
        "        return self.serving_output(output)\r\n",
        "\r\n",
        "\r\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\r\n",
        "    This model inherits from :class:`~transformers.TFPreTrainedModel`. Check the superclass documentation for the\r\n",
        "    generic methods the library implements for all its model (such as downloading or saving, resizing the input\r\n",
        "    embeddings, pruning heads etc.)\r\n",
        "    This model is also a `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras/Model>`__ subclass. Use\r\n",
        "    it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage\r\n",
        "    and behavior.\r\n",
        "    .. note::\r\n",
        "        TF 2.0 models accepts two formats as inputs:\r\n",
        "        - having all inputs as keyword arguments (like PyTorch models), or\r\n",
        "        - having all inputs as a list, tuple or dict in the first positional arguments.\r\n",
        "        This second option is useful when using :meth:`tf.keras.Model.fit` method which currently requires having all\r\n",
        "        the tensors in the first argument of the model call function: :obj:`model(inputs)`.\r\n",
        "        If you choose this second option, there are three possibilities you can use to gather all the input Tensors in\r\n",
        "        the first positional argument :\r\n",
        "        - a single Tensor with :obj:`input_ids` only and nothing else: :obj:`model(inputs_ids)`\r\n",
        "        - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:\r\n",
        "          :obj:`model([input_ids, attention_mask])` or :obj:`model([input_ids, attention_mask, token_type_ids])`\r\n",
        "        - a dictionary with one or several input Tensors associated to the input names given in the docstring:\r\n",
        "          :obj:`model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids})`\r\n",
        "    Parameters:\r\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\r\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\r\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\r\n",
        "            weights.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\r\n",
        "    Args:\r\n",
        "        input_ids (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`):\r\n",
        "            Indices of input sequence tokens in the vocabulary.\r\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\r\n",
        "            :func:`transformers.PreTrainedTokenizer.__call__` and :func:`transformers.PreTrainedTokenizer.encode` for\r\n",
        "            details.\r\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\r\n",
        "        attention_mask (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`, `optional`):\r\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\r\n",
        "            - 1 for tokens that are **not masked**,\r\n",
        "            - 0 for tokens that are **masked**.\r\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\r\n",
        "        token_type_ids (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`, `optional`):\r\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\r\n",
        "            1]``:\r\n",
        "            - 0 corresponds to a `sentence A` token,\r\n",
        "            - 1 corresponds to a `sentence B` token.\r\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`__\r\n",
        "        position_ids (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`({0})`, `optional`):\r\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\r\n",
        "            config.max_position_embeddings - 1]``.\r\n",
        "            `What are position IDs? <../glossary.html#position-ids>`__\r\n",
        "        head_mask (:obj:`Numpy array` or :obj:`tf.Tensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\r\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\r\n",
        "            - 1 indicates the head is **not masked**,\r\n",
        "            - 0 indicates the head is **masked**.\r\n",
        "        inputs_embeds (:obj:`tf.Tensor` of shape :obj:`({0}, hidden_size)`, `optional`):\r\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\r\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\r\n",
        "            vectors than the model's internal embedding lookup matrix.\r\n",
        "        output_attentions (:obj:`bool`, `optional`):\r\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\r\n",
        "            tensors for more detail.\r\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\r\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\r\n",
        "            more detail.\r\n",
        "        return_dict (:obj:`bool`, `optional`):\r\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\r\n",
        "        training (:obj:`bool`, `optional`, defaults to :obj:`False`):\r\n",
        "            Whether or not to use the model in training mode (some modules like dropout modules have different\r\n",
        "            behaviors between training and evaluation).\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaModel(TFRobertaPreTrainedModel):\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFBaseModelOutputWithPooling,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            input_ids=inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertModel.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFBaseModelOutputWithPooling(\r\n",
        "            last_hidden_state=output.last_hidden_state,\r\n",
        "            pooler_output=output.pooler_output,\r\n",
        "            hidden_states=hs,\r\n",
        "            attentions=attns,\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaLMHead(tf.keras.layers.Layer):\r\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, config, input_embeddings, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = config.vocab_size\r\n",
        "        self.hidden_size = config.hidden_size\r\n",
        "        self.dense = tf.keras.layers.Dense(\r\n",
        "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n",
        "        )\r\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\r\n",
        "        self.act = get_tf_activation(\"gelu\")\r\n",
        "\r\n",
        "        # The output weights are the same as the input embeddings, but there is\r\n",
        "        # an output-only bias for each token.\r\n",
        "        self.decoder = input_embeddings\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\r\n",
        "\r\n",
        "        super().build(input_shape)\r\n",
        "\r\n",
        "    def get_output_embeddings(self):\r\n",
        "        return self.decoder\r\n",
        "\r\n",
        "    def set_output_embeddings(self, value):\r\n",
        "        self.decoder.weight = value\r\n",
        "        self.decoder.vocab_size = shape_list(value)[0]\r\n",
        "\r\n",
        "    def get_bias(self):\r\n",
        "        return {\"bias\": self.bias}\r\n",
        "\r\n",
        "    def set_bias(self, value):\r\n",
        "        self.bias = value[\"bias\"]\r\n",
        "        self.vocab_size = shape_list(value[\"bias\"])[0]\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        hidden_states = self.dense(hidden_states)\r\n",
        "        hidden_states = self.act(hidden_states)\r\n",
        "        hidden_states = self.layer_norm(hidden_states)\r\n",
        "\r\n",
        "        # project back to size of vocabulary with bias\r\n",
        "        seq_length = shape_list(tensor=hidden_states)[1]\r\n",
        "        hidden_states = tf.reshape(tensor=hidden_states, shape=[-1, self.hidden_size])\r\n",
        "        hidden_states = tf.matmul(a=hidden_states, b=self.decoder.weight, transpose_b=True)\r\n",
        "        hidden_states = tf.reshape(tensor=hidden_states, shape=[-1, seq_length, self.vocab_size])\r\n",
        "        hidden_states = tf.nn.bias_add(value=hidden_states, bias=self.bias)\r\n",
        "\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\r\n",
        "class TFRobertaForMaskedLM(TFRobertaPreTrainedModel, TFMaskedLanguageModelingLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head.decoder.weight\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.lm_head = TFRobertaLMHead(config, self.roberta.embeddings.word_embeddings, name=\"lm_head\")\r\n",
        "\r\n",
        "    def get_lm_head(self):\r\n",
        "        return self.lm_head\r\n",
        "\r\n",
        "    def get_prefix_bias_name(self):\r\n",
        "        warnings.warn(\"The method get_prefix_bias_name is deprecated. Please use `get_bias` instead.\", FutureWarning)\r\n",
        "        return self.name + \"/\" + self.lm_head.name\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFMaskedLMOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\r\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\r\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\r\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "\r\n",
        "        sequence_output = outputs[0]\r\n",
        "        prediction_scores = self.lm_head(sequence_output)\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], prediction_scores)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (prediction_scores,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFMaskedLMOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=prediction_scores,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFMaskedLMOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "class TFRobertaClassificationHead(tf.keras.layers.Layer):\r\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.dense = tf.keras.layers.Dense(\r\n",
        "            config.hidden_size,\r\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\r\n",
        "            activation=\"tanh\",\r\n",
        "            name=\"dense\",\r\n",
        "        )\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.out_proj = tf.keras.layers.Dense(\r\n",
        "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"out_proj\"\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, features, training=False):\r\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\r\n",
        "        x = self.dropout(x, training=training)\r\n",
        "        x = self.dense(x)\r\n",
        "        x = self.dropout(x, training=training)\r\n",
        "        x = self.out_proj(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\r\n",
        "    pooled output) e.g. for GLUE tasks.\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForSequenceClassification(TFRobertaPreTrainedModel, TFSequenceClassificationLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.classifier = TFRobertaClassificationHead(config, name=\"classifier\")\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFSequenceClassifierOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\r\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\r\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        sequence_output = outputs[0]\r\n",
        "        logits = self.classifier(sequence_output, training=inputs[\"training\"])\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (logits,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFSequenceClassifierOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFSequenceClassifierOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\r\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForMultipleChoice(TFRobertaPreTrainedModel, TFMultipleChoiceLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"lm_head\"]\r\n",
        "    _keys_to_ignore_on_load_missing = [r\"dropout\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\r\n",
        "        )\r\n",
        "\r\n",
        "    @property\r\n",
        "    def dummy_inputs(self):\r\n",
        "        \"\"\"\r\n",
        "        Dummy inputs to build the network.\r\n",
        "        Returns:\r\n",
        "            tf.Tensor with dummy inputs\r\n",
        "        \"\"\"\r\n",
        "        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFMultipleChoiceModelOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\r\n",
        "            num_choices]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\r\n",
        "            :obj:`input_ids` above)\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "\r\n",
        "        if inputs[\"input_ids\"] is not None:\r\n",
        "            num_choices = shape_list(inputs[\"input_ids\"])[1]\r\n",
        "            seq_length = shape_list(inputs[\"input_ids\"])[2]\r\n",
        "        else:\r\n",
        "            num_choices = shape_list(inputs_embeds)[1]\r\n",
        "            seq_length = shape_list(inputs_embeds)[2]\r\n",
        "\r\n",
        "        flat_input_ids = tf.reshape(inputs[\"input_ids\"], (-1, seq_length)) if inputs[\"input_ids\"] is not None else None\r\n",
        "        flat_attention_mask = (\r\n",
        "            tf.reshape(inputs[\"attention_mask\"], (-1, seq_length)) if inputs[\"attention_mask\"] is not None else None\r\n",
        "        )\r\n",
        "        flat_token_type_ids = (\r\n",
        "            tf.reshape(inputs[\"token_type_ids\"], (-1, seq_length)) if inputs[\"token_type_ids\"] is not None else None\r\n",
        "        )\r\n",
        "        flat_position_ids = (\r\n",
        "            tf.reshape(inputs[\"position_ids\"], (-1, seq_length)) if inputs[\"position_ids\"] is not None else None\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            flat_input_ids,\r\n",
        "            flat_attention_mask,\r\n",
        "            flat_token_type_ids,\r\n",
        "            flat_position_ids,\r\n",
        "            inputs[\"head_mask\"],\r\n",
        "            inputs[\"inputs_embeds\"],\r\n",
        "            inputs[\"output_attentions\"],\r\n",
        "            inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        pooled_output = outputs[1]\r\n",
        "        pooled_output = self.dropout(pooled_output, training=inputs[\"training\"])\r\n",
        "        logits = self.classifier(pooled_output)\r\n",
        "        reshaped_logits = tf.reshape(logits, (-1, num_choices))\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], reshaped_logits)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (reshaped_logits,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFMultipleChoiceModelOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=reshaped_logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    @tf.function(\r\n",
        "        input_signature=[\r\n",
        "            {\r\n",
        "                \"input_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"input_ids\"),\r\n",
        "                \"attention_mask\": tf.TensorSpec((None, None, None), tf.int32, name=\"attention_mask\"),\r\n",
        "            }\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    def serving(self, inputs):\r\n",
        "        output = self.call(inputs)\r\n",
        "\r\n",
        "        return self.serving_output(output)\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFMultipleChoiceModelOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\r\n",
        "    Named-Entity-Recognition (NER) tasks.\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForTokenClassification(TFRobertaPreTrainedModel, TFTokenClassificationLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\r\n",
        "    _keys_to_ignore_on_load_missing = [r\"dropout\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\r\n",
        "        )\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFTokenClassifierOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        labels=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\r\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\r\n",
        "            1]``.\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            labels=labels,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        sequence_output = outputs[0]\r\n",
        "\r\n",
        "        sequence_output = self.dropout(sequence_output, training=inputs[\"training\"])\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "\r\n",
        "        loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (logits,) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFTokenClassifierOutput(\r\n",
        "            loss=loss,\r\n",
        "            logits=logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFTokenClassifierOutput(logits=output.logits, hidden_states=hs, attentions=attns)\r\n",
        "\r\n",
        "\r\n",
        "@add_start_docstrings(\r\n",
        "    \"\"\"\r\n",
        "    RoBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\r\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\r\n",
        "    \"\"\",\r\n",
        "    ROBERTA_START_DOCSTRING,\r\n",
        ")\r\n",
        "class TFRobertaForQuestionAnswering(TFRobertaPreTrainedModel, TFQuestionAnsweringLoss):\r\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\r\n",
        "\r\n",
        "    def __init__(self, config, *inputs, **kwargs):\r\n",
        "        super().__init__(config, *inputs, **kwargs)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\r\n",
        "        self.qa_outputs = tf.keras.layers.Dense(\r\n",
        "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\r\n",
        "        )\r\n",
        "\r\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\r\n",
        "    @add_code_sample_docstrings(\r\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\r\n",
        "        checkpoint=\"roberta-base\",\r\n",
        "        output_type=TFQuestionAnsweringModelOutput,\r\n",
        "        config_class=_CONFIG_FOR_DOC,\r\n",
        "    )\r\n",
        "    def call(\r\n",
        "        self,\r\n",
        "        input_ids=None,\r\n",
        "        attention_mask=None,\r\n",
        "        token_type_ids=None,\r\n",
        "        position_ids=None,\r\n",
        "        head_mask=None,\r\n",
        "        inputs_embeds=None,\r\n",
        "        output_attentions=None,\r\n",
        "        output_hidden_states=None,\r\n",
        "        return_dict=None,\r\n",
        "        start_positions=None,\r\n",
        "        end_positions=None,\r\n",
        "        training=False,\r\n",
        "        **kwargs,\r\n",
        "    ):\r\n",
        "        r\"\"\"\r\n",
        "        start_positions (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\r\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\r\n",
        "            sequence are not taken into account for computing the loss.\r\n",
        "        end_positions (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\r\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\r\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\r\n",
        "            sequence are not taken into account for computing the loss.\r\n",
        "        \"\"\"\r\n",
        "        inputs = input_processing(\r\n",
        "            func=self.call,\r\n",
        "            config=self.config,\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "            position_ids=position_ids,\r\n",
        "            head_mask=head_mask,\r\n",
        "            inputs_embeds=inputs_embeds,\r\n",
        "            output_attentions=output_attentions,\r\n",
        "            output_hidden_states=output_hidden_states,\r\n",
        "            return_dict=return_dict,\r\n",
        "            start_positions=start_positions,\r\n",
        "            end_positions=end_positions,\r\n",
        "            training=training,\r\n",
        "            kwargs_call=kwargs,\r\n",
        "        )\r\n",
        "        outputs = self.roberta(\r\n",
        "            inputs[\"input_ids\"],\r\n",
        "            attention_mask=inputs[\"attention_mask\"],\r\n",
        "            token_type_ids=inputs[\"token_type_ids\"],\r\n",
        "            position_ids=inputs[\"position_ids\"],\r\n",
        "            head_mask=inputs[\"head_mask\"],\r\n",
        "            inputs_embeds=inputs[\"inputs_embeds\"],\r\n",
        "            output_attentions=inputs[\"output_attentions\"],\r\n",
        "            output_hidden_states=inputs[\"output_hidden_states\"],\r\n",
        "            return_dict=inputs[\"return_dict\"],\r\n",
        "            training=inputs[\"training\"],\r\n",
        "        )\r\n",
        "        sequence_output = outputs[0]\r\n",
        "\r\n",
        "        logits = self.qa_outputs(sequence_output)\r\n",
        "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\r\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\r\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\r\n",
        "\r\n",
        "        loss = None\r\n",
        "        if inputs[\"start_positions\"] is not None and inputs[\"end_positions\"] is not None:\r\n",
        "            labels = {\"start_position\": inputs[\"start_positions\"]}\r\n",
        "            labels[\"end_position\"] = inputs[\"end_positions\"]\r\n",
        "            loss = self.compute_loss(labels, (start_logits, end_logits))\r\n",
        "\r\n",
        "        if not inputs[\"return_dict\"]:\r\n",
        "            output = (start_logits, end_logits) + outputs[2:]\r\n",
        "            return ((loss,) + output) if loss is not None else output\r\n",
        "\r\n",
        "        return TFQuestionAnsweringModelOutput(\r\n",
        "            loss=loss,\r\n",
        "            start_logits=start_logits,\r\n",
        "            end_logits=end_logits,\r\n",
        "            hidden_states=outputs.hidden_states,\r\n",
        "            attentions=outputs.attentions,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering.serving_output\r\n",
        "    def serving_output(self, output):\r\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\r\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\r\n",
        "\r\n",
        "        return TFQuestionAnsweringModelOutput(\r\n",
        "            start_logits=output.start_logits, end_logits=output.end_logits, hidden_states=hs, attentions=attns\r\n",
        "        )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6VRa-qbMA6h"
      },
      "source": [
        "### Performer Metrics / Time vs Non-Performer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKCWt47zNStE"
      },
      "source": [
        "#### Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhvEMwmlFhpi",
        "outputId": "e49a586a-d36f-4370-83cf-c283f52d2b15"
      },
      "source": [
        "tokenizer = WWMTokenizer(col=\"text\", seq_len=10000)\r\n",
        "inputs = tokenizer.tokenizer_cn(\"谁\"*5000, return_tensors=\"tf\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5002 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyJgz2JbwPvq",
        "outputId": "62c85f63-281c-44a0-d3bf-e812a888edf3"
      },
      "source": [
        "from transformers import RobertaConfig\r\n",
        "import time\r\n",
        "\r\n",
        "config = RobertaConfig.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\r\n",
        "config.performer = True     # Change this to see the time difference\r\n",
        "config.max_position_embeddings = 10000 # BERT / Roberta are constrained to 512 pos embeddings by default\r\n",
        "model = TFRobertaForMaskedLM(config)\r\n",
        "\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "model(**inputs)\r\n",
        "\r\n",
        "t1 = time.time()\r\n",
        "total = t1-t0\r\n",
        "print(\"Time: \", total)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.17473077774047852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTm_P6ASIxug"
      },
      "source": [
        "#### Results:\r\n",
        "\r\n",
        "CPU: <br>\r\n",
        "**Self-Attention:** Time:  71.5938012599945 <br>\r\n",
        "**Performer:**      Time:  24.672444105148315 + Less Memory\r\n",
        "\r\n",
        "GPU (Check with !nvidia-smi): <br>\r\n",
        "V100: <br>\r\n",
        "**Self-Attention:** OOM Error <br>\r\n",
        "**Performer:**      Time: 1.0582544803619385 <br> \r\n",
        "\r\n",
        "P100: <br>\r\n",
        "**Self-Attention:** Time: 0.5554635524749756 (Sometimes 0.20..) <br>\r\n",
        "**Performer:**      Time: 0.1933746337890625"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-y3EpDRJcCB"
      },
      "source": [
        "#### Batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHzBwk_qHoNK",
        "outputId": "e7e59b67-53c8-4422-b29e-fd651637ba3e"
      },
      "source": [
        "tokenizer = WWMTokenizer(col=\"text\", seq_len=10000)\r\n",
        "batch_input_str = [(\"我\"*5000), (\"你\"*5000), (\"他\"*5000)]\r\n",
        "inputs = tokenizer.tokenizer_cn.batch_encode_plus(batch_input_str, pad_to_max_length=True, return_tensors=\"tf\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5002 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o71E1t5uJOWE",
        "outputId": "e4bcd145-9433-4d12-f49a-57b3e328d9be"
      },
      "source": [
        "from transformers import RobertaConfig\r\n",
        "import time\r\n",
        "\r\n",
        "config = RobertaConfig.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\r\n",
        "config.performer = True     # Change this to see the time difference\r\n",
        "config.max_position_embeddings = 10000 # BERT / Roberta are constrained to 512 pos embeddings by default\r\n",
        "model = TFRobertaForMaskedLM(config)\r\n",
        "\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "model(**inputs)\r\n",
        "\r\n",
        "t1 = time.time()\r\n",
        "total = t1-t0\r\n",
        "print(\"Time: \", total)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.18730378150939941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8frOUHTmJk4A"
      },
      "source": [
        "#### Results:\r\n",
        "\r\n",
        "CPU: <br>\r\n",
        "**Self-Attention:** Time:  286.82335352897644 <br>\r\n",
        "**Performer:**      Time:  71.07585096359253 + Less Memory\r\n",
        "\r\n",
        "GPU: <br>\r\n",
        "V100 <br>\r\n",
        "**Self-Attention:** OOM Error <br>\r\n",
        "**Performer:** Time: 0.24328374862670898\r\n",
        "\r\n",
        "P100 <br>\r\n",
        "**Self-Attention:** Time: OOM Error <br>\r\n",
        "**Performer:**      Time: 0.18490052223205566"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JOIPTwdRFr3"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcnt_AVoqoFX"
      },
      "source": [
        "### TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2zFKqggvgOo",
        "outputId": "e31b55fb-3f7b-4539-c386-30ceac40b3bb"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "\r\n",
        "# If this cell takes very long to run; factory reset and run this cell the very first\r\n",
        "\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "try:\r\n",
        "    # Locate TPU\r\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n",
        "    # Establish communication / Locates TPU on the network\r\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\r\n",
        "    # Connect\r\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "    # Start TPU\r\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "    # Set strategy\r\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n",
        "\r\n",
        "    print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\r\n",
        "\r\n",
        "except ValueError:\r\n",
        "    print(\"No TPU found\")\r\n",
        "\r\n",
        "# Check devices\r\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.52.139.218:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.52.139.218:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of accelerators:  8\n",
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ee9P8mI5mH",
        "outputId": "0d87e515-39b6-435a-ed2b-2ded970697f7"
      },
      "source": [
        "tokenizer = WWMTokenizer(col=\"text\", seq_len=10000)\r\n",
        "batch_input_str = [(\"我\"*5000), (\"你\"*5000), (\"他\"*5000)]\r\n",
        "inputs = tokenizer.tokenizer_cn.batch_encode_plus(batch_input_str, pad_to_max_length=True, return_tensors=\"tf\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5002 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1mnkC9BHlTn",
        "outputId": "4c9bd295-a302-42bf-b096-39eaccf18237"
      },
      "source": [
        "from transformers import RobertaConfig\r\n",
        "import time\r\n",
        "\r\n",
        "config = RobertaConfig.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\r\n",
        "config.performer = True     # Change this to see the time difference\r\n",
        "config.max_position_embeddings = 10000 # BERT / Roberta are constrained to 512 pos embeddings by default\r\n",
        "\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "# Initialize model & perform computation on TPU\r\n",
        "with tf.device('/TPU:0'):\r\n",
        "    model = TFRobertaForMaskedLM(config)\r\n",
        "    model(**inputs)\r\n",
        "\r\n",
        "t1 = time.time()\r\n",
        "total = t1-t0\r\n",
        "print(\"Time: \", total)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  16.99319338798523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFlhYVCsIoU4"
      },
      "source": [
        "#### Results:\r\n",
        "\r\n",
        "Using TPU makes more sense for a large-scale training, as the communication cost for just 1 example is too high. \r\n",
        "\r\n",
        "Simple:\r\n",
        "TPU <br>\r\n",
        "**Self-Attention:** Time: 2.5 <br>\r\n",
        "**Performer:**      Time: 2.5\r\n",
        "\r\n",
        "More or less the same - perhaps this changes as we increase scale\r\n",
        "\r\n",
        "Batch:\r\n",
        "TPU <br>\r\n",
        "**Self-Attention:** Time: OOM / Failed to allocate <br>\r\n",
        "**Performer:**      Time: 16.99319338798523"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib0EhoFjOieZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}