{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wecredo_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FISZI4tOXiWv",
        "outputId": "9e76bb95-b61b-42c7-df31-bec937695d44"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "tf_version = tf.__version__\r\n",
        "print(\"Tensorflow: \", tf_version)\r\n",
        "\r\n",
        "tf_version_split = tf_version.split('.')\r\n",
        "assert int(tf_version_split[0])==2 and int(tf_version_split[-2])>=4, f\"Tensorflow version should be '2.4+,x', given {tf_version}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow:  2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlnegDBbNqh"
      },
      "source": [
        "## WIP - DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgEmRgUSZBUR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEqA2mHZ3UD"
      },
      "source": [
        "# WIP - Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIAchjJUt26P",
        "outputId": "b90b9690-04d2-473a-a9c2-79d5bca14ad8"
      },
      "source": [
        "# TODO: Remove dependency on performer repo?\r\n",
        "!git clone https://github.com/xl402/performer.git\r\n",
        "\r\n",
        "import os\r\n",
        "import sys\r\n",
        "module_path = os.path.abspath(os.path.join('./performer'))\r\n",
        "if module_path not in sys.path:\r\n",
        "    sys.path.append(module_path)\r\n",
        "\r\n",
        "from performer.networks.linear_attention import Performer   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'performer'...\n",
            "remote: Enumerating objects: 182, done.\u001b[K\n",
            "remote: Counting objects: 100% (182/182), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 691 (delta 64), reused 148 (delta 37), pack-reused 509\u001b[K\n",
            "Receiving objects: 100% (691/691), 691.80 KiB | 453.00 KiB/s, done.\n",
            "Resolving deltas: 100% (340/340), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucYTZFREPGUj"
      },
      "source": [
        "\"\"\" Helper Functions for Model Architecture \"\"\"\r\n",
        "\r\n",
        "def gelu_new(x):\r\n",
        "    \"\"\"\r\n",
        "    Gaussian Error Linear Unit. This is a smoother version of the GELU. Original paper: https://arxiv.org/abs/1606.0841\r\n",
        "    Args:\r\n",
        "        x: float Tensor to perform activation\r\n",
        "    Returns:\r\n",
        "        `x` with the GELU activation applied.\r\n",
        "    \"\"\"\r\n",
        "    x = tf.convert_to_tensor(x)\r\n",
        "    pi = tf.cast(math.pi, x.dtype)\r\n",
        "    coeff = tf.cast(0.044715, x.dtype)\r\n",
        "    cdf = 0.5 * (1.0 + tf.tanh(tf.sqrt(2.0 / pi) * (x + coeff * tf.pow(x, 3))))\r\n",
        "\r\n",
        "    return x * cdf\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUAma2nL8A1q"
      },
      "source": [
        "\"\"\" Wecredo Model Architecture for Large-Scale CN NLP Training \"\"\"\r\n",
        "\r\n",
        "####################################################\r\n",
        "# TF 2.0 Model  constructed using Keras imperative API by sub-classing\r\n",
        "# - tf.keras.layers.Layer for the layers\r\n",
        "# - tf.keras.Model for the final model\r\n",
        "####################################################\r\n",
        "\r\n",
        "class T5LayerNorm(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, epsilon=1e-6, **kwargs):\r\n",
        "        \"\"\"\r\n",
        "        Construct a layernorm module in the T5 style No bias and no subtraction of mean.\r\n",
        "        \"\"\"\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.variance_epsilon = epsilon\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        \"\"\"Build shared word embedding layer \"\"\"\r\n",
        "        self.weight = self.add_weight(\"weight\", shape=(input_shape[-1],), initializer=\"ones\")\r\n",
        "        super().build(input_shape)\r\n",
        "\r\n",
        "    def call(self, hidden_states):\r\n",
        "        variance = tf.math.reduce_mean(tf.math.square(hidden_states), axis=-1, keepdims=True)\r\n",
        "        hidden_states = hidden_states * tf.math.rsqrt(variance + self.variance_epsilon)\r\n",
        "        return self.weight * hidden_states\r\n",
        "\r\n",
        "# TODO - Define default config? // Remove config calls?\r\n",
        "class T5DenseReluDense(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, config, **kwargs):\r\n",
        "        \"\"\"\r\n",
        "        Constructs a Relu Feed-Forward Layer. This is the default FF in T5. \r\n",
        "        \"\"\"\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.wi = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi\")\r\n",
        "        self.wo = tf.keras.layers.Dense(config.d_model, use_bias=False, name=\"wo\")\r\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\r\n",
        "        self.act = tf.keras.activations.relu\r\n",
        "\r\n",
        "    def call(self, hidden_states, training=False):\r\n",
        "        hidden_states = self.wi(hidden_states)\r\n",
        "        hidden_states = self.act(hidden_states)\r\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\r\n",
        "        hidden_states = self.wo(hidden_states)\r\n",
        "        return hidden_states\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "3AE55OukZ0yy",
        "outputId": "23cd0adb-d8e3-4bc1-ad7d-3330225926d3"
      },
      "source": [
        "import mesh_tensorflow as mtf\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "import math\r\n",
        "import mesh_tensorflow.transformer as mtf_transformer\r\n",
        "import random\r\n",
        "from models.utils import parse_inputs, entmax_cross_entropy_with_logits\r\n",
        "\r\n",
        "# --------------------------------------------------------------------------------\r\n",
        "# LAYERS:\r\n",
        "\r\n",
        "sentinel = object()\r\n",
        "\r\n",
        "\r\n",
        "def exists(x):\r\n",
        "    return x is not None\r\n",
        "\r\n",
        "\r\n",
        "def identity(x, *args, **kwargs):\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def is_incremental_inference(context):\r\n",
        "    return exists(context) and context.mode == \"incremental\"\r\n",
        "\r\n",
        "\r\n",
        "def norm(x, axis, epsilon=1e-8):\r\n",
        "    x -= mtf.reduce_mean(x, reduced_dim=axis, name=\"norm_reduce_mean_u\")\r\n",
        "    s = mtf.reduce_mean(mtf.square(x), reduced_dim=axis, name=\"norm_reduce_mean_s\")\r\n",
        "    return x * mtf.rsqrt(s + epsilon)\r\n",
        "\r\n",
        "\r\n",
        "# ReZero implementation\r\n",
        "def rezero(x, scope, dtype):\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        g = mtf.get_variable(x.mesh, \"g\", [], initializer=tf.constant_initializer(0), dtype=dtype)\r\n",
        "        return x * g\r\n",
        "\r\n",
        "\r\n",
        "def scale_norm(x, scope, *, variable_dtype, axis=sentinel, epsilon=1e-5, params=None):\r\n",
        "    if axis is sentinel:\r\n",
        "        axis = x.shape[-1]\r\n",
        "\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        g = mtf.get_variable(x.mesh, \"g\", [], initializer=tf.constant_initializer(1),\r\n",
        "                             master_dtype=variable_dtype.master_dtype,\r\n",
        "                             slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                             activation_dtype=variable_dtype.activation_dtype)\r\n",
        "\r\n",
        "        x = norm(x, axis, epsilon)\r\n",
        "        x = x * g\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "def layer_norm(x, scope, *, variable_dtype, axis=sentinel, epsilon=1e-5, params=None):\r\n",
        "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\r\n",
        "    if axis is sentinel:\r\n",
        "        axis = x.shape[-1]\r\n",
        "\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        n_state = x.shape[-1]\r\n",
        "\r\n",
        "        g = mtf.get_variable(x.mesh, \"g\", [n_state], initializer=tf.constant_initializer(1),\r\n",
        "                             master_dtype=variable_dtype.master_dtype,\r\n",
        "                             slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                             activation_dtype=variable_dtype.activation_dtype)\r\n",
        "        b = mtf.get_variable(x.mesh, \"b\", [n_state], initializer=tf.constant_initializer(0),\r\n",
        "                             master_dtype=variable_dtype.master_dtype,\r\n",
        "                             slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                             activation_dtype=variable_dtype.activation_dtype)\r\n",
        "\r\n",
        "        x = norm(x, axis, epsilon)\r\n",
        "        x = x * g + b\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "### INTEGRATE PERFORMER ATTENTION ###\r\n",
        "def linear_attention(q, k, v):\r\n",
        "    batch_dim, seq_dim, head_dim, dim_out = (v.shape[0], v.shape[1], v.shape[2], v.shape[3])\r\n",
        "    q = mtf.rename_dimension(q, \"features_per_head\", \"features_per_head_in\")\r\n",
        "    k = mtf.rename_dimension(k, \"features_per_head\", \"features_per_head_in\")\r\n",
        "\r\n",
        "    dim_in = k.shape[-1]\r\n",
        "\r\n",
        "    q = mtf.softmax(q, dim_in)\r\n",
        "    k = mtf.softmax(k, seq_dim)\r\n",
        "\r\n",
        "    context = mtf.einsum([k, v], output_shape=[batch_dim, head_dim, dim_in, dim_out])\r\n",
        "    attn = mtf.einsum([q, context], output_shape=[batch_dim, seq_dim, head_dim, dim_out])\r\n",
        "    return attn\r\n",
        "\r\n",
        "\r\n",
        "def causal_linear_attention(q, k, v, epsilon=1e-6):\r\n",
        "    batch_dim, seq_dim, head_dim, dim_out = (v.shape[0], v.shape[1], v.shape[2], v.shape[3])\r\n",
        "    q = mtf.rename_dimension(q, \"features_per_head\", \"features_per_head_in\")\r\n",
        "    k = mtf.rename_dimension(k, \"features_per_head\", \"features_per_head_in\")\r\n",
        "\r\n",
        "    dim_in = k.shape[-1]\r\n",
        "\r\n",
        "    q = mtf.softmax(q, dim_in)\r\n",
        "    k = mtf.exp(k)\r\n",
        "\r\n",
        "    cumulative_k = mtf.cumsum(k, seq_dim)\r\n",
        "    context = mtf.einsum([k, v], output_shape=[batch_dim, seq_dim, head_dim, dim_in, dim_out])\r\n",
        "    cumulative_context = mtf.cumsum(context, seq_dim)\r\n",
        "\r\n",
        "    cumulative_context /= (cumulative_k + epsilon)\r\n",
        "    attn = mtf.einsum([q, cumulative_context], output_shape=[batch_dim, seq_dim, head_dim, dim_out])\r\n",
        "    return attn\r\n",
        "\r\n",
        "\r\n",
        "def linear(x, scope, nf, *, w_init_stdev=0.02, variable_dtype, params=None, scale=False):\r\n",
        "    # nf = number of features\r\n",
        "    if params[\"scale_by_depth\"] and scale:\r\n",
        "        # Scale by sqrt(num_layers), only happens at the final projection before a res block output\r\n",
        "        w_init_stdev = w_init_stdev * (1. / math.sqrt(params[\"n_layer\"]))\r\n",
        "    if params[\"scale_by_in\"]:  # Scale by sqrt(num_input_features)\r\n",
        "        w_init_stdev = w_init_stdev * (1. / math.sqrt(x.shape[-1].size))  # Dimension is a namedtuple of (name, size)\r\n",
        "    # Not in the variable_scope because mtf already has a variable_scope in it\r\n",
        "    with tf.variable_scope(\"conv1d_main\"):\r\n",
        "        c = mtf.layers.dense(x, new_dims=[nf], reduced_dims=[x.shape[-1]], name=scope, use_bias=True,\r\n",
        "                             kernel_initializer=tf.random_normal_initializer(stddev=w_init_stdev),\r\n",
        "                             variable_dtype=variable_dtype,\r\n",
        "                             )\r\n",
        "        return c\r\n",
        "\r\n",
        "\r\n",
        "def memory_key_values(k, v, num_mem_kv, dim_batch, dim_heads, variable_dtype, mesh):\r\n",
        "    \"\"\"memory / key values from all attention paper\"\"\"\r\n",
        "\r\n",
        "    dim_mem_kv = mtf.Dimension(\"mem_kv_sequence\", num_mem_kv)\r\n",
        "    emb_dim = k.shape[-1]\r\n",
        "    mem_std = 1 / math.sqrt(emb_dim.size)\r\n",
        "\r\n",
        "    mem_k = mtf.get_variable(mesh, \"mem_k\", mtf.Shape([dim_mem_kv, dim_heads, emb_dim]),\r\n",
        "                             initializer=tf.random_normal_initializer(stddev=mem_std),\r\n",
        "                             master_dtype=variable_dtype.master_dtype,\r\n",
        "                             slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                             activation_dtype=variable_dtype.activation_dtype,\r\n",
        "                             )\r\n",
        "    mem_v = mtf.get_variable(mesh, \"mem_v\", mtf.Shape([dim_mem_kv, dim_heads, emb_dim]),\r\n",
        "                             initializer=tf.random_normal_initializer(stddev=mem_std),\r\n",
        "                             master_dtype=variable_dtype.master_dtype,\r\n",
        "                             slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                             activation_dtype=variable_dtype.activation_dtype)\r\n",
        "\r\n",
        "    mem_k, mem_v = map(lambda t: mtf.broadcast(t, [dim_batch, dim_mem_kv, dim_heads, emb_dim]),\r\n",
        "                       (mem_k, mem_v))\r\n",
        "    mem_k, mem_v = map(lambda t: mtf.rename_dimension(t, \"mem_kv_sequence\", \"sequence\"),\r\n",
        "                       (mem_k, mem_v))\r\n",
        "\r\n",
        "    k = mtf.concat([mem_k, k], \"sequence\")\r\n",
        "    v = mtf.concat([mem_v, v], \"sequence\")\r\n",
        "    return k, v\r\n",
        "\r\n",
        "\r\n",
        "def attn(x, scope, n_state, *, attention_type, params, bias, dim_seq, memory_length_dim, variable_dtype, context=None):\r\n",
        "    # x :: [batch, seq, n_embd]\r\n",
        "    x_shape, dim_batch, *_, dim_embd, mesh = x.shape, *x.shape, x.mesh\r\n",
        "\r\n",
        "    # n_state is the same as config[\"n_embd\"], which is also the same as dim_embd.\r\n",
        "    assert n_state.size % params[\"n_head\"] == 0\r\n",
        "\r\n",
        "    dim_heads = mtf.Dimension(\"heads\", params[\"n_head\"])\r\n",
        "\r\n",
        "    num_mem_kv = params.get(\"num_mem_kv\", 0)\r\n",
        "    use_num_mem_kv = num_mem_kv > 0\r\n",
        "\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        # Compute attention inputs\r\n",
        "        dim_kv = mtf.Dimension(\"features_per_head\", params[\"n_embd\"] // params[\"n_head\"])\r\n",
        "        mtfparams = mtf.transformer.attention.attention_params_simple(\r\n",
        "            x.mesh,\r\n",
        "            io_dim=dim_embd,\r\n",
        "            kv_dim=dim_kv,\r\n",
        "            heads_dim=dim_heads,\r\n",
        "            variable_dtype=variable_dtype\r\n",
        "        )\r\n",
        "        q = mtfparams.compute_q(x)\r\n",
        "        k = mtfparams.compute_k(x)\r\n",
        "        v = mtfparams.compute_v(x)\r\n",
        "\r\n",
        "        if is_incremental_inference(context):\r\n",
        "            one_hot = mtf.one_hot(context.position - 1, dim_seq, dtype=variable_dtype.master_dtype)\r\n",
        "            inv_one_hot = 1.0 - one_hot\r\n",
        "            old_k, old_v = context.get_states(2)\r\n",
        "            k = old_k * inv_one_hot + k * one_hot\r\n",
        "            v = old_v * inv_one_hot + v * one_hot\r\n",
        "\r\n",
        "        if exists(context):\r\n",
        "            context.record_new_states([k, v])\r\n",
        "\r\n",
        "        with tf.variable_scope(\"attention\"):\r\n",
        "            if attention_type == \"local\":\r\n",
        "                # `local_attention_1d` has built in autoregressive masking, so we don't need mask_attn_weights.\r\n",
        "                radius = params.get(\"local_attention_radius\", 256)\r\n",
        "\r\n",
        "                if is_incremental_inference(context):\r\n",
        "                    q *= one_hot\r\n",
        "\r\n",
        "                a = mtf_transformer.attention.local_attention_1d(\r\n",
        "                    q, k, v,\r\n",
        "                    length_dim=k.shape[1],\r\n",
        "                    key_dim=dim_kv,\r\n",
        "                    value_dim=dim_kv,\r\n",
        "                    radius=radius,\r\n",
        "                    length_dim_num_splits=1,\r\n",
        "                    fully_autoregressive=params[\"causal\"],\r\n",
        "                    attention_kwargs={},\r\n",
        "                )\r\n",
        "\r\n",
        "                if is_incremental_inference(context):\r\n",
        "                    a = mtf.gather(a, context.position - 1, dim_seq)\r\n",
        "\r\n",
        "            elif attention_type == \"global\":\r\n",
        "\r\n",
        "                # TODO: pass in fake context\r\n",
        "                # Broadcast mask bias across batch and heads\r\n",
        "                if exists(bias):\r\n",
        "                    if not is_incremental_inference(context):\r\n",
        "                        broadcasted_bias = mtf.broadcast(bias, [dim_batch, dim_heads, bias.shape[-2], bias.shape[-1]])\r\n",
        "                    else:\r\n",
        "                        # In the incremental case, a custom mask needs to be built that masks out all key/values that are greater than the current position\r\n",
        "                        bias = mtf.gather(bias, context.position - 1, dim_seq)\r\n",
        "                        broadcasted_bias = mtf.broadcast(bias, [dim_batch, dim_heads, bias.shape[-1]])\r\n",
        "\r\n",
        "                # memory key / values, from all-attention paper\r\n",
        "                if use_num_mem_kv:\r\n",
        "                    k, v = memory_key_values(k, v, num_mem_kv, dim_batch, dim_heads, variable_dtype, mesh)\r\n",
        "\r\n",
        "                k = mtf.replace_dimensions(k, k.shape[1], memory_length_dim)\r\n",
        "                v = mtf.replace_dimensions(v, v.shape[1], memory_length_dim)\r\n",
        "\r\n",
        "                attn_dropout_rate = params[\"attn_dropout\"] if params[\"mode\"] == \"train\" else 0\r\n",
        "\r\n",
        "                a = mtf_transformer.attention.attention(\r\n",
        "                    q, k, v,\r\n",
        "                    memory_length_dim=memory_length_dim,\r\n",
        "                    key_dim=dim_kv,\r\n",
        "                    value_dim=dim_kv,\r\n",
        "                    bias=broadcasted_bias,\r\n",
        "                    dropout_rate=attn_dropout_rate\r\n",
        "                )\r\n",
        "\r\n",
        "            elif attention_type == \"linear\":\r\n",
        "                linear_attn_fn = causal_linear_attention if params[\"causal\"] else linear_attention\r\n",
        "                a = linear_attn_fn(q, k, v)\r\n",
        "\r\n",
        "            else:\r\n",
        "                raise NotImplementedError(\"Unknown attention type {}!\".format(attention_type))\r\n",
        "\r\n",
        "        with tf.variable_scope(\"compute_output\"):\r\n",
        "            a = mtfparams.compute_output(a, x_shape)\r\n",
        "\r\n",
        "        with tf.variable_scope(\"compute_output_bias\"):\r\n",
        "            b = mtf.get_variable(x.mesh, \"o_b\", [dim_embd], initializer=tf.constant_initializer(0),\r\n",
        "                                 master_dtype=variable_dtype.master_dtype,\r\n",
        "                                 slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                                 activation_dtype=variable_dtype.activation_dtype)\r\n",
        "            a += b\r\n",
        "\r\n",
        "        if params[\"mode\"] == \"train\" and params[\"res_dropout\"] > 0:\r\n",
        "            a = mtf.dropout(a, rate=params[\"res_dropout\"], name=\"res_dropout\")\r\n",
        "        return a\r\n",
        "\r\n",
        "\r\n",
        "def get_activation_fn(params):\r\n",
        "    activation_fn = params.get(\"activation_fn\", \"gelu\")\r\n",
        "    \r\n",
        "\r\n",
        "    def _arcsinh(x):\r\n",
        "        return mtf.log(x + mtf.sqrt(1 + x ** 2))\r\n",
        "    def _var(x, init):\r\n",
        "        return mtf.get_variable(x.mesh, f\"activation-{random.randint(0, 2**32):x}\", [], initializer=tf.constant_initializer(init), dtype=x.dtype)\r\n",
        "    def _pos_var(x, val):\r\n",
        "        return mtf.softplus(_var(x, 0)) + val\r\n",
        "    \r\n",
        "    if activation_fn == \"gelu\": # https://arxiv.org/abs/1606.08415\r\n",
        "        return mtf.gelu\r\n",
        "    elif activation_fn == \"relu\":\r\n",
        "        return mtf.relu\r\n",
        "    elif activation_fn == \"sigmoid\":\r\n",
        "        return mtf.sigmoid\r\n",
        "    elif activation_fn == \"tanh\":\r\n",
        "        return mtf.tanh\r\n",
        "    elif activation_fn == \"selu\": # https://arxiv.org/abs/1706.02515\r\n",
        "        return mtf.selu\r\n",
        "    elif activation_fn == \"elu\": # https://arxiv.org/abs/1511.07289\r\n",
        "        return mtf.elu\r\n",
        "    elif activation_fn == \"lrelu001\":\r\n",
        "        return lambda x: mtf.leaky_relu(x, alpha=0.01)\r\n",
        "    elif activation_fn == \"lrelu020\":\r\n",
        "        return lambda x: mtf.leaky_relu(x, alpha=0.20)\r\n",
        "\r\n",
        "    elif activation_fn == \"abs\": \r\n",
        "        return mtf.abs\r\n",
        "    elif activation_fn == \"id\":\r\n",
        "        return lambda x: x\r\n",
        "    elif activation_fn == \"sin\":\r\n",
        "        return mtf.sin\r\n",
        "    elif activation_fn == \"cos\":\r\n",
        "        return mtf.cos\r\n",
        "    elif activation_fn == \"sign\":\r\n",
        "        return mtf.sign\r\n",
        "    elif activation_fn == \"triangle_relax\":\r\n",
        "        return lambda x: mtf.sin(x)-mtf.sin(3*x)/9+mtf.sin(5*x)/25-mtf.sin(7*x)/49\r\n",
        "    elif activation_fn == \"square_relax\":\r\n",
        "        return lambda x: mtf.cos(x)-mtf.cos(3*x)/3+mtf.cos(5*x)/5-mtf.cos(7*x)/7\r\n",
        "    elif activation_fn == \"spike\":\r\n",
        "        return lambda x: 1/(1+x**2)\r\n",
        "    elif activation_fn == \"spike2\":\r\n",
        "        return lambda x: mtf.exp(-x**2)\r\n",
        "    \r\n",
        "    elif activation_fn == \"tanhshrink\":\r\n",
        "        return lambda x: x - tanh(x)\r\n",
        "    elif activation_fn == \"softsign\":\r\n",
        "        return lambda x: x / (mtf.abs(x) + 1)\r\n",
        "    elif activation_fn == \"softmax\":\r\n",
        "        return lambda x: mtf.softmax(x, x.shape[-1])\r\n",
        "    elif activation_fn == \"logsoftmax\":\r\n",
        "        return lambda x: mtf.log_softmax(x, x.shape[-1])\r\n",
        "    elif activation_fn == \"bipolarsigmoid\":\r\n",
        "        return lambda x: mtf.sigmoid(x) * 2 - 1\r\n",
        "    elif activation_fn == \"rrelu\":  # https://arxiv.org/abs/1505.00853\r\n",
        "        def _rrelu_fn(x):\r\n",
        "            negative_scale = random.random()\r\n",
        "            return (negative_scale * mtf.abs(x) + x) / (1 + negative_scale)\r\n",
        "        return _rrelu_fn\r\n",
        "    elif activation_fn == \"elish\":  # https://arxiv.org/abs/1808.00783v1\r\n",
        "        def _elish_fn(x):\r\n",
        "            cond = mtf.cast(mtf.greater(x, 0), x.dtype)\r\n",
        "            exp = mtf.exp(x)\r\n",
        "            return cond * x / (1 + exp) + (1 - cond) * (exp - 1) / (1 / exp + 1)\r\n",
        "        return _elish_fn\r\n",
        "    \r\n",
        "    elif activation_fn == \"silu\": # https://arxiv.org/abs/1710.05941\r\n",
        "        return mtf.swish\r\n",
        "    \r\n",
        "    elif activation_fn == \"arcsinh\":\r\n",
        "        return _arcsinh\r\n",
        "    \r\n",
        "    \r\n",
        "    # parametric\r\n",
        "    elif activation_fn == \"aria\":  # https://arxiv.org/abs/1805.08878\r\n",
        "        return lambda x: x * (_var(x, 0) + _var(x, 1) / (_pos_var(x, 0) + _var(x, 1) * mtf.exp(_var(x, -1) * x) ** (1 / _pos_var(x, 1))))\r\n",
        "    elif activation_fn == \"prelu\":  # https://arxiv.org/abs/1502.01852\r\n",
        "        return lambda x: mtf.leaky_relu(x, alpha=_var(x, 0.2))\r\n",
        "    elif activation_fn == \"parcsinh\":\r\n",
        "        return lambda x: _var(x, 1) * _arcsinh(x * _pos_var(x, 1))\r\n",
        "    elif activation_fn == \"psoftplus\":\r\n",
        "        return lambda x: _var(x, 1) * mtf.softplus(x * _var(x, 1)) + _var(x, 0)\r\n",
        "    elif activation_fn == \"proottanh\":\r\n",
        "        return lambda x: (x ** _pos_var(x, 2) + _pos_var(x, 1)) ** (1 / _pos_var(x, 3)) * mtf.tanh(x)\r\n",
        "     \r\n",
        "    # https://arxiv.org/abs/1710.05941, https://arxiv.org/abs/1901.02671\r\n",
        "    elif activation_fn == \"maxsig\": \r\n",
        "        return lambda x: mtf.maximum(x, mtf.sigmoid(x))\r\n",
        "    elif activation_fn == \"cosid\": \r\n",
        "        return lambda x: mtf.cos(x) - x\r\n",
        "    elif activation_fn == \"minsin\": \r\n",
        "        return lambda x: mtf.minimum(x, mtf.sin(x))\r\n",
        "    elif activation_fn == \"maxtanh\": \r\n",
        "        return lambda x: mtf.maximum(x, mtf.tanh(x))\r\n",
        "    \r\n",
        "    elif activation_fn == \"softplus\":\r\n",
        "        return mtf.softplus\r\n",
        "    elif activation_fn == \"mish\": # https://arxiv.org/abs/1908.08681\r\n",
        "        return lambda x: x * mtf.tanh(mtf.softplus(x))\r\n",
        "    elif activation_fn == \"tanhexp\": # https://arxiv.org/abs/2003.09855\r\n",
        "        return lambda x: x * mtf.tanh(mtf.exp(x))\r\n",
        "    elif activation_fn == \"lisht\": # https://arxiv.org/abs/1901.05894\r\n",
        "        return lambda x: x * mtf.tanh(x)\r\n",
        "    elif activation_fn == \"seagull\": # https://arxiv.org/abs/2011.11713\r\n",
        "        return lambda x: mtf.log(1 + x ** 2)\r\n",
        "    elif activation_fn == \"snake\": # https://arxiv.org/abs/2006.08195\r\n",
        "        return lambda x: x + mtf.sin(x) ** 2\r\n",
        "    \r\n",
        "    elif activation_fn == \"roottanh\":  # made up\r\n",
        "        return lambda x: (x ** 2 + 1) ** (1/3) * mtf.tanh(x)\r\n",
        "    elif activation_fn == \"softplusmone\":  # made up\r\n",
        "        return lambda x: mtf.softplus(x) - 1\r\n",
        "    \r\n",
        "    else:\r\n",
        "        raise ValueError('unknown activation function \"activation_fn\" in config')\r\n",
        "\r\n",
        "def mlp(x, scope, n_state, *, variable_dtype, params):\r\n",
        "    activation_fn = get_activation_fn(params)\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        nx = x.shape[-1]\r\n",
        "        h = activation_fn(linear(x, \"c_fc\", n_state, variable_dtype=variable_dtype, params=params))\r\n",
        "        h2 = linear(h, \"c_proj\", nx, variable_dtype=variable_dtype, params=params, scale=True)\r\n",
        "        if params[\"mode\"] == \"train\" and params[\"res_dropout\"] > 0:\r\n",
        "            h2 = mtf.dropout(h2, rate=params[\"res_dropout\"], name=\"mlp_dropout\")\r\n",
        "        return h2\r\n",
        "\r\n",
        "\r\n",
        "def mlp_glu(x, scope, n_state, *, variable_dtype, params):\r\n",
        "    activation_fn = get_activation_fn(params)\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        nx = x.shape[-1]\r\n",
        "        h = linear(x, \"c_fc\", n_state, params=params)\r\n",
        "\r\n",
        "        h, gate = mtf.split(h, h.shape[-1], 2)\r\n",
        "        h *= activation_fn(gate)\r\n",
        "\r\n",
        "        h2 = linear(h, \"c_proj\", nx, variable_dtype=variable_dtype, params=params, scale=True)\r\n",
        "        if params[\"mode\"] == \"train\" and params[\"res_dropout\"] > 0:\r\n",
        "            h2 = mtf.dropout(h2, rate=params[\"res_dropout\"], name=\"mlp_dropout\")\r\n",
        "        return h2\r\n",
        "\r\n",
        "\r\n",
        "def block(params, scope, layer_num, bias, sequence_dim, memory_length_dim, variable_dtype, context=None):\r\n",
        "    use_mlp_glu = params[\"mlp_glu\"] == True\r\n",
        "    use_scale_norm = params[\"scalenorm\"] == True\r\n",
        "    use_moe = exists(params[\"moe_layers\"]) and (layer_num in params[\"moe_layers\"])\r\n",
        "    use_rezero = params[\"rezero\"] == True\r\n",
        "    macaron_attention = params[\"macaron\"] == True\r\n",
        "\r\n",
        "    def fn(x):\r\n",
        "        with tf.variable_scope(scope):\r\n",
        "            nx = x.shape[-1]  # Grab last dimension from input\r\n",
        "\r\n",
        "            if use_rezero:\r\n",
        "                prenorm = identity\r\n",
        "            elif use_scale_norm:\r\n",
        "                prenorm = scale_norm\r\n",
        "            else:\r\n",
        "                prenorm = layer_norm\r\n",
        "\r\n",
        "            pre_residual_fn = rezero if use_rezero else identity\r\n",
        "\r\n",
        "            attention_type = params[\"attention_types\"][layer_num]\r\n",
        "            \r\n",
        "            if macaron_attention:\r\n",
        "                mult = 0.5\r\n",
        "                mlp_fn = mlp_glu if use_mlp_glu else mlp\r\n",
        "                intermediate_size = nx.size * 4 * (1 if not use_mlp_glu else 2)\r\n",
        "                # Define intermediate layer of mlp - to split\r\n",
        "                dim_intermediate_expanded = mtf.Dimension(\"intermediate_expanded\", intermediate_size)\r\n",
        "                m = mlp_fn(x, \"mlp_macaron\", dim_intermediate_expanded, variable_dtype=variable_dtype, params=params)\r\n",
        "                \r\n",
        "                x = x + (m * mult)\r\n",
        "            else:\r\n",
        "                mult = 1\r\n",
        "\r\n",
        "            if attention_type != \"none\":\r\n",
        "                res_x = prenorm(x, \"norm_1\", variable_dtype=variable_dtype, params=params)\r\n",
        "                a = attn(res_x, \"attn\", nx, attention_type=attention_type,\r\n",
        "                         params=params, bias=bias, dim_seq=sequence_dim, memory_length_dim=memory_length_dim,\r\n",
        "                         variable_dtype=variable_dtype, context=context)\r\n",
        "            else:\r\n",
        "                a = x\r\n",
        "\r\n",
        "            x = x + pre_residual_fn(a, \"norm_rezero_1\", dtype=variable_dtype)\r\n",
        "\r\n",
        "            res_x = prenorm(x, \"norm_2\", variable_dtype=variable_dtype, params=params)\r\n",
        "\r\n",
        "            if use_moe:\r\n",
        "                moe_params = mtf.transformer.moe.HParams()\r\n",
        "                mtf.transformer.moe.set_default_moe_hparams(moe_params)\r\n",
        "                moe_params.add_hparam(\"moe_min_expert_capacity\", 1)\r\n",
        "                moe_params.add_hparam(\"moe_use_experts_attention\", False)\r\n",
        "\r\n",
        "                # Override defaults\r\n",
        "                for k, v in params[\"moe_params\"].items():\r\n",
        "                    moe_params.add_hparam(k, v)\r\n",
        "\r\n",
        "                moe_train = params[\"mode\"] == \"train\"\r\n",
        "\r\n",
        "                m, aux_loss = mtf.transformer.moe.transformer_moe_layer_v1(res_x, x.shape[-1], moe_params,\r\n",
        "                                                                           train=moe_train,\r\n",
        "                                                                           mesh_shape=params[\"mesh_shape\"],\r\n",
        "                                                                           layout=params[\"layout\"],\r\n",
        "                                                                           activation=params.get(\"moe_activation\", \"relu\"),\r\n",
        "                                                                           variable_dtype=variable_dtype,\r\n",
        "                                                                           num_microbatches=params[\"num_microbatches\"])\r\n",
        "                m = mtf.dropout(m, rate=params[\"res_dropout\"], name=\"moe_dropout\")\r\n",
        "            else:\r\n",
        "\r\n",
        "                mlp_fn = mlp_glu if use_mlp_glu else mlp\r\n",
        "                intermediate_size = nx.size * 4 * (1 if not use_mlp_glu else 2)\r\n",
        "\r\n",
        "                # Define intermediate layer of mlp - to split\r\n",
        "                dim_intermediate_expanded = mtf.Dimension(\"intermediate_expanded\", intermediate_size)\r\n",
        "\r\n",
        "                m = mlp_fn(res_x, \"mlp\", dim_intermediate_expanded, variable_dtype=variable_dtype, params=params)\r\n",
        "                aux_loss = mtf.zeros(x.mesh, mtf.Shape([]), dtype=variable_dtype.slice_dtype)\r\n",
        "\r\n",
        "            x = x + pre_residual_fn((m*mult), \"norm_rezero_2\", variable_dtype)\r\n",
        "            return x, aux_loss\r\n",
        "\r\n",
        "    return fn\r\n",
        "\r\n",
        "\r\n",
        "def axial_positional_emb(embd_dim, mesh, params, variable_dtype):\r\n",
        "    # Use axial position encoding\r\n",
        "    axial_dim_1, axial_dim_2 = params[\"axial_pos_emb\"]\r\n",
        "\r\n",
        "    axial_dim = mtf.Dimension(\"axial_dim\", axial_dim_1 * axial_dim_2)\r\n",
        "    dim_axials = [mtf.Dimension(f\"axial_dim_{i}\", t) for i, t in enumerate((axial_dim_1, axial_dim_2))]\r\n",
        "\r\n",
        "    axial_wpe_1 = mtf.get_variable(mesh, \"axial_wpe_1\", mtf.Shape([dim_axials[0], embd_dim]),\r\n",
        "                                   initializer=tf.random_normal_initializer(stddev=0.01),\r\n",
        "                                   master_dtype=variable_dtype.master_dtype,\r\n",
        "                                   slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                                   activation_dtype=variable_dtype.activation_dtype)\r\n",
        "\r\n",
        "    axial_wpe_2 = mtf.get_variable(mesh, \"axial_wpe_2\", mtf.Shape([dim_axials[1], embd_dim]),\r\n",
        "                                   initializer=tf.random_normal_initializer(stddev=0.01),\r\n",
        "                                   master_dtype=variable_dtype.master_dtype,\r\n",
        "                                   slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                                   activation_dtype=variable_dtype.activation_dtype)\r\n",
        "\r\n",
        "    axial_wpe_1, axial_wpe_2 = map(lambda t: mtf.broadcast(t, [dim_axials[0], dim_axials[1], embd_dim]),\r\n",
        "                                   (axial_wpe_1, axial_wpe_2))\r\n",
        "    wpe = (axial_wpe_1 + axial_wpe_2) / 2\r\n",
        "\r\n",
        "    wpe = mtf.reshape(wpe, [axial_dim, embd_dim])\r\n",
        "\r\n",
        "    return wpe\r\n",
        "\r\n",
        "# --------------------------------------------------------------------------------\r\n",
        "# MODEL:\r\n",
        "\r\n",
        "def model(mtf_features, other_features, params, mesh, variable_dtype, context=None):\r\n",
        "    \"\"\"Wecredo_Model implemented in mesh tensorflow.\"\"\"\r\n",
        "\r\n",
        "    x, batch_dim, sequence_dim, embd_dim, vocab_dim, embed_sequence_dim = parse_inputs(mtf_features, other_features)\r\n",
        "\r\n",
        "    if is_incremental_inference(context):\r\n",
        "        # reshape inputs if in inference mode\r\n",
        "        x = mtf.gather(x, context.position - 1, sequence_dim)\r\n",
        "        x = mtf.reshape(x, [batch_dim])\r\n",
        "\r\n",
        "    use_axial_pos_emb = params[\"axial_pos_emb\"] is not None\r\n",
        "\r\n",
        "    if not use_axial_pos_emb:\r\n",
        "        # Use standard position encoding\r\n",
        "        wpe = mtf.get_variable(mesh, \"wpe\", mtf.Shape([embed_sequence_dim, embd_dim]),\r\n",
        "                               initializer=tf.random_normal_initializer(stddev=0.01),\r\n",
        "                               master_dtype=variable_dtype.master_dtype,\r\n",
        "                               slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                               activation_dtype=variable_dtype.activation_dtype)\r\n",
        "    else:\r\n",
        "        wpe = axial_positional_emb(embd_dim, mesh, params, variable_dtype)\r\n",
        "\r\n",
        "    # Text encoding\r\n",
        "    wte = mtf.get_variable(mesh, \"wte\", mtf.Shape([vocab_dim, embd_dim]),\r\n",
        "                           initializer=tf.random_normal_initializer(stddev=0.02),\r\n",
        "                           master_dtype=variable_dtype.master_dtype,\r\n",
        "                           slice_dtype=variable_dtype.slice_dtype,\r\n",
        "                           activation_dtype=variable_dtype.activation_dtype)\r\n",
        "\r\n",
        "    with tf.variable_scope(\"token_embd\"):\r\n",
        "        # Text embedding\r\n",
        "        h = mtf.gather(wte, x, vocab_dim)\r\n",
        "        if params[\"embed_dropout\"] > 0 and params[\"mode\"] == \"train\":\r\n",
        "            h = mtf.dropout(h, rate=params[\"embed_dropout\"], name=\"wte_dropout\")\r\n",
        "\r\n",
        "    with tf.variable_scope(\"pos_embd\"):\r\n",
        "        # Positional embedding\r\n",
        "        position_indices = mtf.range(mesh, sequence_dim, tf.int64) if not is_incremental_inference(context) else (\r\n",
        "                context.position - 1)\r\n",
        "        pos_emb = mtf.gather(wpe, position_indices, wpe.shape[0])\r\n",
        "        if params[\"embed_dropout\"] > 0 and params[\"mode\"] == \"train\":\r\n",
        "            pos_emb = mtf.dropout(pos_emb, rate=params[\"embed_dropout\"], name=\"wte_dropout\")\r\n",
        "        h += pos_emb\r\n",
        "\r\n",
        "    aux_losses = 0  # instantiate auxiliary losses (for MOE models)\r\n",
        "\r\n",
        "    for layer in range(params[\"n_layer\"]):\r\n",
        "        # attn blocks\r\n",
        "        share_parameters = exists(params[\"share_parameters\"]) and params[\"share_parameters\"] == True\r\n",
        "        block_scope = f\"h{layer}\" if not share_parameters else \"\"\r\n",
        "\r\n",
        "        block_fn = block(params=params, scope=block_scope, layer_num=layer,\r\n",
        "                         bias=other_features[\"attn_bias\"],\r\n",
        "                         sequence_dim=sequence_dim,\r\n",
        "                         memory_length_dim=other_features[\"memory_length_dim\"],\r\n",
        "                         variable_dtype=variable_dtype,\r\n",
        "                         context=context)\r\n",
        "\r\n",
        "        # If true and in train mode, enable gradient checkpointing\r\n",
        "        recompute_grad = params[\"recompute_grad\"] and (params[\"mode\"] == \"train\") == True\r\n",
        "        h, loss = block_fn(h) if not recompute_grad else mtf.recompute_grad(block_fn, [h])\r\n",
        "        aux_losses += loss\r\n",
        "\r\n",
        "    no_weight_tie_emb = params[\"no_weight_tie\"] == True\r\n",
        "    if no_weight_tie_emb:\r\n",
        "        with tf.variable_scope(\"wte_final_linear\"):\r\n",
        "            logits = linear(h, \"linear_out\", vocab_dim, variable_dtype=variable_dtype, params=params)\r\n",
        "    else:\r\n",
        "        # Layer normalize & affine transform\r\n",
        "        h = layer_norm(h, \"ln_f\", variable_dtype=variable_dtype)\r\n",
        "        seq_dim = sequence_dim if not is_incremental_inference(context) else mtf.Dimension(\"sequence\", 1)\r\n",
        "        with tf.variable_scope(\"wte_final_einsum\"):\r\n",
        "            # Equivalent to tf.matmul\r\n",
        "            logits = mtf.einsum([h, wte], output_shape=[batch_dim, seq_dim, vocab_dim])\r\n",
        "\r\n",
        "    if params[\"mode\"] in [\"train\", \"eval\"]:\r\n",
        "        labels = mtf_features[\"labels\"]\r\n",
        "        z_loss = params.get(\"z_loss\", 1e-4) # an auxiliary loss used to stabilize mtf xentropy\r\n",
        "\r\n",
        "        # Go to full precision for the logits \r\n",
        "        logits = mtf.cast(logits, tf.float32)\r\n",
        "\r\n",
        "        use_entmax_loss = params.get(\"entmax_loss\", False)\r\n",
        "        loss_fn = mtf.layers.softmax_cross_entropy_with_logits if not use_entmax_loss else entmax_cross_entropy_with_logits\r\n",
        "\r\n",
        "        with tf.variable_scope(\"xentropy_final\"):\r\n",
        "            loss_batch = loss_fn(logits=logits, targets=labels,\r\n",
        "                                 vocab_dim=logits.shape[-1], z_loss=z_loss)\r\n",
        "\r\n",
        "        # For non-autoregressive models (masked language modeling training)\r\n",
        "        # Make sure labels with padding tokens are not counted in the loss\r\n",
        "        if not params[\"causal\"]:\r\n",
        "            padding_id = params.get(\"padding_id\", 0)\r\n",
        "            loss_batch = mtf.where(mtf.not_equal(labels, padding_id), loss_batch, mtf.zeros_like(loss_batch))\r\n",
        "\r\n",
        "        with tf.variable_scope(\"reduce_mean_final\"):\r\n",
        "            loss = mtf.reduce_mean(loss_batch)\r\n",
        "\r\n",
        "        loss += aux_losses  # Add on auxiliary losses (currently only used for MoE)\r\n",
        "        loss /= params[\"num_microbatches\"]\r\n",
        "        # Convert to train dtype\r\n",
        "        loss = mtf.cast(loss, variable_dtype.slice_dtype)\r\n",
        "    else:\r\n",
        "        loss = None\r\n",
        "        loss_batch = None\r\n",
        "\r\n",
        "    # Cast back to checkpoint dtype\r\n",
        "    logits = mtf.cast(logits, variable_dtype.master_dtype)\r\n",
        "    return logits, loss, loss_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6da1f4f0ae9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmesh_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmesh_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtf_transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mesh_tensorflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDMXjYnwbUJo"
      },
      "source": [
        "### WIP - CONFIGURING THE MODEL & TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KehXHGzvbZAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrPZyZmB8wsk"
      },
      "source": [
        "---\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvxOXQLbpokZ"
      },
      "source": [
        "# EXPERIMENTS \r\n",
        "\r\n",
        "Experiments to test the current implementation - Whether it works and produces acceptable results on simple tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsTVnTJeWR1l"
      },
      "source": [
        "## SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCuaa2BJXat8",
        "outputId": "ef0f6990-8aa3-4a4d-b601-97d628e25d15"
      },
      "source": [
        "!pip install datasets\r\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
            "\r\u001b[K     |██                              | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 11.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, pyarrow, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.1 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 5.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=772688c173b6de3115678b31c2ebd2018d085580092bf4887761681ecf057720\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGL_VHcFnmi7"
      },
      "source": [
        "import transformers\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Load SQuAD from datasets or TFDS\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "from datasets import load_dataset\r\n",
        "\r\n",
        "# If import error below, restart session; reinstall transformers\r\n",
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDYZPe85aBTm",
        "outputId": "e47badbd-6900-47a6-b5fc-170c5dad26a7"
      },
      "source": [
        "### Prepare Data ###\r\n",
        "\r\n",
        "# Dict of form: dict_keys(['answers', 'context', 'id', 'question', 'title'])\r\n",
        "train_ds = load_dataset('squad', split='train')\r\n",
        "valid_ds = load_dataset('squad', split='validation')\r\n",
        "\r\n",
        "# Tokenize data to prepare for feeding in model \r\n",
        "# Using huggingface tokenizer for now - We can easily swap this for a CN one lateron\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\r\n",
        "\r\n",
        "encoder_max_len = 250\r\n",
        "decoder_max_len = 54\r\n",
        "batch_size = 4\r\n",
        "buffer_size = 1000\r\n",
        "\r\n",
        "ntrain = len(train_ds)\r\n",
        "nvalid = len(valid_ds)\r\n",
        "steps = int(np.ceil(ntrain/batch_size))\r\n",
        "valid_steps = int(np.ceil(nvalid/batch_size))\r\n",
        "\r\n",
        "def tokenize(example):\r\n",
        "    \"\"\"\r\n",
        "    Prepares input example for model; TODO: Remove max_len?\r\n",
        "\r\n",
        "    eos_token=\"</s>\",\r\n",
        "    unk_token=\"<unk>\",\r\n",
        "    pad_token=\"<pad>\"\r\n",
        "    \"\"\"\r\n",
        "    context = example['context']\r\n",
        "    question = example['question']\r\n",
        "    answer = example['answers']['text']\r\n",
        "\r\n",
        "    question_plus = f\"answer_me: {str(question)}\"\r\n",
        "    question_plus += f\" context: {str(context)} </s>\"\r\n",
        "    \r\n",
        "    answer_plus = ', '.join([i for i in list(answer)])\r\n",
        "    answer_plus = f\"{answer_plus} </s>\"\r\n",
        "\r\n",
        "    encoder_inputs = tokenizer(question_plus, truncation=True, \r\n",
        "                              return_tensors='tf', max_length=encoder_max_len,\r\n",
        "                              pad_to_max_length=True)\r\n",
        "    \r\n",
        "    decoder_inputs = tokenizer(answer_plus, truncation=True, \r\n",
        "                               return_tensors='tf', max_length=decoder_max_len,\r\n",
        "                               pad_to_max_length=True)\r\n",
        "    \r\n",
        "    input_ids = encoder_inputs['input_ids'][0]\r\n",
        "    input_attention = encoder_inputs['attention_mask'][0]\r\n",
        "    target_ids = decoder_inputs['input_ids'][0]\r\n",
        "    target_attention = decoder_inputs['attention_mask'][0]\r\n",
        "    \r\n",
        "    outputs = {'input_ids': input_ids, 'attention_mask': input_attention, \r\n",
        "               'labels': target_ids, 'decoder_attention_mask': target_attention}\r\n",
        "    return outputs\r\n",
        "\r\n",
        "def to_tf_dataset(dataset): \r\n",
        "  \"\"\"\r\n",
        "  Turns dataset into a TF compatible dataset; TODO: Combine with tokenize?  / Load TF dataset directly by loading Squad from tfds\r\n",
        "  \"\"\" \r\n",
        "  columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\r\n",
        "  dataset.set_format(type='tensorflow', columns=columns)\r\n",
        "  return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \r\n",
        "                'labels':tf.int32, 'decoder_attention_mask':tf.int32,  }\r\n",
        "  return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \r\n",
        "                  'labels': tf.TensorShape([None]), 'decoder_attention_mask':tf.TensorShape([None])}\r\n",
        "  ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\r\n",
        "  return ds\r\n",
        "\r\n",
        "\r\n",
        "train_ds = train_ds.map(tokenize)\r\n",
        "valid_ds = valid_ds.map(tokenize)\r\n",
        "\r\n",
        "train_ds = to_tf_dataset(train_ds)\r\n",
        "valid_ds = to_tf_dataset(valid_ds)\r\n",
        "\r\n",
        "train_ds = train_ds.shuffle(buffer_size).batch(batch_size)\r\n",
        "valid_ds = train_ds.shuffle(buffer_size).batch(batch_size)\r\n",
        "\r\n",
        "#for ex in train_ds.take(1):\r\n",
        "#    print(ex.keys())\r\n",
        "#    print(ex[\"answers\"].keys())\r\n",
        "#    print(ex[\"answers\"][\"text\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n",
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-fe2060d78a38fa6c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-7ad1f6e7023309c8.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obcuf30FYjC2"
      },
      "source": [
        "### Model ###\r\n",
        "\r\n",
        "# Wrapping the original model with two simple functions: Train & Test\r\n",
        "# Wrapping a Huggingface model for now but lateron the Wecredo model architecture\r\n",
        "\r\n",
        "class Wrapper(TFT5ForConditionalGeneration):\r\n",
        "    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):\r\n",
        "        super().__init__(*args, **kwargs)\r\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name='loss') \r\n",
        "    \r\n",
        "    # > Graph execution w/ tf.function\r\n",
        "    @tf.function\r\n",
        "    def train_step(self, data):\r\n",
        "        x = data\r\n",
        "        y = x[\"labels\"]\r\n",
        "        y = tf.reshape(y, [-1, 1])\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "            # > Feeds it just into TFT5ForConditionalGeneration; training=True turns on dropout\r\n",
        "            outputs = self(x, training=True)\r\n",
        "\r\n",
        "            # TODO: Manually compute loss; not have transformer autocompute it\r\n",
        "            loss = outputs[0]  \r\n",
        "            logits = outputs[1]\r\n",
        "\r\n",
        "            # Reduce loss to single digit\r\n",
        "            loss = tf.reduce_mean(loss)\r\n",
        "            grads = tape.gradient(loss, self.trainable_variables)\r\n",
        "            \r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n",
        "        lr = self.optimizer._decayed_lr(tf.float32)\r\n",
        "        \r\n",
        "        self.loss_tracker.update_state(loss)        \r\n",
        "        self.compiled_metrics.update_state(y, logits)\r\n",
        "        metrics = {m.name: m.result() for m in self.metrics}\r\n",
        "        metrics.update({'lr': lr})\r\n",
        "        \r\n",
        "        return metrics\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        x = data\r\n",
        "        y = x[\"labels\"]\r\n",
        "        y = tf.reshape(y, [-1, 1])\r\n",
        "        output = self(x, training=False)\r\n",
        "        loss = output[0]\r\n",
        "        loss = tf.reduce_mean(loss)\r\n",
        "        logits = output[1]\r\n",
        "        \r\n",
        "        self.loss_tracker.update_state(loss)\r\n",
        "        self.compiled_metrics.update_state(y, logits)\r\n",
        "        return {m.name: m.result() for m in self.metrics}\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "j2At3O2QjzyH",
        "outputId": "16bc9228-2e3a-48f6-bf70-92dbcc3bb7ff"
      },
      "source": [
        "### Training ###\r\n",
        "\r\n",
        "# \"Friendlier\" metric as only looks whether ground truth is in models top 5 preds\r\n",
        "metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(name='accuracy')]\r\n",
        "\r\n",
        "learning_rate = 0.001 \r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
        "\r\n",
        "model = Wrapper.from_pretrained(\"t5-base\")\r\n",
        "model.compile(optimizer=optimizer, metrics=metrics)\r\n",
        "\r\n",
        "# TODO: Move to eager execution; TODO: Train on TPU\r\n",
        "# TODO - Breaks at 2nd epoch?\r\n",
        "model.fit(train_ds, epochs=5, steps_per_epoch=steps, validation_data=valid_ds, validation_steps=valid_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing Wrapper.\n",
            "\n",
            "All the layers of Wrapper were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wrapper for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc9be7795f8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fc9dc127d90> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc9be7795f8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fc9dc127d90> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc9d9ab78c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function wrap at 0x7fc9d9ab78c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  918/21900 [>.............................] - ETA: 1:25:48 - accuracy: 0.9796 - loss: 0.1103 - lr: 0.0010"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7716ffc12c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8qwAKNQLLlf",
        "outputId": "f3848de6-29d2-42cf-ccba-fd5843e2ba75"
      },
      "source": [
        "### Eager Training ###\r\n",
        "\r\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-base\")\r\n",
        "\r\n",
        "loss_object = tf.keras.metrics.Mean(name='loss') \r\n",
        "\r\n",
        "loss_history_train = []\r\n",
        "loss_history_val = []\r\n",
        "\r\n",
        "def train_step(data):\r\n",
        "    x = data\r\n",
        "    #y = x[\"labels\"]\r\n",
        "    #y = tf.reshape(y, [-1, 1])\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "        # > Feeds it just into TFT5ForConditionalGeneration; training=True turns on dropout\r\n",
        "        outputs = model(x, training=True)\r\n",
        "\r\n",
        "        # TODO: Manually compute loss; not have transformer autocompute it\r\n",
        "        loss = outputs[0]  \r\n",
        "        logits = outputs[1]\r\n",
        "\r\n",
        "        # Reduce loss to single digit\r\n",
        "        loss = tf.reduce_mean(loss)\r\n",
        "\r\n",
        "    loss_history_train.append(loss_value.numpy().mean())\r\n",
        "    # Calculate grads & update\r\n",
        "    grads = tape.gradient(loss, model.trainable_variables)    \r\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
        "    \r\n",
        "    #loss_object.update_state(loss)        \r\n",
        "    #self.compiled_metrics.update_state(y, logits)\r\n",
        "    #metrics = {m.name: m.result() for m in self.metrics}\r\n",
        "    #return metrics\r\n",
        "\r\n",
        "def test_step(data):\r\n",
        "    x = data\r\n",
        "    #y = x[\"labels\"]\r\n",
        "    #y = tf.reshape(y, [-1, 1])\r\n",
        "    output = model(x, training=False)\r\n",
        "    loss = output[0]\r\n",
        "    loss = tf.reduce_mean(loss)\r\n",
        "    loss_history_val.append(loss_value.numpy().mean())\r\n",
        "    logits = output[1]\r\n",
        "\r\n",
        "\r\n",
        "def train(epochs):\r\n",
        "  for epoch in range(epochs):\r\n",
        "    for (batch, (train, val)) in enumerate(zip(train_ds, valid_ds)):\r\n",
        "      train_step(train)\r\n",
        "      test_step(val)\r\n",
        "      if batch % 5 == 0:\r\n",
        "        print('Batch {}, Last Train Loss {}, Last Val Loss {}'.format(batch, loss_history_train[-1], loss_history_val[-1]))\r\n",
        "    print ('Epoch {} finished'.format(epoch))\r\n",
        "\r\n",
        "\r\n",
        "train(epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-964de7304e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-964de7304e6d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2574\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2575\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2577\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Wa46Qarmwa",
        "outputId": "f0eae424-c315-4f8c-e818-89e7310f626b"
      },
      "source": [
        "context = \"Beijing (/ˌbeɪˈdʒɪŋ/ BAY-JING[10][11] Mandarin pronunciation: [pèi.tɕíŋ] (About this soundlisten)), alternatively romanized as Peking[12] (/ˌpiˈkɪŋ/ PEE-KING),[13] is the capital of the People's Republic of China. It is the world's most populous national capital city, with over 21 million residents within an administrative area of 16,410.5 km2 (6336 sq. mi.).[4] It is located in Northern China, is governed as a municipality under the direct administration of the State Council with 16 urban, suburban, and rural districts.[14] Beijing is mostly surrounded by Hebei Province with the exception of neighboring Tianjin to the southeast; together, the three divisions form the Jingjinji megalopolis and the national capital region of China\"\r\n",
        "\r\n",
        "question = \"What is the capital of China?\"\r\n",
        "\r\n",
        "input_text =  f\"answer_me: {question} context: {context} </s>\"\r\n",
        "encoded_query = tokenizer(input_text, return_tensors='tf', pad_to_max_length=True, truncation=True, max_length=encoder_max_len)\r\n",
        "input_ids = encoded_query[\"input_ids\"]\r\n",
        "attention_mask = encoded_query[\"attention_mask\"]\r\n",
        "generated_answer = model.generate(input_ids, attention_mask=attention_mask, \r\n",
        "                                 max_length=decoder_max_len, top_p=0.95, top_k=50, repetition_penalty=2)\r\n",
        "decoded_answer = tokenizer.decode(generated_answer.numpy()[0])\r\n",
        "print(\"Answer: \", decoded_answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Answer:  <pad> the capital</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CBh5BFXucR0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC6c0nZobngy"
      },
      "source": [
        "## CN-ENG Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHP6fXUbLmT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNFDzmw3WaYn"
      },
      "source": [
        "## Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMEz1l5r8bRU"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjzWDhQr8-lM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}